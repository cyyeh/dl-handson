{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recitation-11",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "51jQJkusXmDY",
        "colab_type": "code",
        "outputId": "62151c58-4e24-4f34-cfbe-0bd7a6915ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd recitation-11/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/recitation-11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY_IfzezV9YI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import rbm_demo_utils\n",
        "import rbm_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ur_UQBgWOK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = rbm_demo_utils.MNIST(max_len=2000) # Load MNIST Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJhiZO2dXsGt",
        "colab_type": "code",
        "outputId": "5af675fb-841d-4a96-d72e-ae2c4c4d03db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(mnist[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDiRB2IEkCDuQBGEHkiDsQBL/B9PZXNHlaOf2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8A0081SZTZi",
        "colab_type": "text"
      },
      "source": [
        "# Hopfield Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNHwhCSKXu5O",
        "colab_type": "code",
        "outputId": "c705e4be-7067-41e6-8cda-f7f7af74fcab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "net = rbm_models.HopfieldNet(mnist[0][0].size()[0]) # initalized a hopfield net with 784 units (Mnist is 28 x 28)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/recitation-11/rbm_models.py:40: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  torch.nn.init.xavier_normal(self.weights)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKk7uNWSXwdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = rbm_models.HopfieldTrainWrapper(net, 1) # This is a trainer. the \"1\" input means to evolve once every iter."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqpHSlsXxuN",
        "colab_type": "code",
        "outputId": "51a3dfb3-e856-443e-f477-4c238af4ce27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rbm_demo_utils.train([trainer], mnist, rbm_demo_utils.IdentityLoss(), 300, 500, 0.001, 'hopfield')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded checkpoint directly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UwsNSLcXznm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = torch.autograd.Variable(mnist[np.random.randint(2000, 60000)][0].unsqueeze(dim=0))\n",
        "example[0, -10:] = -1\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "if net.weights.data.is_cuda:\n",
        "    example = example.cuda()\n",
        "example_ev = net(example, num_iters=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0xoAO_0ZC1i",
        "colab_type": "code",
        "outputId": "842e7618-def6-4b47-abf9-1ad5605d524a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZQeSoOxAEpQdSIKyA0n8H1yB59sRSBfuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYDFwU5zZFHY",
        "colab_type": "code",
        "outputId": "7d86da38-5ce2-4feb-feb6-23a4b8c1eb62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example_ev.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxzIeQAsZdPB",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic Hopfield Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAx4cgGiZipG",
        "colab_type": "code",
        "outputId": "c444017f-b1f0-446a-bf68-bbba22a97a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "net = rbm_models.StochasticHopfieldNet(mnist[0][0].size()[0], temperature=0.1) # initalized a stochastic hopfield net with 784 units (Mnist is 28 x 28)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/recitation-11/rbm_models.py:40: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  torch.nn.init.xavier_normal(self.weights)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0f_oG8cccqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = rbm_models.StochasticHopfieldTrainWrapper(net, 3, num_rand_sample_iters=500) # This is a trainer. the \"3\" input means to evolve once every iter."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-4dKxIqceRM",
        "colab_type": "code",
        "outputId": "ec7b76d2-c3b7-445c-b180-5fbbbf9b1d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rbm_demo_utils.train([trainer], mnist, rbm_demo_utils.IdentityLoss(), 300, 500, 0.001, 'stochastic_hopfield_temp_0_2_evolve_3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[+][2020-04-04 18:54:10.219539] Training iteration 0 (batch 0 of epoch 0).\n",
            "Loss: 0.996139\n",
            "[+][2020-04-04 18:54:10.502094] Training iteration 1 (batch 1 of epoch 0).\n",
            "Loss: 0.661355\n",
            "[+][2020-04-04 18:54:10.780805] Training iteration 2 (batch 2 of epoch 0).\n",
            "Loss: 0.395983\n",
            "[+][2020-04-04 18:54:11.054001] Training iteration 3 (batch 3 of epoch 0).\n",
            "Loss: 0.401181\n",
            "[+][2020-04-04 18:54:11.335444] Training iteration 4 (batch 4 of epoch 0).\n",
            "Loss: 0.379917\n",
            "[+][2020-04-04 18:54:11.610491] Training iteration 5 (batch 1 of epoch 1).\n",
            "Loss: 0.375950\n",
            "[+][2020-04-04 18:54:11.888452] Training iteration 6 (batch 2 of epoch 1).\n",
            "Loss: 0.353409\n",
            "[+][2020-04-04 18:54:12.164994] Training iteration 7 (batch 3 of epoch 1).\n",
            "Loss: 0.345076\n",
            "[+][2020-04-04 18:54:12.449457] Training iteration 8 (batch 4 of epoch 1).\n",
            "Loss: 0.335887\n",
            "[+][2020-04-04 18:54:12.718290] Training iteration 9 (batch 1 of epoch 2).\n",
            "Loss: 0.319902\n",
            "[+][2020-04-04 18:54:13.011382] Training iteration 10 (batch 2 of epoch 2).\n",
            "Loss: 0.312337\n",
            "[+][2020-04-04 18:54:13.291918] Training iteration 11 (batch 3 of epoch 2).\n",
            "Loss: 0.294670\n",
            "[+][2020-04-04 18:54:13.565871] Training iteration 12 (batch 4 of epoch 2).\n",
            "Loss: 0.294081\n",
            "[+][2020-04-04 18:54:13.844641] Training iteration 13 (batch 1 of epoch 3).\n",
            "Loss: 0.282899\n",
            "[+][2020-04-04 18:54:14.117559] Training iteration 14 (batch 2 of epoch 3).\n",
            "Loss: 0.267550\n",
            "[+][2020-04-04 18:54:14.398525] Training iteration 15 (batch 3 of epoch 3).\n",
            "Loss: 0.264038\n",
            "[+][2020-04-04 18:54:14.672274] Training iteration 16 (batch 4 of epoch 3).\n",
            "Loss: 0.260534\n",
            "[+][2020-04-04 18:54:14.949539] Training iteration 17 (batch 1 of epoch 4).\n",
            "Loss: 0.254935\n",
            "[+][2020-04-04 18:54:15.223966] Training iteration 18 (batch 2 of epoch 4).\n",
            "Loss: 0.231586\n",
            "[+][2020-04-04 18:54:15.512333] Training iteration 19 (batch 3 of epoch 4).\n",
            "Loss: 0.242725\n",
            "[+][2020-04-04 18:54:15.784534] Training iteration 20 (batch 4 of epoch 4).\n",
            "Loss: 0.238320\n",
            "[+][2020-04-04 18:54:16.092866] Training iteration 21 (batch 1 of epoch 5).\n",
            "Loss: 0.230034\n",
            "[+][2020-04-04 18:54:16.411582] Training iteration 22 (batch 2 of epoch 5).\n",
            "Loss: 0.229440\n",
            "[+][2020-04-04 18:54:16.685452] Training iteration 23 (batch 3 of epoch 5).\n",
            "Loss: 0.222756\n",
            "[+][2020-04-04 18:54:16.960929] Training iteration 24 (batch 4 of epoch 5).\n",
            "Loss: 0.217421\n",
            "[+][2020-04-04 18:54:17.237803] Training iteration 25 (batch 1 of epoch 6).\n",
            "Loss: 0.211343\n",
            "[+][2020-04-04 18:54:17.518376] Training iteration 26 (batch 2 of epoch 6).\n",
            "Loss: 0.211537\n",
            "[+][2020-04-04 18:54:17.789631] Training iteration 27 (batch 3 of epoch 6).\n",
            "Loss: 0.203570\n",
            "[+][2020-04-04 18:54:18.082548] Training iteration 28 (batch 4 of epoch 6).\n",
            "Loss: 0.205741\n",
            "[+][2020-04-04 18:54:18.367417] Training iteration 29 (batch 1 of epoch 7).\n",
            "Loss: 0.206651\n",
            "[+][2020-04-04 18:54:18.637609] Training iteration 30 (batch 2 of epoch 7).\n",
            "Loss: 0.182561\n",
            "[+][2020-04-04 18:54:18.916678] Training iteration 31 (batch 3 of epoch 7).\n",
            "Loss: 0.193176\n",
            "[+][2020-04-04 18:54:19.193742] Training iteration 32 (batch 4 of epoch 7).\n",
            "Loss: 0.185358\n",
            "[+][2020-04-04 18:54:19.471867] Training iteration 33 (batch 1 of epoch 8).\n",
            "Loss: 0.184364\n",
            "[+][2020-04-04 18:54:19.744065] Training iteration 34 (batch 2 of epoch 8).\n",
            "Loss: 0.197633\n",
            "[+][2020-04-04 18:54:20.028444] Training iteration 35 (batch 3 of epoch 8).\n",
            "Loss: 0.172661\n",
            "[+][2020-04-04 18:54:20.307917] Training iteration 36 (batch 4 of epoch 8).\n",
            "Loss: 0.184711\n",
            "[+][2020-04-04 18:54:20.586025] Training iteration 37 (batch 1 of epoch 9).\n",
            "Loss: 0.172348\n",
            "[+][2020-04-04 18:54:20.872546] Training iteration 38 (batch 2 of epoch 9).\n",
            "Loss: 0.177885\n",
            "[+][2020-04-04 18:54:21.147587] Training iteration 39 (batch 3 of epoch 9).\n",
            "Loss: 0.170478\n",
            "[+][2020-04-04 18:54:21.423903] Training iteration 40 (batch 4 of epoch 9).\n",
            "Loss: 0.174123\n",
            "[+][2020-04-04 18:54:21.699250] Training iteration 41 (batch 1 of epoch 10).\n",
            "Loss: 0.163879\n",
            "[+][2020-04-04 18:54:21.992962] Training iteration 42 (batch 2 of epoch 10).\n",
            "Loss: 0.167455\n",
            "[+][2020-04-04 18:54:22.269076] Training iteration 43 (batch 3 of epoch 10).\n",
            "Loss: 0.182800\n",
            "[+][2020-04-04 18:54:22.546741] Training iteration 44 (batch 4 of epoch 10).\n",
            "Loss: 0.173000\n",
            "[+][2020-04-04 18:54:22.827598] Training iteration 45 (batch 1 of epoch 11).\n",
            "Loss: 0.160342\n",
            "[+][2020-04-04 18:54:23.106822] Training iteration 46 (batch 2 of epoch 11).\n",
            "Loss: 0.159607\n",
            "[+][2020-04-04 18:54:23.382141] Training iteration 47 (batch 3 of epoch 11).\n",
            "Loss: 0.159224\n",
            "[+][2020-04-04 18:54:23.662031] Training iteration 48 (batch 4 of epoch 11).\n",
            "Loss: 0.155184\n",
            "[+][2020-04-04 18:54:23.940406] Training iteration 49 (batch 1 of epoch 12).\n",
            "Loss: 0.170898\n",
            "[+][2020-04-04 18:54:24.231830] Training iteration 50 (batch 2 of epoch 12).\n",
            "Loss: 0.162279\n",
            "[+][2020-04-04 18:54:24.508202] Training iteration 51 (batch 3 of epoch 12).\n",
            "Loss: 0.153542\n",
            "[+][2020-04-04 18:54:24.807070] Training iteration 52 (batch 4 of epoch 12).\n",
            "Loss: 0.159858\n",
            "[+][2020-04-04 18:54:25.087728] Training iteration 53 (batch 1 of epoch 13).\n",
            "Loss: 0.153838\n",
            "[+][2020-04-04 18:54:25.361739] Training iteration 54 (batch 2 of epoch 13).\n",
            "Loss: 0.143776\n",
            "[+][2020-04-04 18:54:25.649219] Training iteration 55 (batch 3 of epoch 13).\n",
            "Loss: 0.149438\n",
            "[+][2020-04-04 18:54:25.926666] Training iteration 56 (batch 4 of epoch 13).\n",
            "Loss: 0.142753\n",
            "[+][2020-04-04 18:54:26.211459] Training iteration 57 (batch 1 of epoch 14).\n",
            "Loss: 0.144433\n",
            "[+][2020-04-04 18:54:26.500913] Training iteration 58 (batch 2 of epoch 14).\n",
            "Loss: 0.149517\n",
            "[+][2020-04-04 18:54:26.779060] Training iteration 59 (batch 3 of epoch 14).\n",
            "Loss: 0.139986\n",
            "[+][2020-04-04 18:54:27.054323] Training iteration 60 (batch 4 of epoch 14).\n",
            "Loss: 0.143401\n",
            "[+][2020-04-04 18:54:27.327318] Training iteration 61 (batch 1 of epoch 15).\n",
            "Loss: 0.145329\n",
            "[+][2020-04-04 18:54:27.618367] Training iteration 62 (batch 2 of epoch 15).\n",
            "Loss: 0.157796\n",
            "[+][2020-04-04 18:54:27.892582] Training iteration 63 (batch 3 of epoch 15).\n",
            "Loss: 0.143205\n",
            "[+][2020-04-04 18:54:28.172496] Training iteration 64 (batch 4 of epoch 15).\n",
            "Loss: 0.153559\n",
            "[+][2020-04-04 18:54:28.447029] Training iteration 65 (batch 1 of epoch 16).\n",
            "Loss: 0.147613\n",
            "[+][2020-04-04 18:54:28.725685] Training iteration 66 (batch 2 of epoch 16).\n",
            "Loss: 0.139906\n",
            "[+][2020-04-04 18:54:29.002015] Training iteration 67 (batch 3 of epoch 16).\n",
            "Loss: 0.143180\n",
            "[+][2020-04-04 18:54:29.278033] Training iteration 68 (batch 4 of epoch 16).\n",
            "Loss: 0.143905\n",
            "[+][2020-04-04 18:54:29.552414] Training iteration 69 (batch 1 of epoch 17).\n",
            "Loss: 0.133612\n",
            "[+][2020-04-04 18:54:29.828523] Training iteration 70 (batch 2 of epoch 17).\n",
            "Loss: 0.135954\n",
            "[+][2020-04-04 18:54:30.115786] Training iteration 71 (batch 3 of epoch 17).\n",
            "Loss: 0.153162\n",
            "[+][2020-04-04 18:54:30.401003] Training iteration 72 (batch 4 of epoch 17).\n",
            "Loss: 0.131451\n",
            "[+][2020-04-04 18:54:30.679909] Training iteration 73 (batch 1 of epoch 18).\n",
            "Loss: 0.138029\n",
            "[+][2020-04-04 18:54:30.968892] Training iteration 74 (batch 2 of epoch 18).\n",
            "Loss: 0.153923\n",
            "[+][2020-04-04 18:54:31.247424] Training iteration 75 (batch 3 of epoch 18).\n",
            "Loss: 0.156553\n",
            "[+][2020-04-04 18:54:31.519108] Training iteration 76 (batch 4 of epoch 18).\n",
            "Loss: 0.140722\n",
            "[+][2020-04-04 18:54:31.800946] Training iteration 77 (batch 1 of epoch 19).\n",
            "Loss: 0.138784\n",
            "[+][2020-04-04 18:54:32.078781] Training iteration 78 (batch 2 of epoch 19).\n",
            "Loss: 0.142606\n",
            "[+][2020-04-04 18:54:32.351824] Training iteration 79 (batch 3 of epoch 19).\n",
            "Loss: 0.139859\n",
            "[+][2020-04-04 18:54:32.637802] Training iteration 80 (batch 4 of epoch 19).\n",
            "Loss: 0.137821\n",
            "[+][2020-04-04 18:54:32.911602] Training iteration 81 (batch 1 of epoch 20).\n",
            "Loss: 0.143562\n",
            "[+][2020-04-04 18:54:33.188295] Training iteration 82 (batch 2 of epoch 20).\n",
            "Loss: 0.125964\n",
            "[+][2020-04-04 18:54:33.464361] Training iteration 83 (batch 3 of epoch 20).\n",
            "Loss: 0.144489\n",
            "[+][2020-04-04 18:54:33.741812] Training iteration 84 (batch 4 of epoch 20).\n",
            "Loss: 0.142111\n",
            "[+][2020-04-04 18:54:34.020098] Training iteration 85 (batch 1 of epoch 21).\n",
            "Loss: 0.139477\n",
            "[+][2020-04-04 18:54:34.314751] Training iteration 86 (batch 2 of epoch 21).\n",
            "Loss: 0.142279\n",
            "[+][2020-04-04 18:54:34.590616] Training iteration 87 (batch 3 of epoch 21).\n",
            "Loss: 0.139254\n",
            "[+][2020-04-04 18:54:34.869140] Training iteration 88 (batch 4 of epoch 21).\n",
            "Loss: 0.146379\n",
            "[+][2020-04-04 18:54:35.148149] Training iteration 89 (batch 1 of epoch 22).\n",
            "Loss: 0.132843\n",
            "[+][2020-04-04 18:54:35.446804] Training iteration 90 (batch 2 of epoch 22).\n",
            "Loss: 0.131025\n",
            "[+][2020-04-04 18:54:35.722521] Training iteration 91 (batch 3 of epoch 22).\n",
            "Loss: 0.131126\n",
            "[+][2020-04-04 18:54:35.992567] Training iteration 92 (batch 4 of epoch 22).\n",
            "Loss: 0.134869\n",
            "[+][2020-04-04 18:54:36.295446] Training iteration 93 (batch 1 of epoch 23).\n",
            "Loss: 0.130819\n",
            "[+][2020-04-04 18:54:36.569346] Training iteration 94 (batch 2 of epoch 23).\n",
            "Loss: 0.138029\n",
            "[+][2020-04-04 18:54:36.860990] Training iteration 95 (batch 3 of epoch 23).\n",
            "Loss: 0.137564\n",
            "[+][2020-04-04 18:54:37.137835] Training iteration 96 (batch 4 of epoch 23).\n",
            "Loss: 0.142604\n",
            "[+][2020-04-04 18:54:37.417180] Training iteration 97 (batch 1 of epoch 24).\n",
            "Loss: 0.125709\n",
            "[+][2020-04-04 18:54:37.703715] Training iteration 98 (batch 2 of epoch 24).\n",
            "Loss: 0.127462\n",
            "[+][2020-04-04 18:54:37.982369] Training iteration 99 (batch 3 of epoch 24).\n",
            "Loss: 0.126492\n",
            "[+][2020-04-04 18:54:38.261369] Training iteration 100 (batch 4 of epoch 24).\n",
            "Loss: 0.134834\n",
            "[+][2020-04-04 18:54:38.533417] Training iteration 101 (batch 1 of epoch 25).\n",
            "Loss: 0.134406\n",
            "[+][2020-04-04 18:54:38.812442] Training iteration 102 (batch 2 of epoch 25).\n",
            "Loss: 0.137603\n",
            "[+][2020-04-04 18:54:39.088449] Training iteration 103 (batch 3 of epoch 25).\n",
            "Loss: 0.134827\n",
            "[+][2020-04-04 18:54:39.370013] Training iteration 104 (batch 4 of epoch 25).\n",
            "Loss: 0.127175\n",
            "[+][2020-04-04 18:54:39.646738] Training iteration 105 (batch 1 of epoch 26).\n",
            "Loss: 0.136771\n",
            "[+][2020-04-04 18:54:39.925156] Training iteration 106 (batch 2 of epoch 26).\n",
            "Loss: 0.125562\n",
            "[+][2020-04-04 18:54:40.229056] Training iteration 107 (batch 3 of epoch 26).\n",
            "Loss: 0.126699\n",
            "[+][2020-04-04 18:54:40.529528] Training iteration 108 (batch 4 of epoch 26).\n",
            "Loss: 0.124335\n",
            "[+][2020-04-04 18:54:40.811070] Training iteration 109 (batch 1 of epoch 27).\n",
            "Loss: 0.125526\n",
            "[+][2020-04-04 18:54:41.096270] Training iteration 110 (batch 2 of epoch 27).\n",
            "Loss: 0.124276\n",
            "[+][2020-04-04 18:54:41.385746] Training iteration 111 (batch 3 of epoch 27).\n",
            "Loss: 0.134416\n",
            "[+][2020-04-04 18:54:41.658249] Training iteration 112 (batch 4 of epoch 27).\n",
            "Loss: 0.130893\n",
            "[+][2020-04-04 18:54:41.938145] Training iteration 113 (batch 1 of epoch 28).\n",
            "Loss: 0.130219\n",
            "[+][2020-04-04 18:54:42.217112] Training iteration 114 (batch 2 of epoch 28).\n",
            "Loss: 0.122147\n",
            "[+][2020-04-04 18:54:42.495905] Training iteration 115 (batch 3 of epoch 28).\n",
            "Loss: 0.127922\n",
            "[+][2020-04-04 18:54:42.768467] Training iteration 116 (batch 4 of epoch 28).\n",
            "Loss: 0.129710\n",
            "[+][2020-04-04 18:54:43.049158] Training iteration 117 (batch 1 of epoch 29).\n",
            "Loss: 0.135900\n",
            "[+][2020-04-04 18:54:43.326342] Training iteration 118 (batch 2 of epoch 29).\n",
            "Loss: 0.121867\n",
            "[+][2020-04-04 18:54:43.602054] Training iteration 119 (batch 3 of epoch 29).\n",
            "Loss: 0.120149\n",
            "[+][2020-04-04 18:54:43.880603] Training iteration 120 (batch 4 of epoch 29).\n",
            "Loss: 0.129846\n",
            "[+][2020-04-04 18:54:44.156396] Training iteration 121 (batch 1 of epoch 30).\n",
            "Loss: 0.136740\n",
            "[+][2020-04-04 18:54:44.425416] Training iteration 122 (batch 2 of epoch 30).\n",
            "Loss: 0.137896\n",
            "[+][2020-04-04 18:54:44.699665] Training iteration 123 (batch 3 of epoch 30).\n",
            "Loss: 0.130462\n",
            "[+][2020-04-04 18:54:44.983332] Training iteration 124 (batch 4 of epoch 30).\n",
            "Loss: 0.117752\n",
            "[+][2020-04-04 18:54:45.264522] Training iteration 125 (batch 1 of epoch 31).\n",
            "Loss: 0.122942\n",
            "[+][2020-04-04 18:54:45.538999] Training iteration 126 (batch 2 of epoch 31).\n",
            "Loss: 0.131877\n",
            "[+][2020-04-04 18:54:45.812764] Training iteration 127 (batch 3 of epoch 31).\n",
            "Loss: 0.120695\n",
            "[+][2020-04-04 18:54:46.093959] Training iteration 128 (batch 4 of epoch 31).\n",
            "Loss: 0.120346\n",
            "[+][2020-04-04 18:54:46.371400] Training iteration 129 (batch 1 of epoch 32).\n",
            "Loss: 0.116380\n",
            "[+][2020-04-04 18:54:46.642344] Training iteration 130 (batch 2 of epoch 32).\n",
            "Loss: 0.123843\n",
            "[+][2020-04-04 18:54:46.922370] Training iteration 131 (batch 3 of epoch 32).\n",
            "Loss: 0.119890\n",
            "[+][2020-04-04 18:54:47.217928] Training iteration 132 (batch 4 of epoch 32).\n",
            "Loss: 0.118183\n",
            "[+][2020-04-04 18:54:47.489604] Training iteration 133 (batch 1 of epoch 33).\n",
            "Loss: 0.117474\n",
            "[+][2020-04-04 18:54:47.764220] Training iteration 134 (batch 2 of epoch 33).\n",
            "Loss: 0.113926\n",
            "[+][2020-04-04 18:54:48.068140] Training iteration 135 (batch 3 of epoch 33).\n",
            "Loss: 0.112414\n",
            "[+][2020-04-04 18:54:48.357196] Training iteration 136 (batch 4 of epoch 33).\n",
            "Loss: 0.133036\n",
            "[+][2020-04-04 18:54:48.632052] Training iteration 137 (batch 1 of epoch 34).\n",
            "Loss: 0.125269\n",
            "[+][2020-04-04 18:54:48.909810] Training iteration 138 (batch 2 of epoch 34).\n",
            "Loss: 0.126974\n",
            "[+][2020-04-04 18:54:49.205897] Training iteration 139 (batch 3 of epoch 34).\n",
            "Loss: 0.125017\n",
            "[+][2020-04-04 18:54:49.478020] Training iteration 140 (batch 4 of epoch 34).\n",
            "Loss: 0.127668\n",
            "[+][2020-04-04 18:54:49.758154] Training iteration 141 (batch 1 of epoch 35).\n",
            "Loss: 0.134778\n",
            "[+][2020-04-04 18:54:50.040875] Training iteration 142 (batch 2 of epoch 35).\n",
            "Loss: 0.122652\n",
            "[+][2020-04-04 18:54:50.321759] Training iteration 143 (batch 3 of epoch 35).\n",
            "Loss: 0.121220\n",
            "[+][2020-04-04 18:54:50.595457] Training iteration 144 (batch 4 of epoch 35).\n",
            "Loss: 0.123745\n",
            "[+][2020-04-04 18:54:50.881040] Training iteration 145 (batch 1 of epoch 36).\n",
            "Loss: 0.129664\n",
            "[+][2020-04-04 18:54:51.167537] Training iteration 146 (batch 2 of epoch 36).\n",
            "Loss: 0.128484\n",
            "[+][2020-04-04 18:54:51.447728] Training iteration 147 (batch 3 of epoch 36).\n",
            "Loss: 0.126769\n",
            "[+][2020-04-04 18:54:51.724745] Training iteration 148 (batch 4 of epoch 36).\n",
            "Loss: 0.111770\n",
            "[+][2020-04-04 18:54:52.007341] Training iteration 149 (batch 1 of epoch 37).\n",
            "Loss: 0.123424\n",
            "[+][2020-04-04 18:54:52.319625] Training iteration 150 (batch 2 of epoch 37).\n",
            "Loss: 0.130432\n",
            "[+][2020-04-04 18:54:52.611709] Training iteration 151 (batch 3 of epoch 37).\n",
            "Loss: 0.118060\n",
            "[+][2020-04-04 18:54:52.902876] Training iteration 152 (batch 4 of epoch 37).\n",
            "Loss: 0.116534\n",
            "[+][2020-04-04 18:54:53.205648] Training iteration 153 (batch 1 of epoch 38).\n",
            "Loss: 0.128461\n",
            "[+][2020-04-04 18:54:53.511082] Training iteration 154 (batch 2 of epoch 38).\n",
            "Loss: 0.121234\n",
            "[+][2020-04-04 18:54:53.800200] Training iteration 155 (batch 3 of epoch 38).\n",
            "Loss: 0.108948\n",
            "[+][2020-04-04 18:54:54.100808] Training iteration 156 (batch 4 of epoch 38).\n",
            "Loss: 0.124980\n",
            "[+][2020-04-04 18:54:54.414045] Training iteration 157 (batch 1 of epoch 39).\n",
            "Loss: 0.124193\n",
            "[+][2020-04-04 18:54:54.705715] Training iteration 158 (batch 2 of epoch 39).\n",
            "Loss: 0.111004\n",
            "[+][2020-04-04 18:54:54.997551] Training iteration 159 (batch 3 of epoch 39).\n",
            "Loss: 0.124118\n",
            "[+][2020-04-04 18:54:55.303033] Training iteration 160 (batch 4 of epoch 39).\n",
            "Loss: 0.127235\n",
            "[+][2020-04-04 18:54:55.589817] Training iteration 161 (batch 1 of epoch 40).\n",
            "Loss: 0.136239\n",
            "[+][2020-04-04 18:54:55.883838] Training iteration 162 (batch 2 of epoch 40).\n",
            "Loss: 0.116323\n",
            "[+][2020-04-04 18:54:56.168070] Training iteration 163 (batch 3 of epoch 40).\n",
            "Loss: 0.121902\n",
            "[+][2020-04-04 18:54:56.449433] Training iteration 164 (batch 4 of epoch 40).\n",
            "Loss: 0.132102\n",
            "[+][2020-04-04 18:54:56.736331] Training iteration 165 (batch 1 of epoch 41).\n",
            "Loss: 0.123885\n",
            "[+][2020-04-04 18:54:57.034193] Training iteration 166 (batch 2 of epoch 41).\n",
            "Loss: 0.126085\n",
            "[+][2020-04-04 18:54:57.356380] Training iteration 167 (batch 3 of epoch 41).\n",
            "Loss: 0.130917\n",
            "[+][2020-04-04 18:54:57.642575] Training iteration 168 (batch 4 of epoch 41).\n",
            "Loss: 0.121063\n",
            "[+][2020-04-04 18:54:57.935998] Training iteration 169 (batch 1 of epoch 42).\n",
            "Loss: 0.119397\n",
            "[+][2020-04-04 18:54:58.231510] Training iteration 170 (batch 2 of epoch 42).\n",
            "Loss: 0.121457\n",
            "[+][2020-04-04 18:54:58.527755] Training iteration 171 (batch 3 of epoch 42).\n",
            "Loss: 0.124496\n",
            "[+][2020-04-04 18:54:58.807014] Training iteration 172 (batch 4 of epoch 42).\n",
            "Loss: 0.127928\n",
            "[+][2020-04-04 18:54:59.098365] Training iteration 173 (batch 1 of epoch 43).\n",
            "Loss: 0.128144\n",
            "[+][2020-04-04 18:54:59.405534] Training iteration 174 (batch 2 of epoch 43).\n",
            "Loss: 0.116528\n",
            "[+][2020-04-04 18:54:59.682073] Training iteration 175 (batch 3 of epoch 43).\n",
            "Loss: 0.116249\n",
            "[+][2020-04-04 18:54:59.953415] Training iteration 176 (batch 4 of epoch 43).\n",
            "Loss: 0.120605\n",
            "[+][2020-04-04 18:55:00.256340] Training iteration 177 (batch 1 of epoch 44).\n",
            "Loss: 0.127769\n",
            "[+][2020-04-04 18:55:00.545286] Training iteration 178 (batch 2 of epoch 44).\n",
            "Loss: 0.129632\n",
            "[+][2020-04-04 18:55:00.815373] Training iteration 179 (batch 3 of epoch 44).\n",
            "Loss: 0.119934\n",
            "[+][2020-04-04 18:55:01.101935] Training iteration 180 (batch 4 of epoch 44).\n",
            "Loss: 0.112176\n",
            "[+][2020-04-04 18:55:01.436031] Training iteration 181 (batch 1 of epoch 45).\n",
            "Loss: 0.121767\n",
            "[+][2020-04-04 18:55:01.710395] Training iteration 182 (batch 2 of epoch 45).\n",
            "Loss: 0.114805\n",
            "[+][2020-04-04 18:55:01.987133] Training iteration 183 (batch 3 of epoch 45).\n",
            "Loss: 0.102416\n",
            "[+][2020-04-04 18:55:02.283941] Training iteration 184 (batch 4 of epoch 45).\n",
            "Loss: 0.109791\n",
            "[+][2020-04-04 18:55:02.566948] Training iteration 185 (batch 1 of epoch 46).\n",
            "Loss: 0.128700\n",
            "[+][2020-04-04 18:55:02.839431] Training iteration 186 (batch 2 of epoch 46).\n",
            "Loss: 0.129304\n",
            "[+][2020-04-04 18:55:03.126616] Training iteration 187 (batch 3 of epoch 46).\n",
            "Loss: 0.119586\n",
            "[+][2020-04-04 18:55:03.417478] Training iteration 188 (batch 4 of epoch 46).\n",
            "Loss: 0.109070\n",
            "[+][2020-04-04 18:55:03.690347] Training iteration 189 (batch 1 of epoch 47).\n",
            "Loss: 0.115936\n",
            "[+][2020-04-04 18:55:03.967196] Training iteration 190 (batch 2 of epoch 47).\n",
            "Loss: 0.131177\n",
            "[+][2020-04-04 18:55:04.242619] Training iteration 191 (batch 3 of epoch 47).\n",
            "Loss: 0.124697\n",
            "[+][2020-04-04 18:55:04.530456] Training iteration 192 (batch 4 of epoch 47).\n",
            "Loss: 0.108708\n",
            "[+][2020-04-04 18:55:04.827074] Training iteration 193 (batch 1 of epoch 48).\n",
            "Loss: 0.104641\n",
            "[+][2020-04-04 18:55:05.098335] Training iteration 194 (batch 2 of epoch 48).\n",
            "Loss: 0.126468\n",
            "[+][2020-04-04 18:55:05.387589] Training iteration 195 (batch 3 of epoch 48).\n",
            "Loss: 0.129780\n",
            "[+][2020-04-04 18:55:05.668510] Training iteration 196 (batch 4 of epoch 48).\n",
            "Loss: 0.120541\n",
            "[+][2020-04-04 18:55:05.944320] Training iteration 197 (batch 1 of epoch 49).\n",
            "Loss: 0.126121\n",
            "[+][2020-04-04 18:55:06.229657] Training iteration 198 (batch 2 of epoch 49).\n",
            "Loss: 0.125962\n",
            "[+][2020-04-04 18:55:06.514590] Training iteration 199 (batch 3 of epoch 49).\n",
            "Loss: 0.115453\n",
            "[+][2020-04-04 18:55:06.790038] Training iteration 200 (batch 4 of epoch 49).\n",
            "Loss: 0.113301\n",
            "[+][2020-04-04 18:55:07.067781] Training iteration 201 (batch 1 of epoch 50).\n",
            "Loss: 0.124976\n",
            "[+][2020-04-04 18:55:07.344993] Training iteration 202 (batch 2 of epoch 50).\n",
            "Loss: 0.113362\n",
            "[+][2020-04-04 18:55:07.631835] Training iteration 203 (batch 3 of epoch 50).\n",
            "Loss: 0.135159\n",
            "[+][2020-04-04 18:55:08.028321] Training iteration 204 (batch 4 of epoch 50).\n",
            "Loss: 0.129804\n",
            "[+][2020-04-04 18:55:08.310138] Training iteration 205 (batch 1 of epoch 51).\n",
            "Loss: 0.109693\n",
            "[+][2020-04-04 18:55:08.606335] Training iteration 206 (batch 2 of epoch 51).\n",
            "Loss: 0.117362\n",
            "[+][2020-04-04 18:55:08.881164] Training iteration 207 (batch 3 of epoch 51).\n",
            "Loss: 0.114089\n",
            "[+][2020-04-04 18:55:09.164643] Training iteration 208 (batch 4 of epoch 51).\n",
            "Loss: 0.105874\n",
            "[+][2020-04-04 18:55:09.448849] Training iteration 209 (batch 1 of epoch 52).\n",
            "Loss: 0.105523\n",
            "[+][2020-04-04 18:55:09.719154] Training iteration 210 (batch 2 of epoch 52).\n",
            "Loss: 0.107224\n",
            "[+][2020-04-04 18:55:09.996687] Training iteration 211 (batch 3 of epoch 52).\n",
            "Loss: 0.119532\n",
            "[+][2020-04-04 18:55:10.299095] Training iteration 212 (batch 4 of epoch 52).\n",
            "Loss: 0.118108\n",
            "[+][2020-04-04 18:55:10.581528] Training iteration 213 (batch 1 of epoch 53).\n",
            "Loss: 0.110154\n",
            "[+][2020-04-04 18:55:10.863991] Training iteration 214 (batch 2 of epoch 53).\n",
            "Loss: 0.120114\n",
            "[+][2020-04-04 18:55:11.160370] Training iteration 215 (batch 3 of epoch 53).\n",
            "Loss: 0.130151\n",
            "[+][2020-04-04 18:55:11.443522] Training iteration 216 (batch 4 of epoch 53).\n",
            "Loss: 0.116212\n",
            "[+][2020-04-04 18:55:11.728634] Training iteration 217 (batch 1 of epoch 54).\n",
            "Loss: 0.123924\n",
            "[+][2020-04-04 18:55:12.010775] Training iteration 218 (batch 2 of epoch 54).\n",
            "Loss: 0.126049\n",
            "[+][2020-04-04 18:55:12.309189] Training iteration 219 (batch 3 of epoch 54).\n",
            "Loss: 0.109603\n",
            "[+][2020-04-04 18:55:12.594126] Training iteration 220 (batch 4 of epoch 54).\n",
            "Loss: 0.102978\n",
            "[+][2020-04-04 18:55:12.873719] Training iteration 221 (batch 1 of epoch 55).\n",
            "Loss: 0.119388\n",
            "[+][2020-04-04 18:55:13.147628] Training iteration 222 (batch 2 of epoch 55).\n",
            "Loss: 0.112825\n",
            "[+][2020-04-04 18:55:13.424632] Training iteration 223 (batch 3 of epoch 55).\n",
            "Loss: 0.107455\n",
            "[+][2020-04-04 18:55:13.716464] Training iteration 224 (batch 4 of epoch 55).\n",
            "Loss: 0.116370\n",
            "[+][2020-04-04 18:55:13.993214] Training iteration 225 (batch 1 of epoch 56).\n",
            "Loss: 0.128832\n",
            "[+][2020-04-04 18:55:14.264979] Training iteration 226 (batch 2 of epoch 56).\n",
            "Loss: 0.134311\n",
            "[+][2020-04-04 18:55:14.558370] Training iteration 227 (batch 3 of epoch 56).\n",
            "Loss: 0.127704\n",
            "[+][2020-04-04 18:55:14.829214] Training iteration 228 (batch 4 of epoch 56).\n",
            "Loss: 0.118968\n",
            "[+][2020-04-04 18:55:15.111134] Training iteration 229 (batch 1 of epoch 57).\n",
            "Loss: 0.148156\n",
            "[+][2020-04-04 18:55:15.391500] Training iteration 230 (batch 2 of epoch 57).\n",
            "Loss: 0.120457\n",
            "[+][2020-04-04 18:55:15.677744] Training iteration 231 (batch 3 of epoch 57).\n",
            "Loss: 0.117912\n",
            "[+][2020-04-04 18:55:15.953889] Training iteration 232 (batch 4 of epoch 57).\n",
            "Loss: 0.122526\n",
            "[+][2020-04-04 18:55:16.233334] Training iteration 233 (batch 1 of epoch 58).\n",
            "Loss: 0.116605\n",
            "[+][2020-04-04 18:55:16.516097] Training iteration 234 (batch 2 of epoch 58).\n",
            "Loss: 0.121756\n",
            "[+][2020-04-04 18:55:16.799578] Training iteration 235 (batch 3 of epoch 58).\n",
            "Loss: 0.105979\n",
            "[+][2020-04-04 18:55:17.097049] Training iteration 236 (batch 4 of epoch 58).\n",
            "Loss: 0.112646\n",
            "[+][2020-04-04 18:55:17.372193] Training iteration 237 (batch 1 of epoch 59).\n",
            "Loss: 0.116451\n",
            "[+][2020-04-04 18:55:17.659385] Training iteration 238 (batch 2 of epoch 59).\n",
            "Loss: 0.119174\n",
            "[+][2020-04-04 18:55:17.934010] Training iteration 239 (batch 3 of epoch 59).\n",
            "Loss: 0.107064\n",
            "[+][2020-04-04 18:55:18.211583] Training iteration 240 (batch 4 of epoch 59).\n",
            "Loss: 0.129628\n",
            "[+][2020-04-04 18:55:18.487392] Training iteration 241 (batch 1 of epoch 60).\n",
            "Loss: 0.103734\n",
            "[+][2020-04-04 18:55:18.778320] Training iteration 242 (batch 2 of epoch 60).\n",
            "Loss: 0.107596\n",
            "[+][2020-04-04 18:55:19.050800] Training iteration 243 (batch 3 of epoch 60).\n",
            "Loss: 0.099951\n",
            "[+][2020-04-04 18:55:19.327786] Training iteration 244 (batch 4 of epoch 60).\n",
            "Loss: 0.109208\n",
            "[+][2020-04-04 18:55:19.616049] Training iteration 245 (batch 1 of epoch 61).\n",
            "Loss: 0.122560\n",
            "[+][2020-04-04 18:55:19.889959] Training iteration 246 (batch 2 of epoch 61).\n",
            "Loss: 0.109862\n",
            "[+][2020-04-04 18:55:20.196068] Training iteration 247 (batch 3 of epoch 61).\n",
            "Loss: 0.091552\n",
            "[+][2020-04-04 18:55:20.472829] Training iteration 248 (batch 4 of epoch 61).\n",
            "Loss: 0.101821\n",
            "[+][2020-04-04 18:55:20.756087] Training iteration 249 (batch 1 of epoch 62).\n",
            "Loss: 0.103484\n",
            "[+][2020-04-04 18:55:21.034097] Training iteration 250 (batch 2 of epoch 62).\n",
            "Loss: 0.107676\n",
            "[+][2020-04-04 18:55:21.312855] Training iteration 251 (batch 3 of epoch 62).\n",
            "Loss: 0.110001\n",
            "[+][2020-04-04 18:55:21.592102] Training iteration 252 (batch 4 of epoch 62).\n",
            "Loss: 0.113969\n",
            "[+][2020-04-04 18:55:21.876115] Training iteration 253 (batch 1 of epoch 63).\n",
            "Loss: 0.117051\n",
            "[+][2020-04-04 18:55:22.180838] Training iteration 254 (batch 2 of epoch 63).\n",
            "Loss: 0.118162\n",
            "[+][2020-04-04 18:55:22.451261] Training iteration 255 (batch 3 of epoch 63).\n",
            "Loss: 0.117897\n",
            "[+][2020-04-04 18:55:22.745133] Training iteration 256 (batch 4 of epoch 63).\n",
            "Loss: 0.106214\n",
            "[+][2020-04-04 18:55:23.023814] Training iteration 257 (batch 1 of epoch 64).\n",
            "Loss: 0.113217\n",
            "[+][2020-04-04 18:55:23.305540] Training iteration 258 (batch 2 of epoch 64).\n",
            "Loss: 0.099827\n",
            "[+][2020-04-04 18:55:23.582684] Training iteration 259 (batch 3 of epoch 64).\n",
            "Loss: 0.103861\n",
            "[+][2020-04-04 18:55:23.864955] Training iteration 260 (batch 4 of epoch 64).\n",
            "Loss: 0.104845\n",
            "[+][2020-04-04 18:55:24.141165] Training iteration 261 (batch 1 of epoch 65).\n",
            "Loss: 0.118009\n",
            "[+][2020-04-04 18:55:24.411935] Training iteration 262 (batch 2 of epoch 65).\n",
            "Loss: 0.101412\n",
            "[+][2020-04-04 18:55:24.705642] Training iteration 263 (batch 3 of epoch 65).\n",
            "Loss: 0.114949\n",
            "[+][2020-04-04 18:55:24.984338] Training iteration 264 (batch 4 of epoch 65).\n",
            "Loss: 0.113949\n",
            "[+][2020-04-04 18:55:25.279962] Training iteration 265 (batch 1 of epoch 66).\n",
            "Loss: 0.121932\n",
            "[+][2020-04-04 18:55:25.553222] Training iteration 266 (batch 2 of epoch 66).\n",
            "Loss: 0.104150\n",
            "[+][2020-04-04 18:55:25.839902] Training iteration 267 (batch 3 of epoch 66).\n",
            "Loss: 0.122514\n",
            "[+][2020-04-04 18:55:26.145341] Training iteration 268 (batch 4 of epoch 66).\n",
            "Loss: 0.121915\n",
            "[+][2020-04-04 18:55:26.422817] Training iteration 269 (batch 1 of epoch 67).\n",
            "Loss: 0.119087\n",
            "[+][2020-04-04 18:55:26.705081] Training iteration 270 (batch 2 of epoch 67).\n",
            "Loss: 0.116820\n",
            "[+][2020-04-04 18:55:26.985194] Training iteration 271 (batch 3 of epoch 67).\n",
            "Loss: 0.118731\n",
            "[+][2020-04-04 18:55:27.281713] Training iteration 272 (batch 4 of epoch 67).\n",
            "Loss: 0.117242\n",
            "[+][2020-04-04 18:55:27.568311] Training iteration 273 (batch 1 of epoch 68).\n",
            "Loss: 0.105170\n",
            "[+][2020-04-04 18:55:27.850414] Training iteration 274 (batch 2 of epoch 68).\n",
            "Loss: 0.105368\n",
            "[+][2020-04-04 18:55:28.123462] Training iteration 275 (batch 3 of epoch 68).\n",
            "Loss: 0.104374\n",
            "[+][2020-04-04 18:55:28.398637] Training iteration 276 (batch 4 of epoch 68).\n",
            "Loss: 0.120862\n",
            "[+][2020-04-04 18:55:28.680245] Training iteration 277 (batch 1 of epoch 69).\n",
            "Loss: 0.119419\n",
            "[+][2020-04-04 18:55:28.957631] Training iteration 278 (batch 2 of epoch 69).\n",
            "Loss: 0.105029\n",
            "[+][2020-04-04 18:55:29.241684] Training iteration 279 (batch 3 of epoch 69).\n",
            "Loss: 0.115345\n",
            "[+][2020-04-04 18:55:29.516445] Training iteration 280 (batch 4 of epoch 69).\n",
            "Loss: 0.101353\n",
            "[+][2020-04-04 18:55:29.791762] Training iteration 281 (batch 1 of epoch 70).\n",
            "Loss: 0.119538\n",
            "[+][2020-04-04 18:55:30.083596] Training iteration 282 (batch 2 of epoch 70).\n",
            "Loss: 0.122985\n",
            "[+][2020-04-04 18:55:30.358474] Training iteration 283 (batch 3 of epoch 70).\n",
            "Loss: 0.139879\n",
            "[+][2020-04-04 18:55:30.629837] Training iteration 284 (batch 4 of epoch 70).\n",
            "Loss: 0.128181\n",
            "[+][2020-04-04 18:55:30.915879] Training iteration 285 (batch 1 of epoch 71).\n",
            "Loss: 0.122638\n",
            "[+][2020-04-04 18:55:31.215623] Training iteration 286 (batch 2 of epoch 71).\n",
            "Loss: 0.115872\n",
            "[+][2020-04-04 18:55:31.484738] Training iteration 287 (batch 3 of epoch 71).\n",
            "Loss: 0.111015\n",
            "[+][2020-04-04 18:55:31.760777] Training iteration 288 (batch 4 of epoch 71).\n",
            "Loss: 0.109549\n",
            "[+][2020-04-04 18:55:32.041157] Training iteration 289 (batch 1 of epoch 72).\n",
            "Loss: 0.098646\n",
            "[+][2020-04-04 18:55:32.314786] Training iteration 290 (batch 2 of epoch 72).\n",
            "Loss: 0.109694\n",
            "[+][2020-04-04 18:55:32.590292] Training iteration 291 (batch 3 of epoch 72).\n",
            "Loss: 0.093086\n",
            "[+][2020-04-04 18:55:32.880195] Training iteration 292 (batch 4 of epoch 72).\n",
            "Loss: 0.122916\n",
            "[+][2020-04-04 18:55:33.163786] Training iteration 293 (batch 1 of epoch 73).\n",
            "Loss: 0.116253\n",
            "[+][2020-04-04 18:55:33.442202] Training iteration 294 (batch 2 of epoch 73).\n",
            "Loss: 0.111166\n",
            "[+][2020-04-04 18:55:33.721752] Training iteration 295 (batch 3 of epoch 73).\n",
            "Loss: 0.114761\n",
            "[+][2020-04-04 18:55:33.998002] Training iteration 296 (batch 4 of epoch 73).\n",
            "Loss: 0.111336\n",
            "[+][2020-04-04 18:55:34.274523] Training iteration 297 (batch 1 of epoch 74).\n",
            "Loss: 0.104859\n",
            "[+][2020-04-04 18:55:34.549061] Training iteration 298 (batch 2 of epoch 74).\n",
            "Loss: 0.104907\n",
            "[+][2020-04-04 18:55:34.825078] Training iteration 299 (batch 3 of epoch 74).\n",
            "Loss: 0.124777\n",
            "[+][2020-04-04 18:55:35.101857] Training iteration 300 (batch 4 of epoch 74).\n",
            "Loss: 0.111973\n",
            "[+][2020-04-04 18:55:35.378391] Training iteration 301 (batch 1 of epoch 75).\n",
            "Loss: 0.121174\n",
            "[+][2020-04-04 18:55:35.661670] Training iteration 302 (batch 2 of epoch 75).\n",
            "Loss: 0.122437\n",
            "[+][2020-04-04 18:55:35.939409] Training iteration 303 (batch 3 of epoch 75).\n",
            "Loss: 0.134159\n",
            "[+][2020-04-04 18:55:36.216110] Training iteration 304 (batch 4 of epoch 75).\n",
            "Loss: 0.124206\n",
            "[+][2020-04-04 18:55:36.494601] Training iteration 305 (batch 1 of epoch 76).\n",
            "Loss: 0.118873\n",
            "[+][2020-04-04 18:55:36.770030] Training iteration 306 (batch 2 of epoch 76).\n",
            "Loss: 0.102550\n",
            "[+][2020-04-04 18:55:37.046258] Training iteration 307 (batch 3 of epoch 76).\n",
            "Loss: 0.113276\n",
            "[+][2020-04-04 18:55:37.323032] Training iteration 308 (batch 4 of epoch 76).\n",
            "Loss: 0.104464\n",
            "[+][2020-04-04 18:55:37.596368] Training iteration 309 (batch 1 of epoch 77).\n",
            "Loss: 0.099830\n",
            "[+][2020-04-04 18:55:37.885243] Training iteration 310 (batch 2 of epoch 77).\n",
            "Loss: 0.127239\n",
            "[+][2020-04-04 18:55:38.168019] Training iteration 311 (batch 3 of epoch 77).\n",
            "Loss: 0.110658\n",
            "[+][2020-04-04 18:55:38.437779] Training iteration 312 (batch 4 of epoch 77).\n",
            "Loss: 0.103496\n",
            "[+][2020-04-04 18:55:38.721210] Training iteration 313 (batch 1 of epoch 78).\n",
            "Loss: 0.114776\n",
            "[+][2020-04-04 18:55:39.000063] Training iteration 314 (batch 2 of epoch 78).\n",
            "Loss: 0.132916\n",
            "[+][2020-04-04 18:55:39.293769] Training iteration 315 (batch 3 of epoch 78).\n",
            "Loss: 0.129072\n",
            "[+][2020-04-04 18:55:39.570324] Training iteration 316 (batch 4 of epoch 78).\n",
            "Loss: 0.111160\n",
            "[+][2020-04-04 18:55:39.847853] Training iteration 317 (batch 1 of epoch 79).\n",
            "Loss: 0.112459\n",
            "[+][2020-04-04 18:55:40.149853] Training iteration 318 (batch 2 of epoch 79).\n",
            "Loss: 0.093418\n",
            "[+][2020-04-04 18:55:40.424992] Training iteration 319 (batch 3 of epoch 79).\n",
            "Loss: 0.116881\n",
            "[+][2020-04-04 18:55:40.701540] Training iteration 320 (batch 4 of epoch 79).\n",
            "Loss: 0.113120\n",
            "[+][2020-04-04 18:55:40.991856] Training iteration 321 (batch 1 of epoch 80).\n",
            "Loss: 0.103871\n",
            "[+][2020-04-04 18:55:41.275156] Training iteration 322 (batch 2 of epoch 80).\n",
            "Loss: 0.121083\n",
            "[+][2020-04-04 18:55:41.547817] Training iteration 323 (batch 3 of epoch 80).\n",
            "Loss: 0.119996\n",
            "[+][2020-04-04 18:55:41.829023] Training iteration 324 (batch 4 of epoch 80).\n",
            "Loss: 0.112911\n",
            "[+][2020-04-04 18:55:42.109396] Training iteration 325 (batch 1 of epoch 81).\n",
            "Loss: 0.112187\n",
            "[+][2020-04-04 18:55:42.392728] Training iteration 326 (batch 2 of epoch 81).\n",
            "Loss: 0.103648\n",
            "[+][2020-04-04 18:55:42.672779] Training iteration 327 (batch 3 of epoch 81).\n",
            "Loss: 0.098298\n",
            "[+][2020-04-04 18:55:42.949701] Training iteration 328 (batch 4 of epoch 81).\n",
            "Loss: 0.121951\n",
            "[+][2020-04-04 18:55:43.232042] Training iteration 329 (batch 1 of epoch 82).\n",
            "Loss: 0.121734\n",
            "[+][2020-04-04 18:55:43.517702] Training iteration 330 (batch 2 of epoch 82).\n",
            "Loss: 0.114581\n",
            "[+][2020-04-04 18:55:43.798872] Training iteration 331 (batch 3 of epoch 82).\n",
            "Loss: 0.108810\n",
            "[+][2020-04-04 18:55:44.077636] Training iteration 332 (batch 4 of epoch 82).\n",
            "Loss: 0.129336\n",
            "[+][2020-04-04 18:55:44.361909] Training iteration 333 (batch 1 of epoch 83).\n",
            "Loss: 0.113642\n",
            "[+][2020-04-04 18:55:44.637440] Training iteration 334 (batch 2 of epoch 83).\n",
            "Loss: 0.124174\n",
            "[+][2020-04-04 18:55:44.917688] Training iteration 335 (batch 3 of epoch 83).\n",
            "Loss: 0.110698\n",
            "[+][2020-04-04 18:55:45.197815] Training iteration 336 (batch 4 of epoch 83).\n",
            "Loss: 0.110224\n",
            "[+][2020-04-04 18:55:45.467793] Training iteration 337 (batch 1 of epoch 84).\n",
            "Loss: 0.100412\n",
            "[+][2020-04-04 18:55:45.740431] Training iteration 338 (batch 2 of epoch 84).\n",
            "Loss: 0.104248\n",
            "[+][2020-04-04 18:55:46.017708] Training iteration 339 (batch 3 of epoch 84).\n",
            "Loss: 0.104180\n",
            "[+][2020-04-04 18:55:46.295082] Training iteration 340 (batch 4 of epoch 84).\n",
            "Loss: 0.111683\n",
            "[+][2020-04-04 18:55:46.579021] Training iteration 341 (batch 1 of epoch 85).\n",
            "Loss: 0.102526\n",
            "[+][2020-04-04 18:55:46.858135] Training iteration 342 (batch 2 of epoch 85).\n",
            "Loss: 0.103823\n",
            "[+][2020-04-04 18:55:47.129763] Training iteration 343 (batch 3 of epoch 85).\n",
            "Loss: 0.113996\n",
            "[+][2020-04-04 18:55:47.409495] Training iteration 344 (batch 4 of epoch 85).\n",
            "Loss: 0.103793\n",
            "[+][2020-04-04 18:55:47.681799] Training iteration 345 (batch 1 of epoch 86).\n",
            "Loss: 0.108605\n",
            "[+][2020-04-04 18:55:47.954716] Training iteration 346 (batch 2 of epoch 86).\n",
            "Loss: 0.097591\n",
            "[+][2020-04-04 18:55:48.234930] Training iteration 347 (batch 3 of epoch 86).\n",
            "Loss: 0.104325\n",
            "[+][2020-04-04 18:55:48.513983] Training iteration 348 (batch 4 of epoch 86).\n",
            "Loss: 0.108983\n",
            "[+][2020-04-04 18:55:48.790292] Training iteration 349 (batch 1 of epoch 87).\n",
            "Loss: 0.113851\n",
            "[+][2020-04-04 18:55:49.084906] Training iteration 350 (batch 2 of epoch 87).\n",
            "Loss: 0.105214\n",
            "[+][2020-04-04 18:55:49.364970] Training iteration 351 (batch 3 of epoch 87).\n",
            "Loss: 0.105529\n",
            "[+][2020-04-04 18:55:49.636703] Training iteration 352 (batch 4 of epoch 87).\n",
            "Loss: 0.106867\n",
            "[+][2020-04-04 18:55:49.915620] Training iteration 353 (batch 1 of epoch 88).\n",
            "Loss: 0.104759\n",
            "[+][2020-04-04 18:55:50.189048] Training iteration 354 (batch 2 of epoch 88).\n",
            "Loss: 0.104407\n",
            "[+][2020-04-04 18:55:50.485587] Training iteration 355 (batch 3 of epoch 88).\n",
            "Loss: 0.109709\n",
            "[+][2020-04-04 18:55:50.766253] Training iteration 356 (batch 4 of epoch 88).\n",
            "Loss: 0.107472\n",
            "[+][2020-04-04 18:55:51.049078] Training iteration 357 (batch 1 of epoch 89).\n",
            "Loss: 0.096123\n",
            "[+][2020-04-04 18:55:51.329884] Training iteration 358 (batch 2 of epoch 89).\n",
            "Loss: 0.099483\n",
            "[+][2020-04-04 18:55:51.604511] Training iteration 359 (batch 3 of epoch 89).\n",
            "Loss: 0.112689\n",
            "[+][2020-04-04 18:55:51.876932] Training iteration 360 (batch 4 of epoch 89).\n",
            "Loss: 0.108010\n",
            "[+][2020-04-04 18:55:52.157005] Training iteration 361 (batch 1 of epoch 90).\n",
            "Loss: 0.112065\n",
            "[+][2020-04-04 18:55:52.437300] Training iteration 362 (batch 2 of epoch 90).\n",
            "Loss: 0.097742\n",
            "[+][2020-04-04 18:55:52.730116] Training iteration 363 (batch 3 of epoch 90).\n",
            "Loss: 0.114505\n",
            "[+][2020-04-04 18:55:53.013134] Training iteration 364 (batch 4 of epoch 90).\n",
            "Loss: 0.114270\n",
            "[+][2020-04-04 18:55:53.316595] Training iteration 365 (batch 1 of epoch 91).\n",
            "Loss: 0.110771\n",
            "[+][2020-04-04 18:55:53.590409] Training iteration 366 (batch 2 of epoch 91).\n",
            "Loss: 0.108676\n",
            "[+][2020-04-04 18:55:53.865765] Training iteration 367 (batch 3 of epoch 91).\n",
            "Loss: 0.115293\n",
            "[+][2020-04-04 18:55:54.151115] Training iteration 368 (batch 4 of epoch 91).\n",
            "Loss: 0.120719\n",
            "[+][2020-04-04 18:55:54.430715] Training iteration 369 (batch 1 of epoch 92).\n",
            "Loss: 0.118134\n",
            "[+][2020-04-04 18:55:54.702041] Training iteration 370 (batch 2 of epoch 92).\n",
            "Loss: 0.103608\n",
            "[+][2020-04-04 18:55:54.990506] Training iteration 371 (batch 3 of epoch 92).\n",
            "Loss: 0.113880\n",
            "[+][2020-04-04 18:55:55.263102] Training iteration 372 (batch 4 of epoch 92).\n",
            "Loss: 0.116947\n",
            "[+][2020-04-04 18:55:55.541548] Training iteration 373 (batch 1 of epoch 93).\n",
            "Loss: 0.102281\n",
            "[+][2020-04-04 18:55:55.813256] Training iteration 374 (batch 2 of epoch 93).\n",
            "Loss: 0.118880\n",
            "[+][2020-04-04 18:55:56.094028] Training iteration 375 (batch 3 of epoch 93).\n",
            "Loss: 0.110753\n",
            "[+][2020-04-04 18:55:56.372829] Training iteration 376 (batch 4 of epoch 93).\n",
            "Loss: 0.115323\n",
            "[+][2020-04-04 18:55:56.641474] Training iteration 377 (batch 1 of epoch 94).\n",
            "Loss: 0.105319\n",
            "[+][2020-04-04 18:55:56.918673] Training iteration 378 (batch 2 of epoch 94).\n",
            "Loss: 0.099788\n",
            "[+][2020-04-04 18:55:57.199015] Training iteration 379 (batch 3 of epoch 94).\n",
            "Loss: 0.102001\n",
            "[+][2020-04-04 18:55:57.475710] Training iteration 380 (batch 4 of epoch 94).\n",
            "Loss: 0.116872\n",
            "[+][2020-04-04 18:55:57.761477] Training iteration 381 (batch 1 of epoch 95).\n",
            "Loss: 0.118831\n",
            "[+][2020-04-04 18:55:58.036482] Training iteration 382 (batch 2 of epoch 95).\n",
            "Loss: 0.103877\n",
            "[+][2020-04-04 18:55:58.308816] Training iteration 383 (batch 3 of epoch 95).\n",
            "Loss: 0.092165\n",
            "[+][2020-04-04 18:55:58.590013] Training iteration 384 (batch 4 of epoch 95).\n",
            "Loss: 0.112172\n",
            "[+][2020-04-04 18:55:58.865709] Training iteration 385 (batch 1 of epoch 96).\n",
            "Loss: 0.106582\n",
            "[+][2020-04-04 18:55:59.143403] Training iteration 386 (batch 2 of epoch 96).\n",
            "Loss: 0.115640\n",
            "[+][2020-04-04 18:55:59.432429] Training iteration 387 (batch 3 of epoch 96).\n",
            "Loss: 0.102839\n",
            "[+][2020-04-04 18:55:59.706998] Training iteration 388 (batch 4 of epoch 96).\n",
            "Loss: 0.103029\n",
            "[+][2020-04-04 18:55:59.983075] Training iteration 389 (batch 1 of epoch 97).\n",
            "Loss: 0.102326\n",
            "[+][2020-04-04 18:56:00.266728] Training iteration 390 (batch 2 of epoch 97).\n",
            "Loss: 0.114154\n",
            "[+][2020-04-04 18:56:00.552136] Training iteration 391 (batch 3 of epoch 97).\n",
            "Loss: 0.112957\n",
            "[+][2020-04-04 18:56:00.832574] Training iteration 392 (batch 4 of epoch 97).\n",
            "Loss: 0.114513\n",
            "[+][2020-04-04 18:56:01.118128] Training iteration 393 (batch 1 of epoch 98).\n",
            "Loss: 0.113741\n",
            "[+][2020-04-04 18:56:01.388262] Training iteration 394 (batch 2 of epoch 98).\n",
            "Loss: 0.102041\n",
            "[+][2020-04-04 18:56:01.665830] Training iteration 395 (batch 3 of epoch 98).\n",
            "Loss: 0.123600\n",
            "[+][2020-04-04 18:56:01.936456] Training iteration 396 (batch 4 of epoch 98).\n",
            "Loss: 0.112413\n",
            "[+][2020-04-04 18:56:02.217362] Training iteration 397 (batch 1 of epoch 99).\n",
            "Loss: 0.102904\n",
            "[+][2020-04-04 18:56:02.495969] Training iteration 398 (batch 2 of epoch 99).\n",
            "Loss: 0.099092\n",
            "[+][2020-04-04 18:56:02.777338] Training iteration 399 (batch 3 of epoch 99).\n",
            "Loss: 0.116462\n",
            "[+][2020-04-04 18:56:03.058039] Training iteration 400 (batch 4 of epoch 99).\n",
            "Loss: 0.101730\n",
            "[+][2020-04-04 18:56:03.332759] Training iteration 401 (batch 1 of epoch 100).\n",
            "Loss: 0.097144\n",
            "[+][2020-04-04 18:56:03.612498] Training iteration 402 (batch 2 of epoch 100).\n",
            "Loss: 0.103466\n",
            "[+][2020-04-04 18:56:03.881711] Training iteration 403 (batch 3 of epoch 100).\n",
            "Loss: 0.123804\n",
            "[+][2020-04-04 18:56:04.163982] Training iteration 404 (batch 4 of epoch 100).\n",
            "Loss: 0.111887\n",
            "[+][2020-04-04 18:56:04.439555] Training iteration 405 (batch 1 of epoch 101).\n",
            "Loss: 0.121155\n",
            "[+][2020-04-04 18:56:04.735834] Training iteration 406 (batch 2 of epoch 101).\n",
            "Loss: 0.112992\n",
            "[+][2020-04-04 18:56:05.042347] Training iteration 407 (batch 3 of epoch 101).\n",
            "Loss: 0.098475\n",
            "[+][2020-04-04 18:56:05.329963] Training iteration 408 (batch 4 of epoch 101).\n",
            "Loss: 0.098236\n",
            "[+][2020-04-04 18:56:05.612036] Training iteration 409 (batch 1 of epoch 102).\n",
            "Loss: 0.106717\n",
            "[+][2020-04-04 18:56:05.885696] Training iteration 410 (batch 2 of epoch 102).\n",
            "Loss: 0.105803\n",
            "[+][2020-04-04 18:56:06.172992] Training iteration 411 (batch 3 of epoch 102).\n",
            "Loss: 0.104139\n",
            "[+][2020-04-04 18:56:06.450532] Training iteration 412 (batch 4 of epoch 102).\n",
            "Loss: 0.097153\n",
            "[+][2020-04-04 18:56:06.731860] Training iteration 413 (batch 1 of epoch 103).\n",
            "Loss: 0.097075\n",
            "[+][2020-04-04 18:56:07.011924] Training iteration 414 (batch 2 of epoch 103).\n",
            "Loss: 0.105716\n",
            "[+][2020-04-04 18:56:07.298436] Training iteration 415 (batch 3 of epoch 103).\n",
            "Loss: 0.096214\n",
            "[+][2020-04-04 18:56:07.573923] Training iteration 416 (batch 4 of epoch 103).\n",
            "Loss: 0.095823\n",
            "[+][2020-04-04 18:56:07.867001] Training iteration 417 (batch 1 of epoch 104).\n",
            "Loss: 0.088925\n",
            "[+][2020-04-04 18:56:08.149677] Training iteration 418 (batch 2 of epoch 104).\n",
            "Loss: 0.111199\n",
            "[+][2020-04-04 18:56:08.425034] Training iteration 419 (batch 3 of epoch 104).\n",
            "Loss: 0.121304\n",
            "[+][2020-04-04 18:56:08.712293] Training iteration 420 (batch 4 of epoch 104).\n",
            "Loss: 0.110537\n",
            "[+][2020-04-04 18:56:08.994001] Training iteration 421 (batch 1 of epoch 105).\n",
            "Loss: 0.108910\n",
            "[+][2020-04-04 18:56:09.274521] Training iteration 422 (batch 2 of epoch 105).\n",
            "Loss: 0.108370\n",
            "[+][2020-04-04 18:56:09.548390] Training iteration 423 (batch 3 of epoch 105).\n",
            "Loss: 0.114069\n",
            "[+][2020-04-04 18:56:09.835682] Training iteration 424 (batch 4 of epoch 105).\n",
            "Loss: 0.115254\n",
            "[+][2020-04-04 18:56:10.126075] Training iteration 425 (batch 1 of epoch 106).\n",
            "Loss: 0.098671\n",
            "[+][2020-04-04 18:56:10.402420] Training iteration 426 (batch 2 of epoch 106).\n",
            "Loss: 0.114379\n",
            "[+][2020-04-04 18:56:10.697601] Training iteration 427 (batch 3 of epoch 106).\n",
            "Loss: 0.108457\n",
            "[+][2020-04-04 18:56:10.993208] Training iteration 428 (batch 4 of epoch 106).\n",
            "Loss: 0.104631\n",
            "[+][2020-04-04 18:56:11.275158] Training iteration 429 (batch 1 of epoch 107).\n",
            "Loss: 0.116288\n",
            "[+][2020-04-04 18:56:11.548592] Training iteration 430 (batch 2 of epoch 107).\n",
            "Loss: 0.106455\n",
            "[+][2020-04-04 18:56:11.832595] Training iteration 431 (batch 3 of epoch 107).\n",
            "Loss: 0.114587\n",
            "[+][2020-04-04 18:56:12.124016] Training iteration 432 (batch 4 of epoch 107).\n",
            "Loss: 0.110120\n",
            "[+][2020-04-04 18:56:12.401129] Training iteration 433 (batch 1 of epoch 108).\n",
            "Loss: 0.111473\n",
            "[+][2020-04-04 18:56:12.675148] Training iteration 434 (batch 2 of epoch 108).\n",
            "Loss: 0.118377\n",
            "[+][2020-04-04 18:56:12.956475] Training iteration 435 (batch 3 of epoch 108).\n",
            "Loss: 0.113819\n",
            "[+][2020-04-04 18:56:13.248855] Training iteration 436 (batch 4 of epoch 108).\n",
            "Loss: 0.112154\n",
            "[+][2020-04-04 18:56:13.528925] Training iteration 437 (batch 1 of epoch 109).\n",
            "Loss: 0.104542\n",
            "[+][2020-04-04 18:56:13.814983] Training iteration 438 (batch 2 of epoch 109).\n",
            "Loss: 0.109882\n",
            "[+][2020-04-04 18:56:14.093717] Training iteration 439 (batch 3 of epoch 109).\n",
            "Loss: 0.115572\n",
            "[+][2020-04-04 18:56:14.376641] Training iteration 440 (batch 4 of epoch 109).\n",
            "Loss: 0.104245\n",
            "[+][2020-04-04 18:56:14.652059] Training iteration 441 (batch 1 of epoch 110).\n",
            "Loss: 0.099544\n",
            "[+][2020-04-04 18:56:14.944620] Training iteration 442 (batch 2 of epoch 110).\n",
            "Loss: 0.106216\n",
            "[+][2020-04-04 18:56:15.236690] Training iteration 443 (batch 3 of epoch 110).\n",
            "Loss: 0.107817\n",
            "[+][2020-04-04 18:56:15.510844] Training iteration 444 (batch 4 of epoch 110).\n",
            "Loss: 0.113811\n",
            "[+][2020-04-04 18:56:15.795676] Training iteration 445 (batch 1 of epoch 111).\n",
            "Loss: 0.098161\n",
            "[+][2020-04-04 18:56:16.072268] Training iteration 446 (batch 2 of epoch 111).\n",
            "Loss: 0.091449\n",
            "[+][2020-04-04 18:56:16.353841] Training iteration 447 (batch 3 of epoch 111).\n",
            "Loss: 0.119879\n",
            "[+][2020-04-04 18:56:16.630108] Training iteration 448 (batch 4 of epoch 111).\n",
            "Loss: 0.122567\n",
            "[+][2020-04-04 18:56:16.912003] Training iteration 449 (batch 1 of epoch 112).\n",
            "Loss: 0.121962\n",
            "[+][2020-04-04 18:56:17.194633] Training iteration 450 (batch 2 of epoch 112).\n",
            "Loss: 0.113102\n",
            "[+][2020-04-04 18:56:17.473278] Training iteration 451 (batch 3 of epoch 112).\n",
            "Loss: 0.101127\n",
            "[+][2020-04-04 18:56:17.746811] Training iteration 452 (batch 4 of epoch 112).\n",
            "Loss: 0.119657\n",
            "[+][2020-04-04 18:56:18.032569] Training iteration 453 (batch 1 of epoch 113).\n",
            "Loss: 0.120348\n",
            "[+][2020-04-04 18:56:18.313682] Training iteration 454 (batch 2 of epoch 113).\n",
            "Loss: 0.119336\n",
            "[+][2020-04-04 18:56:18.598599] Training iteration 455 (batch 3 of epoch 113).\n",
            "Loss: 0.099794\n",
            "[+][2020-04-04 18:56:18.888405] Training iteration 456 (batch 4 of epoch 113).\n",
            "Loss: 0.115506\n",
            "[+][2020-04-04 18:56:19.171831] Training iteration 457 (batch 1 of epoch 114).\n",
            "Loss: 0.115812\n",
            "[+][2020-04-04 18:56:19.463679] Training iteration 458 (batch 2 of epoch 114).\n",
            "Loss: 0.106337\n",
            "[+][2020-04-04 18:56:19.738715] Training iteration 459 (batch 3 of epoch 114).\n",
            "Loss: 0.118583\n",
            "[+][2020-04-04 18:56:20.018129] Training iteration 460 (batch 4 of epoch 114).\n",
            "Loss: 0.118306\n",
            "[+][2020-04-04 18:56:20.304613] Training iteration 461 (batch 1 of epoch 115).\n",
            "Loss: 0.128913\n",
            "[+][2020-04-04 18:56:20.581829] Training iteration 462 (batch 2 of epoch 115).\n",
            "Loss: 0.115462\n",
            "[+][2020-04-04 18:56:20.861320] Training iteration 463 (batch 3 of epoch 115).\n",
            "Loss: 0.111395\n",
            "[+][2020-04-04 18:56:21.138075] Training iteration 464 (batch 4 of epoch 115).\n",
            "Loss: 0.128587\n",
            "[+][2020-04-04 18:56:21.433221] Training iteration 465 (batch 1 of epoch 116).\n",
            "Loss: 0.114882\n",
            "[+][2020-04-04 18:56:21.710134] Training iteration 466 (batch 2 of epoch 116).\n",
            "Loss: 0.130472\n",
            "[+][2020-04-04 18:56:21.986833] Training iteration 467 (batch 3 of epoch 116).\n",
            "Loss: 0.111922\n",
            "[+][2020-04-04 18:56:22.274852] Training iteration 468 (batch 4 of epoch 116).\n",
            "Loss: 0.101219\n",
            "[+][2020-04-04 18:56:22.547503] Training iteration 469 (batch 1 of epoch 117).\n",
            "Loss: 0.121692\n",
            "[+][2020-04-04 18:56:22.822123] Training iteration 470 (batch 2 of epoch 117).\n",
            "Loss: 0.107738\n",
            "[+][2020-04-04 18:56:23.115747] Training iteration 471 (batch 3 of epoch 117).\n",
            "Loss: 0.114552\n",
            "[+][2020-04-04 18:56:23.401450] Training iteration 472 (batch 4 of epoch 117).\n",
            "Loss: 0.108431\n",
            "[+][2020-04-04 18:56:23.673862] Training iteration 473 (batch 1 of epoch 118).\n",
            "Loss: 0.113035\n",
            "[+][2020-04-04 18:56:23.954728] Training iteration 474 (batch 2 of epoch 118).\n",
            "Loss: 0.106576\n",
            "[+][2020-04-04 18:56:24.229406] Training iteration 475 (batch 3 of epoch 118).\n",
            "Loss: 0.106274\n",
            "[+][2020-04-04 18:56:24.507901] Training iteration 476 (batch 4 of epoch 118).\n",
            "Loss: 0.089434\n",
            "[+][2020-04-04 18:56:24.795334] Training iteration 477 (batch 1 of epoch 119).\n",
            "Loss: 0.123736\n",
            "[+][2020-04-04 18:56:25.083938] Training iteration 478 (batch 2 of epoch 119).\n",
            "Loss: 0.113630\n",
            "[+][2020-04-04 18:56:25.359391] Training iteration 479 (batch 3 of epoch 119).\n",
            "Loss: 0.116844\n",
            "[+][2020-04-04 18:56:25.633666] Training iteration 480 (batch 4 of epoch 119).\n",
            "Loss: 0.114116\n",
            "[+][2020-04-04 18:56:25.906480] Training iteration 481 (batch 1 of epoch 120).\n",
            "Loss: 0.098099\n",
            "[+][2020-04-04 18:56:26.189132] Training iteration 482 (batch 2 of epoch 120).\n",
            "Loss: 0.097564\n",
            "[+][2020-04-04 18:56:26.469329] Training iteration 483 (batch 3 of epoch 120).\n",
            "Loss: 0.119742\n",
            "[+][2020-04-04 18:56:26.741398] Training iteration 484 (batch 4 of epoch 120).\n",
            "Loss: 0.111890\n",
            "[+][2020-04-04 18:56:27.021047] Training iteration 485 (batch 1 of epoch 121).\n",
            "Loss: 0.128051\n",
            "[+][2020-04-04 18:56:27.306631] Training iteration 486 (batch 2 of epoch 121).\n",
            "Loss: 0.115703\n",
            "[+][2020-04-04 18:56:27.579981] Training iteration 487 (batch 3 of epoch 121).\n",
            "Loss: 0.117544\n",
            "[+][2020-04-04 18:56:27.857892] Training iteration 488 (batch 4 of epoch 121).\n",
            "Loss: 0.110756\n",
            "[+][2020-04-04 18:56:28.137049] Training iteration 489 (batch 1 of epoch 122).\n",
            "Loss: 0.105054\n",
            "[+][2020-04-04 18:56:28.416629] Training iteration 490 (batch 2 of epoch 122).\n",
            "Loss: 0.105131\n",
            "[+][2020-04-04 18:56:28.691472] Training iteration 491 (batch 3 of epoch 122).\n",
            "Loss: 0.107518\n",
            "[+][2020-04-04 18:56:28.960685] Training iteration 492 (batch 4 of epoch 122).\n",
            "Loss: 0.088745\n",
            "[+][2020-04-04 18:56:29.242965] Training iteration 493 (batch 1 of epoch 123).\n",
            "Loss: 0.103414\n",
            "[+][2020-04-04 18:56:29.524794] Training iteration 494 (batch 2 of epoch 123).\n",
            "Loss: 0.108036\n",
            "[+][2020-04-04 18:56:29.794868] Training iteration 495 (batch 3 of epoch 123).\n",
            "Loss: 0.100657\n",
            "[+][2020-04-04 18:56:30.076579] Training iteration 496 (batch 4 of epoch 123).\n",
            "Loss: 0.102174\n",
            "[+][2020-04-04 18:56:30.355282] Training iteration 497 (batch 1 of epoch 124).\n",
            "Loss: 0.094522\n",
            "[+][2020-04-04 18:56:30.626156] Training iteration 498 (batch 2 of epoch 124).\n",
            "Loss: 0.108600\n",
            "[+][2020-04-04 18:56:30.909071] Training iteration 499 (batch 3 of epoch 124).\n",
            "Loss: 0.103809\n",
            "[+][2020-04-04 18:56:31.189554] Training iteration 500 (batch 4 of epoch 124).\n",
            "Loss: 0.105439\n",
            "[+][2020-04-04 18:56:31.469421] Training iteration 501 (batch 1 of epoch 125).\n",
            "Loss: 0.104922\n",
            "[+][2020-04-04 18:56:31.739103] Training iteration 502 (batch 2 of epoch 125).\n",
            "Loss: 0.125093\n",
            "[+][2020-04-04 18:56:32.030961] Training iteration 503 (batch 3 of epoch 125).\n",
            "Loss: 0.118721\n",
            "[+][2020-04-04 18:56:32.311122] Training iteration 504 (batch 4 of epoch 125).\n",
            "Loss: 0.104282\n",
            "[+][2020-04-04 18:56:32.589640] Training iteration 505 (batch 1 of epoch 126).\n",
            "Loss: 0.108975\n",
            "[+][2020-04-04 18:56:32.862443] Training iteration 506 (batch 2 of epoch 126).\n",
            "Loss: 0.101562\n",
            "[+][2020-04-04 18:56:33.150899] Training iteration 507 (batch 3 of epoch 126).\n",
            "Loss: 0.093911\n",
            "[+][2020-04-04 18:56:33.428150] Training iteration 508 (batch 4 of epoch 126).\n",
            "Loss: 0.105266\n",
            "[+][2020-04-04 18:56:33.705846] Training iteration 509 (batch 1 of epoch 127).\n",
            "Loss: 0.092493\n",
            "[+][2020-04-04 18:56:33.980427] Training iteration 510 (batch 2 of epoch 127).\n",
            "Loss: 0.103887\n",
            "[+][2020-04-04 18:56:34.258562] Training iteration 511 (batch 3 of epoch 127).\n",
            "Loss: 0.103205\n",
            "[+][2020-04-04 18:56:34.534669] Training iteration 512 (batch 4 of epoch 127).\n",
            "Loss: 0.115943\n",
            "[+][2020-04-04 18:56:34.809401] Training iteration 513 (batch 1 of epoch 128).\n",
            "Loss: 0.107569\n",
            "[+][2020-04-04 18:56:35.076995] Training iteration 514 (batch 2 of epoch 128).\n",
            "Loss: 0.103693\n",
            "[+][2020-04-04 18:56:35.359426] Training iteration 515 (batch 3 of epoch 128).\n",
            "Loss: 0.130552\n",
            "[+][2020-04-04 18:56:35.640791] Training iteration 516 (batch 4 of epoch 128).\n",
            "Loss: 0.124021\n",
            "[+][2020-04-04 18:56:35.912091] Training iteration 517 (batch 1 of epoch 129).\n",
            "Loss: 0.115013\n",
            "[+][2020-04-04 18:56:36.217591] Training iteration 518 (batch 2 of epoch 129).\n",
            "Loss: 0.120491\n",
            "[+][2020-04-04 18:56:36.496916] Training iteration 519 (batch 3 of epoch 129).\n",
            "Loss: 0.115216\n",
            "[+][2020-04-04 18:56:36.769043] Training iteration 520 (batch 4 of epoch 129).\n",
            "Loss: 0.135831\n",
            "[+][2020-04-04 18:56:37.041224] Training iteration 521 (batch 1 of epoch 130).\n",
            "Loss: 0.127968\n",
            "[+][2020-04-04 18:56:37.324893] Training iteration 522 (batch 2 of epoch 130).\n",
            "Loss: 0.113998\n",
            "[+][2020-04-04 18:56:37.600262] Training iteration 523 (batch 3 of epoch 130).\n",
            "Loss: 0.125579\n",
            "[+][2020-04-04 18:56:37.871779] Training iteration 524 (batch 4 of epoch 130).\n",
            "Loss: 0.126595\n",
            "[+][2020-04-04 18:56:38.156034] Training iteration 525 (batch 1 of epoch 131).\n",
            "Loss: 0.125794\n",
            "[+][2020-04-04 18:56:38.454088] Training iteration 526 (batch 2 of epoch 131).\n",
            "Loss: 0.120130\n",
            "[+][2020-04-04 18:56:38.727855] Training iteration 527 (batch 3 of epoch 131).\n",
            "Loss: 0.105525\n",
            "[+][2020-04-04 18:56:39.003035] Training iteration 528 (batch 4 of epoch 131).\n",
            "Loss: 0.111318\n",
            "[+][2020-04-04 18:56:39.309151] Training iteration 529 (batch 1 of epoch 132).\n",
            "Loss: 0.101991\n",
            "[+][2020-04-04 18:56:39.587827] Training iteration 530 (batch 2 of epoch 132).\n",
            "Loss: 0.104956\n",
            "[+][2020-04-04 18:56:39.858273] Training iteration 531 (batch 3 of epoch 132).\n",
            "Loss: 0.100030\n",
            "[+][2020-04-04 18:56:40.134921] Training iteration 532 (batch 4 of epoch 132).\n",
            "Loss: 0.115177\n",
            "[+][2020-04-04 18:56:40.433184] Training iteration 533 (batch 1 of epoch 133).\n",
            "Loss: 0.115114\n",
            "[+][2020-04-04 18:56:40.704700] Training iteration 534 (batch 2 of epoch 133).\n",
            "Loss: 0.104812\n",
            "[+][2020-04-04 18:56:40.977839] Training iteration 535 (batch 3 of epoch 133).\n",
            "Loss: 0.104924\n",
            "[+][2020-04-04 18:56:41.249975] Training iteration 536 (batch 4 of epoch 133).\n",
            "Loss: 0.096743\n",
            "[+][2020-04-04 18:56:41.533000] Training iteration 537 (batch 1 of epoch 134).\n",
            "Loss: 0.117707\n",
            "[+][2020-04-04 18:56:41.803349] Training iteration 538 (batch 2 of epoch 134).\n",
            "Loss: 0.108298\n",
            "[+][2020-04-04 18:56:42.091085] Training iteration 539 (batch 3 of epoch 134).\n",
            "Loss: 0.118888\n",
            "[+][2020-04-04 18:56:42.384860] Training iteration 540 (batch 4 of epoch 134).\n",
            "Loss: 0.123479\n",
            "[+][2020-04-04 18:56:42.662366] Training iteration 541 (batch 1 of epoch 135).\n",
            "Loss: 0.102815\n",
            "[+][2020-04-04 18:56:42.932313] Training iteration 542 (batch 2 of epoch 135).\n",
            "Loss: 0.100950\n",
            "[+][2020-04-04 18:56:43.208222] Training iteration 543 (batch 3 of epoch 135).\n",
            "Loss: 0.109848\n",
            "[+][2020-04-04 18:56:43.494146] Training iteration 544 (batch 4 of epoch 135).\n",
            "Loss: 0.101070\n",
            "[+][2020-04-04 18:56:43.765537] Training iteration 545 (batch 1 of epoch 136).\n",
            "Loss: 0.104347\n",
            "[+][2020-04-04 18:56:44.042148] Training iteration 546 (batch 2 of epoch 136).\n",
            "Loss: 0.094438\n",
            "[+][2020-04-04 18:56:44.316717] Training iteration 547 (batch 3 of epoch 136).\n",
            "Loss: 0.112187\n",
            "[+][2020-04-04 18:56:44.598544] Training iteration 548 (batch 4 of epoch 136).\n",
            "Loss: 0.108650\n",
            "[+][2020-04-04 18:56:44.891085] Training iteration 549 (batch 1 of epoch 137).\n",
            "Loss: 0.110421\n",
            "[+][2020-04-04 18:56:45.177853] Training iteration 550 (batch 2 of epoch 137).\n",
            "Loss: 0.109305\n",
            "[+][2020-04-04 18:56:45.474886] Training iteration 551 (batch 3 of epoch 137).\n",
            "Loss: 0.112845\n",
            "[+][2020-04-04 18:56:45.749251] Training iteration 552 (batch 4 of epoch 137).\n",
            "Loss: 0.107351\n",
            "[+][2020-04-04 18:56:46.021668] Training iteration 553 (batch 1 of epoch 138).\n",
            "Loss: 0.109036\n",
            "[+][2020-04-04 18:56:46.296950] Training iteration 554 (batch 2 of epoch 138).\n",
            "Loss: 0.123863\n",
            "[+][2020-04-04 18:56:46.581973] Training iteration 555 (batch 3 of epoch 138).\n",
            "Loss: 0.119642\n",
            "[+][2020-04-04 18:56:46.853813] Training iteration 556 (batch 4 of epoch 138).\n",
            "Loss: 0.116691\n",
            "[+][2020-04-04 18:56:47.128067] Training iteration 557 (batch 1 of epoch 139).\n",
            "Loss: 0.124923\n",
            "[+][2020-04-04 18:56:47.425965] Training iteration 558 (batch 2 of epoch 139).\n",
            "Loss: 0.105915\n",
            "[+][2020-04-04 18:56:47.699584] Training iteration 559 (batch 3 of epoch 139).\n",
            "Loss: 0.099094\n",
            "[+][2020-04-04 18:56:47.983847] Training iteration 560 (batch 4 of epoch 139).\n",
            "Loss: 0.122939\n",
            "[+][2020-04-04 18:56:48.260003] Training iteration 561 (batch 1 of epoch 140).\n",
            "Loss: 0.106737\n",
            "[+][2020-04-04 18:56:48.542029] Training iteration 562 (batch 2 of epoch 140).\n",
            "Loss: 0.114017\n",
            "[+][2020-04-04 18:56:48.816218] Training iteration 563 (batch 3 of epoch 140).\n",
            "Loss: 0.130788\n",
            "[+][2020-04-04 18:56:49.091301] Training iteration 564 (batch 4 of epoch 140).\n",
            "Loss: 0.125825\n",
            "[+][2020-04-04 18:56:49.375300] Training iteration 565 (batch 1 of epoch 141).\n",
            "Loss: 0.113637\n",
            "[+][2020-04-04 18:56:49.657510] Training iteration 566 (batch 2 of epoch 141).\n",
            "Loss: 0.100201\n",
            "[+][2020-04-04 18:56:49.937380] Training iteration 567 (batch 3 of epoch 141).\n",
            "Loss: 0.114049\n",
            "[+][2020-04-04 18:56:50.209917] Training iteration 568 (batch 4 of epoch 141).\n",
            "Loss: 0.130182\n",
            "[+][2020-04-04 18:56:50.487769] Training iteration 569 (batch 1 of epoch 142).\n",
            "Loss: 0.116702\n",
            "[+][2020-04-04 18:56:50.778121] Training iteration 570 (batch 2 of epoch 142).\n",
            "Loss: 0.126499\n",
            "[+][2020-04-04 18:56:51.049261] Training iteration 571 (batch 3 of epoch 142).\n",
            "Loss: 0.110330\n",
            "[+][2020-04-04 18:56:51.320035] Training iteration 572 (batch 4 of epoch 142).\n",
            "Loss: 0.111453\n",
            "[+][2020-04-04 18:56:51.603006] Training iteration 573 (batch 1 of epoch 143).\n",
            "Loss: 0.119844\n",
            "[+][2020-04-04 18:56:51.875898] Training iteration 574 (batch 2 of epoch 143).\n",
            "Loss: 0.115607\n",
            "[+][2020-04-04 18:56:52.145247] Training iteration 575 (batch 3 of epoch 143).\n",
            "Loss: 0.128989\n",
            "[+][2020-04-04 18:56:52.442765] Training iteration 576 (batch 4 of epoch 143).\n",
            "Loss: 0.113622\n",
            "[+][2020-04-04 18:56:52.731988] Training iteration 577 (batch 1 of epoch 144).\n",
            "Loss: 0.117106\n",
            "[+][2020-04-04 18:56:53.003815] Training iteration 578 (batch 2 of epoch 144).\n",
            "Loss: 0.109056\n",
            "[+][2020-04-04 18:56:53.291469] Training iteration 579 (batch 3 of epoch 144).\n",
            "Loss: 0.106386\n",
            "[+][2020-04-04 18:56:53.578853] Training iteration 580 (batch 4 of epoch 144).\n",
            "Loss: 0.129835\n",
            "[+][2020-04-04 18:56:53.849053] Training iteration 581 (batch 1 of epoch 145).\n",
            "Loss: 0.117874\n",
            "[+][2020-04-04 18:56:54.140489] Training iteration 582 (batch 2 of epoch 145).\n",
            "Loss: 0.109096\n",
            "[+][2020-04-04 18:56:54.416673] Training iteration 583 (batch 3 of epoch 145).\n",
            "Loss: 0.102986\n",
            "[+][2020-04-04 18:56:54.701017] Training iteration 584 (batch 4 of epoch 145).\n",
            "Loss: 0.105458\n",
            "[+][2020-04-04 18:56:54.983951] Training iteration 585 (batch 1 of epoch 146).\n",
            "Loss: 0.104155\n",
            "[+][2020-04-04 18:56:55.278547] Training iteration 586 (batch 2 of epoch 146).\n",
            "Loss: 0.106921\n",
            "[+][2020-04-04 18:56:55.552111] Training iteration 587 (batch 3 of epoch 146).\n",
            "Loss: 0.119761\n",
            "[+][2020-04-04 18:56:55.832588] Training iteration 588 (batch 4 of epoch 146).\n",
            "Loss: 0.106585\n",
            "[+][2020-04-04 18:56:56.126329] Training iteration 589 (batch 1 of epoch 147).\n",
            "Loss: 0.111580\n",
            "[+][2020-04-04 18:56:56.400693] Training iteration 590 (batch 2 of epoch 147).\n",
            "Loss: 0.108152\n",
            "[+][2020-04-04 18:56:56.681680] Training iteration 591 (batch 3 of epoch 147).\n",
            "Loss: 0.118461\n",
            "[+][2020-04-04 18:56:56.954827] Training iteration 592 (batch 4 of epoch 147).\n",
            "Loss: 0.105870\n",
            "[+][2020-04-04 18:56:57.230735] Training iteration 593 (batch 1 of epoch 148).\n",
            "Loss: 0.116798\n",
            "[+][2020-04-04 18:56:57.501418] Training iteration 594 (batch 2 of epoch 148).\n",
            "Loss: 0.107860\n",
            "[+][2020-04-04 18:56:57.790466] Training iteration 595 (batch 3 of epoch 148).\n",
            "Loss: 0.111502\n",
            "[+][2020-04-04 18:56:58.064745] Training iteration 596 (batch 4 of epoch 148).\n",
            "Loss: 0.113941\n",
            "[+][2020-04-04 18:56:58.339613] Training iteration 597 (batch 1 of epoch 149).\n",
            "Loss: 0.099756\n",
            "[+][2020-04-04 18:56:58.614846] Training iteration 598 (batch 2 of epoch 149).\n",
            "Loss: 0.112733\n",
            "[+][2020-04-04 18:56:58.899700] Training iteration 599 (batch 3 of epoch 149).\n",
            "Loss: 0.114241\n",
            "[+][2020-04-04 18:56:59.178108] Training iteration 600 (batch 4 of epoch 149).\n",
            "Loss: 0.108584\n",
            "[+][2020-04-04 18:56:59.449793] Training iteration 601 (batch 1 of epoch 150).\n",
            "Loss: 0.134946\n",
            "[+][2020-04-04 18:56:59.733700] Training iteration 602 (batch 2 of epoch 150).\n",
            "Loss: 0.122036\n",
            "[+][2020-04-04 18:57:00.007458] Training iteration 603 (batch 3 of epoch 150).\n",
            "Loss: 0.119365\n",
            "[+][2020-04-04 18:57:00.298082] Training iteration 604 (batch 4 of epoch 150).\n",
            "Loss: 0.109435\n",
            "[+][2020-04-04 18:57:00.585508] Training iteration 605 (batch 1 of epoch 151).\n",
            "Loss: 0.108524\n",
            "[+][2020-04-04 18:57:00.872767] Training iteration 606 (batch 2 of epoch 151).\n",
            "Loss: 0.124619\n",
            "[+][2020-04-04 18:57:01.145555] Training iteration 607 (batch 3 of epoch 151).\n",
            "Loss: 0.098015\n",
            "[+][2020-04-04 18:57:01.421492] Training iteration 608 (batch 4 of epoch 151).\n",
            "Loss: 0.105049\n",
            "[+][2020-04-04 18:57:01.707191] Training iteration 609 (batch 1 of epoch 152).\n",
            "Loss: 0.104510\n",
            "[+][2020-04-04 18:57:01.977964] Training iteration 610 (batch 2 of epoch 152).\n",
            "Loss: 0.099920\n",
            "[+][2020-04-04 18:57:02.275225] Training iteration 611 (batch 3 of epoch 152).\n",
            "Loss: 0.094460\n",
            "[+][2020-04-04 18:57:02.548960] Training iteration 612 (batch 4 of epoch 152).\n",
            "Loss: 0.095224\n",
            "[+][2020-04-04 18:57:02.830597] Training iteration 613 (batch 1 of epoch 153).\n",
            "Loss: 0.103995\n",
            "[+][2020-04-04 18:57:03.103078] Training iteration 614 (batch 2 of epoch 153).\n",
            "Loss: 0.087032\n",
            "[+][2020-04-04 18:57:03.395564] Training iteration 615 (batch 3 of epoch 153).\n",
            "Loss: 0.071952\n",
            "[+][2020-04-04 18:57:03.666088] Training iteration 616 (batch 4 of epoch 153).\n",
            "Loss: 0.114254\n",
            "[+][2020-04-04 18:57:03.951867] Training iteration 617 (batch 1 of epoch 154).\n",
            "Loss: 0.113712\n",
            "[+][2020-04-04 18:57:04.241112] Training iteration 618 (batch 2 of epoch 154).\n",
            "Loss: 0.110207\n",
            "[+][2020-04-04 18:57:04.511593] Training iteration 619 (batch 3 of epoch 154).\n",
            "Loss: 0.107718\n",
            "[+][2020-04-04 18:57:04.799865] Training iteration 620 (batch 4 of epoch 154).\n",
            "Loss: 0.097362\n",
            "[+][2020-04-04 18:57:05.072174] Training iteration 621 (batch 1 of epoch 155).\n",
            "Loss: 0.106458\n",
            "[+][2020-04-04 18:57:05.348893] Training iteration 622 (batch 2 of epoch 155).\n",
            "Loss: 0.109507\n",
            "[+][2020-04-04 18:57:05.620128] Training iteration 623 (batch 3 of epoch 155).\n",
            "Loss: 0.110105\n",
            "[+][2020-04-04 18:57:05.911460] Training iteration 624 (batch 4 of epoch 155).\n",
            "Loss: 0.098798\n",
            "[+][2020-04-04 18:57:06.195252] Training iteration 625 (batch 1 of epoch 156).\n",
            "Loss: 0.115807\n",
            "[+][2020-04-04 18:57:06.466494] Training iteration 626 (batch 2 of epoch 156).\n",
            "Loss: 0.115822\n",
            "[+][2020-04-04 18:57:06.746203] Training iteration 627 (batch 3 of epoch 156).\n",
            "Loss: 0.114818\n",
            "[+][2020-04-04 18:57:07.026401] Training iteration 628 (batch 4 of epoch 156).\n",
            "Loss: 0.101992\n",
            "[+][2020-04-04 18:57:07.299682] Training iteration 629 (batch 1 of epoch 157).\n",
            "Loss: 0.120632\n",
            "[+][2020-04-04 18:57:07.574036] Training iteration 630 (batch 2 of epoch 157).\n",
            "Loss: 0.128640\n",
            "[+][2020-04-04 18:57:07.857103] Training iteration 631 (batch 3 of epoch 157).\n",
            "Loss: 0.114827\n",
            "[+][2020-04-04 18:57:08.129283] Training iteration 632 (batch 4 of epoch 157).\n",
            "Loss: 0.124738\n",
            "[+][2020-04-04 18:57:08.413587] Training iteration 633 (batch 1 of epoch 158).\n",
            "Loss: 0.108371\n",
            "[+][2020-04-04 18:57:08.690303] Training iteration 634 (batch 2 of epoch 158).\n",
            "Loss: 0.117847\n",
            "[+][2020-04-04 18:57:08.971519] Training iteration 635 (batch 3 of epoch 158).\n",
            "Loss: 0.110357\n",
            "[+][2020-04-04 18:57:09.266757] Training iteration 636 (batch 4 of epoch 158).\n",
            "Loss: 0.116562\n",
            "[+][2020-04-04 18:57:09.541113] Training iteration 637 (batch 1 of epoch 159).\n",
            "Loss: 0.119903\n",
            "[+][2020-04-04 18:57:09.815294] Training iteration 638 (batch 2 of epoch 159).\n",
            "Loss: 0.114559\n",
            "[+][2020-04-04 18:57:10.095411] Training iteration 639 (batch 3 of epoch 159).\n",
            "Loss: 0.110293\n",
            "[+][2020-04-04 18:57:10.369522] Training iteration 640 (batch 4 of epoch 159).\n",
            "Loss: 0.107206\n",
            "[+][2020-04-04 18:57:10.641566] Training iteration 641 (batch 1 of epoch 160).\n",
            "Loss: 0.114369\n",
            "[+][2020-04-04 18:57:10.933935] Training iteration 642 (batch 2 of epoch 160).\n",
            "Loss: 0.106418\n",
            "[+][2020-04-04 18:57:11.210082] Training iteration 643 (batch 3 of epoch 160).\n",
            "Loss: 0.096855\n",
            "[+][2020-04-04 18:57:11.481300] Training iteration 644 (batch 4 of epoch 160).\n",
            "Loss: 0.110763\n",
            "[+][2020-04-04 18:57:11.754147] Training iteration 645 (batch 1 of epoch 161).\n",
            "Loss: 0.106040\n",
            "[+][2020-04-04 18:57:12.039014] Training iteration 646 (batch 2 of epoch 161).\n",
            "Loss: 0.111286\n",
            "[+][2020-04-04 18:57:12.313947] Training iteration 647 (batch 3 of epoch 161).\n",
            "Loss: 0.115911\n",
            "[+][2020-04-04 18:57:12.584557] Training iteration 648 (batch 4 of epoch 161).\n",
            "Loss: 0.110168\n",
            "[+][2020-04-04 18:57:12.867859] Training iteration 649 (batch 1 of epoch 162).\n",
            "Loss: 0.123684\n",
            "[+][2020-04-04 18:57:13.147852] Training iteration 650 (batch 2 of epoch 162).\n",
            "Loss: 0.112482\n",
            "[+][2020-04-04 18:57:13.421558] Training iteration 651 (batch 3 of epoch 162).\n",
            "Loss: 0.102028\n",
            "[+][2020-04-04 18:57:13.705089] Training iteration 652 (batch 4 of epoch 162).\n",
            "Loss: 0.119824\n",
            "[+][2020-04-04 18:57:13.988790] Training iteration 653 (batch 1 of epoch 163).\n",
            "Loss: 0.114118\n",
            "[+][2020-04-04 18:57:14.258853] Training iteration 654 (batch 2 of epoch 163).\n",
            "Loss: 0.124338\n",
            "[+][2020-04-04 18:57:14.531944] Training iteration 655 (batch 3 of epoch 163).\n",
            "Loss: 0.106382\n",
            "[+][2020-04-04 18:57:14.809017] Training iteration 656 (batch 4 of epoch 163).\n",
            "Loss: 0.112201\n",
            "[+][2020-04-04 18:57:15.085183] Training iteration 657 (batch 1 of epoch 164).\n",
            "Loss: 0.115249\n",
            "[+][2020-04-04 18:57:15.362807] Training iteration 658 (batch 2 of epoch 164).\n",
            "Loss: 0.106424\n",
            "[+][2020-04-04 18:57:15.637593] Training iteration 659 (batch 3 of epoch 164).\n",
            "Loss: 0.117478\n",
            "[+][2020-04-04 18:57:15.913473] Training iteration 660 (batch 4 of epoch 164).\n",
            "Loss: 0.099879\n",
            "[+][2020-04-04 18:57:16.205739] Training iteration 661 (batch 1 of epoch 165).\n",
            "Loss: 0.109467\n",
            "[+][2020-04-04 18:57:16.485738] Training iteration 662 (batch 2 of epoch 165).\n",
            "Loss: 0.099582\n",
            "[+][2020-04-04 18:57:16.758198] Training iteration 663 (batch 3 of epoch 165).\n",
            "Loss: 0.104918\n",
            "[+][2020-04-04 18:57:17.045100] Training iteration 664 (batch 4 of epoch 165).\n",
            "Loss: 0.124036\n",
            "[+][2020-04-04 18:57:17.343103] Training iteration 665 (batch 1 of epoch 166).\n",
            "Loss: 0.115180\n",
            "[+][2020-04-04 18:57:17.613427] Training iteration 666 (batch 2 of epoch 166).\n",
            "Loss: 0.114700\n",
            "[+][2020-04-04 18:57:17.890853] Training iteration 667 (batch 3 of epoch 166).\n",
            "Loss: 0.102042\n",
            "[+][2020-04-04 18:57:18.171938] Training iteration 668 (batch 4 of epoch 166).\n",
            "Loss: 0.099594\n",
            "[+][2020-04-04 18:57:18.441898] Training iteration 669 (batch 1 of epoch 167).\n",
            "Loss: 0.105419\n",
            "[+][2020-04-04 18:57:18.716948] Training iteration 670 (batch 2 of epoch 167).\n",
            "Loss: 0.104261\n",
            "[+][2020-04-04 18:57:19.002631] Training iteration 671 (batch 3 of epoch 167).\n",
            "Loss: 0.105915\n",
            "[+][2020-04-04 18:57:19.306815] Training iteration 672 (batch 4 of epoch 167).\n",
            "Loss: 0.116217\n",
            "[+][2020-04-04 18:57:19.582145] Training iteration 673 (batch 1 of epoch 168).\n",
            "Loss: 0.090581\n",
            "[+][2020-04-04 18:57:19.860175] Training iteration 674 (batch 2 of epoch 168).\n",
            "Loss: 0.099651\n",
            "[+][2020-04-04 18:57:20.141484] Training iteration 675 (batch 3 of epoch 168).\n",
            "Loss: 0.124194\n",
            "[+][2020-04-04 18:57:20.414099] Training iteration 676 (batch 4 of epoch 168).\n",
            "Loss: 0.102263\n",
            "[+][2020-04-04 18:57:20.692138] Training iteration 677 (batch 1 of epoch 169).\n",
            "Loss: 0.101748\n",
            "[+][2020-04-04 18:57:20.967362] Training iteration 678 (batch 2 of epoch 169).\n",
            "Loss: 0.100178\n",
            "[+][2020-04-04 18:57:21.249465] Training iteration 679 (batch 3 of epoch 169).\n",
            "Loss: 0.102633\n",
            "[+][2020-04-04 18:57:21.531067] Training iteration 680 (batch 4 of epoch 169).\n",
            "Loss: 0.099474\n",
            "[+][2020-04-04 18:57:21.803923] Training iteration 681 (batch 1 of epoch 170).\n",
            "Loss: 0.110953\n",
            "[+][2020-04-04 18:57:22.082571] Training iteration 682 (batch 2 of epoch 170).\n",
            "Loss: 0.115090\n",
            "[+][2020-04-04 18:57:22.363363] Training iteration 683 (batch 3 of epoch 170).\n",
            "Loss: 0.111439\n",
            "[+][2020-04-04 18:57:22.633706] Training iteration 684 (batch 4 of epoch 170).\n",
            "Loss: 0.098193\n",
            "[+][2020-04-04 18:57:22.913777] Training iteration 685 (batch 1 of epoch 171).\n",
            "Loss: 0.097625\n",
            "[+][2020-04-04 18:57:23.197044] Training iteration 686 (batch 2 of epoch 171).\n",
            "Loss: 0.095302\n",
            "[+][2020-04-04 18:57:23.474837] Training iteration 687 (batch 3 of epoch 171).\n",
            "Loss: 0.094274\n",
            "[+][2020-04-04 18:57:23.755223] Training iteration 688 (batch 4 of epoch 171).\n",
            "Loss: 0.091250\n",
            "[+][2020-04-04 18:57:24.045702] Training iteration 689 (batch 1 of epoch 172).\n",
            "Loss: 0.110941\n",
            "[+][2020-04-04 18:57:24.326556] Training iteration 690 (batch 2 of epoch 172).\n",
            "Loss: 0.114881\n",
            "[+][2020-04-04 18:57:24.600564] Training iteration 691 (batch 3 of epoch 172).\n",
            "Loss: 0.111867\n",
            "[+][2020-04-04 18:57:24.878038] Training iteration 692 (batch 4 of epoch 172).\n",
            "Loss: 0.104289\n",
            "[+][2020-04-04 18:57:25.158061] Training iteration 693 (batch 1 of epoch 173).\n",
            "Loss: 0.099813\n",
            "[+][2020-04-04 18:57:25.438602] Training iteration 694 (batch 2 of epoch 173).\n",
            "Loss: 0.103892\n",
            "[+][2020-04-04 18:57:25.712949] Training iteration 695 (batch 3 of epoch 173).\n",
            "Loss: 0.101480\n",
            "[+][2020-04-04 18:57:25.991863] Training iteration 696 (batch 4 of epoch 173).\n",
            "Loss: 0.100322\n",
            "[+][2020-04-04 18:57:26.298679] Training iteration 697 (batch 1 of epoch 174).\n",
            "Loss: 0.093171\n",
            "[+][2020-04-04 18:57:26.570428] Training iteration 698 (batch 2 of epoch 174).\n",
            "Loss: 0.097015\n",
            "[+][2020-04-04 18:57:26.844717] Training iteration 699 (batch 3 of epoch 174).\n",
            "Loss: 0.095656\n",
            "[+][2020-04-04 18:57:27.123923] Training iteration 700 (batch 4 of epoch 174).\n",
            "Loss: 0.105618\n",
            "[+][2020-04-04 18:57:27.401655] Training iteration 701 (batch 1 of epoch 175).\n",
            "Loss: 0.087549\n",
            "[+][2020-04-04 18:57:27.675165] Training iteration 702 (batch 2 of epoch 175).\n",
            "Loss: 0.102226\n",
            "[+][2020-04-04 18:57:27.951554] Training iteration 703 (batch 3 of epoch 175).\n",
            "Loss: 0.076163\n",
            "[+][2020-04-04 18:57:28.258855] Training iteration 704 (batch 4 of epoch 175).\n",
            "Loss: 0.102018\n",
            "[+][2020-04-04 18:57:28.541523] Training iteration 705 (batch 1 of epoch 176).\n",
            "Loss: 0.090098\n",
            "[+][2020-04-04 18:57:28.814644] Training iteration 706 (batch 2 of epoch 176).\n",
            "Loss: 0.100797\n",
            "[+][2020-04-04 18:57:29.093186] Training iteration 707 (batch 3 of epoch 176).\n",
            "Loss: 0.113279\n",
            "[+][2020-04-04 18:57:29.387610] Training iteration 708 (batch 4 of epoch 176).\n",
            "Loss: 0.107602\n",
            "[+][2020-04-04 18:57:29.661574] Training iteration 709 (batch 1 of epoch 177).\n",
            "Loss: 0.128361\n",
            "[+][2020-04-04 18:57:29.939609] Training iteration 710 (batch 2 of epoch 177).\n",
            "Loss: 0.099402\n",
            "[+][2020-04-04 18:57:30.247443] Training iteration 711 (batch 3 of epoch 177).\n",
            "Loss: 0.102432\n",
            "[+][2020-04-04 18:57:30.531308] Training iteration 712 (batch 4 of epoch 177).\n",
            "Loss: 0.112450\n",
            "[+][2020-04-04 18:57:30.808512] Training iteration 713 (batch 1 of epoch 178).\n",
            "Loss: 0.111046\n",
            "[+][2020-04-04 18:57:31.091818] Training iteration 714 (batch 2 of epoch 178).\n",
            "Loss: 0.103700\n",
            "[+][2020-04-04 18:57:31.373818] Training iteration 715 (batch 3 of epoch 178).\n",
            "Loss: 0.109420\n",
            "[+][2020-04-04 18:57:31.650926] Training iteration 716 (batch 4 of epoch 178).\n",
            "Loss: 0.113081\n",
            "[+][2020-04-04 18:57:31.936019] Training iteration 717 (batch 1 of epoch 179).\n",
            "Loss: 0.097105\n",
            "[+][2020-04-04 18:57:32.221044] Training iteration 718 (batch 2 of epoch 179).\n",
            "Loss: 0.107505\n",
            "[+][2020-04-04 18:57:32.522646] Training iteration 719 (batch 3 of epoch 179).\n",
            "Loss: 0.105603\n",
            "[+][2020-04-04 18:57:32.799944] Training iteration 720 (batch 4 of epoch 179).\n",
            "Loss: 0.103230\n",
            "[+][2020-04-04 18:57:33.082676] Training iteration 721 (batch 1 of epoch 180).\n",
            "Loss: 0.098949\n",
            "[+][2020-04-04 18:57:33.371855] Training iteration 722 (batch 2 of epoch 180).\n",
            "Loss: 0.095446\n",
            "[+][2020-04-04 18:57:33.646809] Training iteration 723 (batch 3 of epoch 180).\n",
            "Loss: 0.091817\n",
            "[+][2020-04-04 18:57:33.925419] Training iteration 724 (batch 4 of epoch 180).\n",
            "Loss: 0.090225\n",
            "[+][2020-04-04 18:57:34.220142] Training iteration 725 (batch 1 of epoch 181).\n",
            "Loss: 0.087113\n",
            "[+][2020-04-04 18:57:34.501110] Training iteration 726 (batch 2 of epoch 181).\n",
            "Loss: 0.101096\n",
            "[+][2020-04-04 18:57:34.785366] Training iteration 727 (batch 3 of epoch 181).\n",
            "Loss: 0.119592\n",
            "[+][2020-04-04 18:57:35.068459] Training iteration 728 (batch 4 of epoch 181).\n",
            "Loss: 0.109997\n",
            "[+][2020-04-04 18:57:35.360956] Training iteration 729 (batch 1 of epoch 182).\n",
            "Loss: 0.109979\n",
            "[+][2020-04-04 18:57:35.645186] Training iteration 730 (batch 2 of epoch 182).\n",
            "Loss: 0.121510\n",
            "[+][2020-04-04 18:57:35.923404] Training iteration 731 (batch 3 of epoch 182).\n",
            "Loss: 0.100226\n",
            "[+][2020-04-04 18:57:36.205797] Training iteration 732 (batch 4 of epoch 182).\n",
            "Loss: 0.118204\n",
            "[+][2020-04-04 18:57:36.492545] Training iteration 733 (batch 1 of epoch 183).\n",
            "Loss: 0.101993\n",
            "[+][2020-04-04 18:57:36.769179] Training iteration 734 (batch 2 of epoch 183).\n",
            "Loss: 0.090725\n",
            "[+][2020-04-04 18:57:37.045001] Training iteration 735 (batch 3 of epoch 183).\n",
            "Loss: 0.105605\n",
            "[+][2020-04-04 18:57:37.333439] Training iteration 736 (batch 4 of epoch 183).\n",
            "Loss: 0.115698\n",
            "[+][2020-04-04 18:57:37.615433] Training iteration 737 (batch 1 of epoch 184).\n",
            "Loss: 0.099789\n",
            "[+][2020-04-04 18:57:37.886105] Training iteration 738 (batch 2 of epoch 184).\n",
            "Loss: 0.116829\n",
            "[+][2020-04-04 18:57:38.170486] Training iteration 739 (batch 3 of epoch 184).\n",
            "Loss: 0.110856\n",
            "[+][2020-04-04 18:57:38.449865] Training iteration 740 (batch 4 of epoch 184).\n",
            "Loss: 0.108228\n",
            "[+][2020-04-04 18:57:38.731406] Training iteration 741 (batch 1 of epoch 185).\n",
            "Loss: 0.106546\n",
            "[+][2020-04-04 18:57:39.008479] Training iteration 742 (batch 2 of epoch 185).\n",
            "Loss: 0.099578\n",
            "[+][2020-04-04 18:57:39.294157] Training iteration 743 (batch 3 of epoch 185).\n",
            "Loss: 0.108873\n",
            "[+][2020-04-04 18:57:39.574216] Training iteration 744 (batch 4 of epoch 185).\n",
            "Loss: 0.100140\n",
            "[+][2020-04-04 18:57:39.842920] Training iteration 745 (batch 1 of epoch 186).\n",
            "Loss: 0.089243\n",
            "[+][2020-04-04 18:57:40.139377] Training iteration 746 (batch 2 of epoch 186).\n",
            "Loss: 0.112194\n",
            "[+][2020-04-04 18:57:40.415076] Training iteration 747 (batch 3 of epoch 186).\n",
            "Loss: 0.106231\n",
            "[+][2020-04-04 18:57:40.703954] Training iteration 748 (batch 4 of epoch 186).\n",
            "Loss: 0.114062\n",
            "[+][2020-04-04 18:57:40.976864] Training iteration 749 (batch 1 of epoch 187).\n",
            "Loss: 0.112776\n",
            "[+][2020-04-04 18:57:41.255916] Training iteration 750 (batch 2 of epoch 187).\n",
            "Loss: 0.104214\n",
            "[+][2020-04-04 18:57:41.536224] Training iteration 751 (batch 3 of epoch 187).\n",
            "Loss: 0.110972\n",
            "[+][2020-04-04 18:57:41.811779] Training iteration 752 (batch 4 of epoch 187).\n",
            "Loss: 0.109265\n",
            "[+][2020-04-04 18:57:42.095549] Training iteration 753 (batch 1 of epoch 188).\n",
            "Loss: 0.104413\n",
            "[+][2020-04-04 18:57:42.378629] Training iteration 754 (batch 2 of epoch 188).\n",
            "Loss: 0.105442\n",
            "[+][2020-04-04 18:57:42.657954] Training iteration 755 (batch 3 of epoch 188).\n",
            "Loss: 0.105287\n",
            "[+][2020-04-04 18:57:42.929362] Training iteration 756 (batch 4 of epoch 188).\n",
            "Loss: 0.118775\n",
            "[+][2020-04-04 18:57:43.218581] Training iteration 757 (batch 1 of epoch 189).\n",
            "Loss: 0.098316\n",
            "[+][2020-04-04 18:57:43.493612] Training iteration 758 (batch 2 of epoch 189).\n",
            "Loss: 0.107550\n",
            "[+][2020-04-04 18:57:43.771778] Training iteration 759 (batch 3 of epoch 189).\n",
            "Loss: 0.097426\n",
            "[+][2020-04-04 18:57:44.041759] Training iteration 760 (batch 4 of epoch 189).\n",
            "Loss: 0.114401\n",
            "[+][2020-04-04 18:57:44.319273] Training iteration 761 (batch 1 of epoch 190).\n",
            "Loss: 0.122912\n",
            "[+][2020-04-04 18:57:44.600206] Training iteration 762 (batch 2 of epoch 190).\n",
            "Loss: 0.103337\n",
            "[+][2020-04-04 18:57:44.877023] Training iteration 763 (batch 3 of epoch 190).\n",
            "Loss: 0.097438\n",
            "[+][2020-04-04 18:57:45.157302] Training iteration 764 (batch 4 of epoch 190).\n",
            "Loss: 0.099593\n",
            "[+][2020-04-04 18:57:45.435583] Training iteration 765 (batch 1 of epoch 191).\n",
            "Loss: 0.086155\n",
            "[+][2020-04-04 18:57:45.716013] Training iteration 766 (batch 2 of epoch 191).\n",
            "Loss: 0.095917\n",
            "[+][2020-04-04 18:57:45.994141] Training iteration 767 (batch 3 of epoch 191).\n",
            "Loss: 0.087074\n",
            "[+][2020-04-04 18:57:46.282634] Training iteration 768 (batch 4 of epoch 191).\n",
            "Loss: 0.100154\n",
            "[+][2020-04-04 18:57:46.558543] Training iteration 769 (batch 1 of epoch 192).\n",
            "Loss: 0.093680\n",
            "[+][2020-04-04 18:57:46.836973] Training iteration 770 (batch 2 of epoch 192).\n",
            "Loss: 0.109611\n",
            "[+][2020-04-04 18:57:47.109977] Training iteration 771 (batch 3 of epoch 192).\n",
            "Loss: 0.114264\n",
            "[+][2020-04-04 18:57:47.390597] Training iteration 772 (batch 4 of epoch 192).\n",
            "Loss: 0.099631\n",
            "[+][2020-04-04 18:57:47.665403] Training iteration 773 (batch 1 of epoch 193).\n",
            "Loss: 0.110466\n",
            "[+][2020-04-04 18:57:47.936858] Training iteration 774 (batch 2 of epoch 193).\n",
            "Loss: 0.101747\n",
            "[+][2020-04-04 18:57:48.241286] Training iteration 775 (batch 3 of epoch 193).\n",
            "Loss: 0.107912\n",
            "[+][2020-04-04 18:57:48.518408] Training iteration 776 (batch 4 of epoch 193).\n",
            "Loss: 0.102296\n",
            "[+][2020-04-04 18:57:48.804120] Training iteration 777 (batch 1 of epoch 194).\n",
            "Loss: 0.116797\n",
            "[+][2020-04-04 18:57:49.073812] Training iteration 778 (batch 2 of epoch 194).\n",
            "Loss: 0.121864\n",
            "[+][2020-04-04 18:57:49.353657] Training iteration 779 (batch 3 of epoch 194).\n",
            "Loss: 0.106128\n",
            "[+][2020-04-04 18:57:49.626891] Training iteration 780 (batch 4 of epoch 194).\n",
            "Loss: 0.100034\n",
            "[+][2020-04-04 18:57:49.909201] Training iteration 781 (batch 1 of epoch 195).\n",
            "Loss: 0.105645\n",
            "[+][2020-04-04 18:57:50.187344] Training iteration 782 (batch 2 of epoch 195).\n",
            "Loss: 0.101370\n",
            "[+][2020-04-04 18:57:50.485568] Training iteration 783 (batch 3 of epoch 195).\n",
            "Loss: 0.093149\n",
            "[+][2020-04-04 18:57:50.765911] Training iteration 784 (batch 4 of epoch 195).\n",
            "Loss: 0.101828\n",
            "[+][2020-04-04 18:57:51.039474] Training iteration 785 (batch 1 of epoch 196).\n",
            "Loss: 0.092556\n",
            "[+][2020-04-04 18:57:51.316144] Training iteration 786 (batch 2 of epoch 196).\n",
            "Loss: 0.076685\n",
            "[+][2020-04-04 18:57:51.591053] Training iteration 787 (batch 3 of epoch 196).\n",
            "Loss: 0.110128\n",
            "[+][2020-04-04 18:57:51.886569] Training iteration 788 (batch 4 of epoch 196).\n",
            "Loss: 0.104645\n",
            "[+][2020-04-04 18:57:52.160331] Training iteration 789 (batch 1 of epoch 197).\n",
            "Loss: 0.113806\n",
            "[+][2020-04-04 18:57:52.437552] Training iteration 790 (batch 2 of epoch 197).\n",
            "Loss: 0.107710\n",
            "[+][2020-04-04 18:57:52.710839] Training iteration 791 (batch 3 of epoch 197).\n",
            "Loss: 0.107281\n",
            "[+][2020-04-04 18:57:52.993980] Training iteration 792 (batch 4 of epoch 197).\n",
            "Loss: 0.109485\n",
            "[+][2020-04-04 18:57:53.283309] Training iteration 793 (batch 1 of epoch 198).\n",
            "Loss: 0.099534\n",
            "[+][2020-04-04 18:57:53.554931] Training iteration 794 (batch 2 of epoch 198).\n",
            "Loss: 0.107005\n",
            "[+][2020-04-04 18:57:53.843507] Training iteration 795 (batch 3 of epoch 198).\n",
            "Loss: 0.106585\n",
            "[+][2020-04-04 18:57:54.126808] Training iteration 796 (batch 4 of epoch 198).\n",
            "Loss: 0.106023\n",
            "[+][2020-04-04 18:57:54.402603] Training iteration 797 (batch 1 of epoch 199).\n",
            "Loss: 0.107139\n",
            "[+][2020-04-04 18:57:54.678150] Training iteration 798 (batch 2 of epoch 199).\n",
            "Loss: 0.083727\n",
            "[+][2020-04-04 18:57:54.958207] Training iteration 799 (batch 3 of epoch 199).\n",
            "Loss: 0.097228\n",
            "[+][2020-04-04 18:57:55.235465] Training iteration 800 (batch 4 of epoch 199).\n",
            "Loss: 0.115371\n",
            "[+][2020-04-04 18:57:55.514622] Training iteration 801 (batch 1 of epoch 200).\n",
            "Loss: 0.114783\n",
            "[+][2020-04-04 18:57:55.789912] Training iteration 802 (batch 2 of epoch 200).\n",
            "Loss: 0.092081\n",
            "[+][2020-04-04 18:57:56.066858] Training iteration 803 (batch 3 of epoch 200).\n",
            "Loss: 0.084712\n",
            "[+][2020-04-04 18:57:56.341104] Training iteration 804 (batch 4 of epoch 200).\n",
            "Loss: 0.123901\n",
            "[+][2020-04-04 18:57:56.627553] Training iteration 805 (batch 1 of epoch 201).\n",
            "Loss: 0.111524\n",
            "[+][2020-04-04 18:57:56.907468] Training iteration 806 (batch 2 of epoch 201).\n",
            "Loss: 0.111640\n",
            "[+][2020-04-04 18:57:57.184006] Training iteration 807 (batch 3 of epoch 201).\n",
            "Loss: 0.109616\n",
            "[+][2020-04-04 18:57:57.465673] Training iteration 808 (batch 4 of epoch 201).\n",
            "Loss: 0.111413\n",
            "[+][2020-04-04 18:57:57.737529] Training iteration 809 (batch 1 of epoch 202).\n",
            "Loss: 0.105673\n",
            "[+][2020-04-04 18:57:58.014680] Training iteration 810 (batch 2 of epoch 202).\n",
            "Loss: 0.099538\n",
            "[+][2020-04-04 18:57:58.291980] Training iteration 811 (batch 3 of epoch 202).\n",
            "Loss: 0.108308\n",
            "[+][2020-04-04 18:57:58.571253] Training iteration 812 (batch 4 of epoch 202).\n",
            "Loss: 0.121222\n",
            "[+][2020-04-04 18:57:58.844248] Training iteration 813 (batch 1 of epoch 203).\n",
            "Loss: 0.103983\n",
            "[+][2020-04-04 18:57:59.135821] Training iteration 814 (batch 2 of epoch 203).\n",
            "Loss: 0.107075\n",
            "[+][2020-04-04 18:57:59.414160] Training iteration 815 (batch 3 of epoch 203).\n",
            "Loss: 0.099246\n",
            "[+][2020-04-04 18:57:59.686936] Training iteration 816 (batch 4 of epoch 203).\n",
            "Loss: 0.116239\n",
            "[+][2020-04-04 18:57:59.968784] Training iteration 817 (batch 1 of epoch 204).\n",
            "Loss: 0.100879\n",
            "[+][2020-04-04 18:58:00.244858] Training iteration 818 (batch 2 of epoch 204).\n",
            "Loss: 0.118024\n",
            "[+][2020-04-04 18:58:00.520117] Training iteration 819 (batch 3 of epoch 204).\n",
            "Loss: 0.100088\n",
            "[+][2020-04-04 18:58:00.794640] Training iteration 820 (batch 4 of epoch 204).\n",
            "Loss: 0.101986\n",
            "[+][2020-04-04 18:58:01.075740] Training iteration 821 (batch 1 of epoch 205).\n",
            "Loss: 0.121248\n",
            "[+][2020-04-04 18:58:01.348919] Training iteration 822 (batch 2 of epoch 205).\n",
            "Loss: 0.103727\n",
            "[+][2020-04-04 18:58:01.619704] Training iteration 823 (batch 3 of epoch 205).\n",
            "Loss: 0.108235\n",
            "[+][2020-04-04 18:58:01.904820] Training iteration 824 (batch 4 of epoch 205).\n",
            "Loss: 0.102571\n",
            "[+][2020-04-04 18:58:02.197760] Training iteration 825 (batch 1 of epoch 206).\n",
            "Loss: 0.101436\n",
            "[+][2020-04-04 18:58:02.472619] Training iteration 826 (batch 2 of epoch 206).\n",
            "Loss: 0.099536\n",
            "[+][2020-04-04 18:58:02.748815] Training iteration 827 (batch 3 of epoch 206).\n",
            "Loss: 0.096854\n",
            "[+][2020-04-04 18:58:03.030114] Training iteration 828 (batch 4 of epoch 206).\n",
            "Loss: 0.102279\n",
            "[+][2020-04-04 18:58:03.301806] Training iteration 829 (batch 1 of epoch 207).\n",
            "Loss: 0.091769\n",
            "[+][2020-04-04 18:58:03.581110] Training iteration 830 (batch 2 of epoch 207).\n",
            "Loss: 0.087146\n",
            "[+][2020-04-04 18:58:03.856630] Training iteration 831 (batch 3 of epoch 207).\n",
            "Loss: 0.107633\n",
            "[+][2020-04-04 18:58:04.135112] Training iteration 832 (batch 4 of epoch 207).\n",
            "Loss: 0.097941\n",
            "[+][2020-04-04 18:58:04.420748] Training iteration 833 (batch 1 of epoch 208).\n",
            "Loss: 0.091519\n",
            "[+][2020-04-04 18:58:04.695938] Training iteration 834 (batch 2 of epoch 208).\n",
            "Loss: 0.111133\n",
            "[+][2020-04-04 18:58:04.967556] Training iteration 835 (batch 3 of epoch 208).\n",
            "Loss: 0.089583\n",
            "[+][2020-04-04 18:58:05.285564] Training iteration 836 (batch 4 of epoch 208).\n",
            "Loss: 0.099386\n",
            "[+][2020-04-04 18:58:05.560450] Training iteration 837 (batch 1 of epoch 209).\n",
            "Loss: 0.100742\n",
            "[+][2020-04-04 18:58:05.835554] Training iteration 838 (batch 2 of epoch 209).\n",
            "Loss: 0.094795\n",
            "[+][2020-04-04 18:58:06.115024] Training iteration 839 (batch 3 of epoch 209).\n",
            "Loss: 0.101059\n",
            "[+][2020-04-04 18:58:06.393773] Training iteration 840 (batch 4 of epoch 209).\n",
            "Loss: 0.102170\n",
            "[+][2020-04-04 18:58:06.666276] Training iteration 841 (batch 1 of epoch 210).\n",
            "Loss: 0.098658\n",
            "[+][2020-04-04 18:58:06.949050] Training iteration 842 (batch 2 of epoch 210).\n",
            "Loss: 0.102823\n",
            "[+][2020-04-04 18:58:07.232409] Training iteration 843 (batch 3 of epoch 210).\n",
            "Loss: 0.101091\n",
            "[+][2020-04-04 18:58:07.508022] Training iteration 844 (batch 4 of epoch 210).\n",
            "Loss: 0.093158\n",
            "[+][2020-04-04 18:58:07.785330] Training iteration 845 (batch 1 of epoch 211).\n",
            "Loss: 0.086405\n",
            "[+][2020-04-04 18:58:08.058923] Training iteration 846 (batch 2 of epoch 211).\n",
            "Loss: 0.106448\n",
            "[+][2020-04-04 18:58:08.335256] Training iteration 847 (batch 3 of epoch 211).\n",
            "Loss: 0.095507\n",
            "[+][2020-04-04 18:58:08.611846] Training iteration 848 (batch 4 of epoch 211).\n",
            "Loss: 0.097725\n",
            "[+][2020-04-04 18:58:08.882536] Training iteration 849 (batch 1 of epoch 212).\n",
            "Loss: 0.089230\n",
            "[+][2020-04-04 18:58:09.167311] Training iteration 850 (batch 2 of epoch 212).\n",
            "Loss: 0.092129\n",
            "[+][2020-04-04 18:58:09.439648] Training iteration 851 (batch 3 of epoch 212).\n",
            "Loss: 0.108918\n",
            "[+][2020-04-04 18:58:09.711438] Training iteration 852 (batch 4 of epoch 212).\n",
            "Loss: 0.099931\n",
            "[+][2020-04-04 18:58:09.992854] Training iteration 853 (batch 1 of epoch 213).\n",
            "Loss: 0.105895\n",
            "[+][2020-04-04 18:58:10.268937] Training iteration 854 (batch 2 of epoch 213).\n",
            "Loss: 0.100818\n",
            "[+][2020-04-04 18:58:10.543597] Training iteration 855 (batch 3 of epoch 213).\n",
            "Loss: 0.113168\n",
            "[+][2020-04-04 18:58:10.815442] Training iteration 856 (batch 4 of epoch 213).\n",
            "Loss: 0.098080\n",
            "[+][2020-04-04 18:58:11.085184] Training iteration 857 (batch 1 of epoch 214).\n",
            "Loss: 0.107722\n",
            "[+][2020-04-04 18:58:11.361309] Training iteration 858 (batch 2 of epoch 214).\n",
            "Loss: 0.120365\n",
            "[+][2020-04-04 18:58:11.637832] Training iteration 859 (batch 3 of epoch 214).\n",
            "Loss: 0.104019\n",
            "[+][2020-04-04 18:58:11.908916] Training iteration 860 (batch 4 of epoch 214).\n",
            "Loss: 0.097714\n",
            "[+][2020-04-04 18:58:12.214926] Training iteration 861 (batch 1 of epoch 215).\n",
            "Loss: 0.095667\n",
            "[+][2020-04-04 18:58:12.502635] Training iteration 862 (batch 2 of epoch 215).\n",
            "Loss: 0.110902\n",
            "[+][2020-04-04 18:58:12.773167] Training iteration 863 (batch 3 of epoch 215).\n",
            "Loss: 0.113940\n",
            "[+][2020-04-04 18:58:13.046793] Training iteration 864 (batch 4 of epoch 215).\n",
            "Loss: 0.102540\n",
            "[+][2020-04-04 18:58:13.329914] Training iteration 865 (batch 1 of epoch 216).\n",
            "Loss: 0.103287\n",
            "[+][2020-04-04 18:58:13.607244] Training iteration 866 (batch 2 of epoch 216).\n",
            "Loss: 0.133311\n",
            "[+][2020-04-04 18:58:13.877861] Training iteration 867 (batch 3 of epoch 216).\n",
            "Loss: 0.111532\n",
            "[+][2020-04-04 18:58:14.153154] Training iteration 868 (batch 4 of epoch 216).\n",
            "Loss: 0.109816\n",
            "[+][2020-04-04 18:58:14.437521] Training iteration 869 (batch 1 of epoch 217).\n",
            "Loss: 0.093018\n",
            "[+][2020-04-04 18:58:14.714002] Training iteration 870 (batch 2 of epoch 217).\n",
            "Loss: 0.099814\n",
            "[+][2020-04-04 18:58:14.993206] Training iteration 871 (batch 3 of epoch 217).\n",
            "Loss: 0.104187\n",
            "[+][2020-04-04 18:58:15.283704] Training iteration 872 (batch 4 of epoch 217).\n",
            "Loss: 0.112409\n",
            "[+][2020-04-04 18:58:15.565342] Training iteration 873 (batch 1 of epoch 218).\n",
            "Loss: 0.105827\n",
            "[+][2020-04-04 18:58:15.837144] Training iteration 874 (batch 2 of epoch 218).\n",
            "Loss: 0.117696\n",
            "[+][2020-04-04 18:58:16.117420] Training iteration 875 (batch 3 of epoch 218).\n",
            "Loss: 0.099174\n",
            "[+][2020-04-04 18:58:16.398648] Training iteration 876 (batch 4 of epoch 218).\n",
            "Loss: 0.109514\n",
            "[+][2020-04-04 18:58:16.678213] Training iteration 877 (batch 1 of epoch 219).\n",
            "Loss: 0.106057\n",
            "[+][2020-04-04 18:58:16.949098] Training iteration 878 (batch 2 of epoch 219).\n",
            "Loss: 0.112371\n",
            "[+][2020-04-04 18:58:17.232983] Training iteration 879 (batch 3 of epoch 219).\n",
            "Loss: 0.105440\n",
            "[+][2020-04-04 18:58:17.514260] Training iteration 880 (batch 4 of epoch 219).\n",
            "Loss: 0.109163\n",
            "[+][2020-04-04 18:58:17.790183] Training iteration 881 (batch 1 of epoch 220).\n",
            "Loss: 0.104262\n",
            "[+][2020-04-04 18:58:18.065703] Training iteration 882 (batch 2 of epoch 220).\n",
            "Loss: 0.105266\n",
            "[+][2020-04-04 18:58:18.352110] Training iteration 883 (batch 3 of epoch 220).\n",
            "Loss: 0.122014\n",
            "[+][2020-04-04 18:58:18.642814] Training iteration 884 (batch 4 of epoch 220).\n",
            "Loss: 0.113086\n",
            "[+][2020-04-04 18:58:18.918000] Training iteration 885 (batch 1 of epoch 221).\n",
            "Loss: 0.104408\n",
            "[+][2020-04-04 18:58:19.206271] Training iteration 886 (batch 2 of epoch 221).\n",
            "Loss: 0.119078\n",
            "[+][2020-04-04 18:58:19.490517] Training iteration 887 (batch 3 of epoch 221).\n",
            "Loss: 0.099594\n",
            "[+][2020-04-04 18:58:19.769211] Training iteration 888 (batch 4 of epoch 221).\n",
            "Loss: 0.107960\n",
            "[+][2020-04-04 18:58:20.040410] Training iteration 889 (batch 1 of epoch 222).\n",
            "Loss: 0.115010\n",
            "[+][2020-04-04 18:58:20.331116] Training iteration 890 (batch 2 of epoch 222).\n",
            "Loss: 0.113371\n",
            "[+][2020-04-04 18:58:20.608727] Training iteration 891 (batch 3 of epoch 222).\n",
            "Loss: 0.116028\n",
            "[+][2020-04-04 18:58:20.881533] Training iteration 892 (batch 4 of epoch 222).\n",
            "Loss: 0.118849\n",
            "[+][2020-04-04 18:58:21.175029] Training iteration 893 (batch 1 of epoch 223).\n",
            "Loss: 0.116093\n",
            "[+][2020-04-04 18:58:21.453793] Training iteration 894 (batch 2 of epoch 223).\n",
            "Loss: 0.118526\n",
            "[+][2020-04-04 18:58:21.731154] Training iteration 895 (batch 3 of epoch 223).\n",
            "Loss: 0.111741\n",
            "[+][2020-04-04 18:58:22.007157] Training iteration 896 (batch 4 of epoch 223).\n",
            "Loss: 0.114391\n",
            "[+][2020-04-04 18:58:22.280147] Training iteration 897 (batch 1 of epoch 224).\n",
            "Loss: 0.112167\n",
            "[+][2020-04-04 18:58:22.567180] Training iteration 898 (batch 2 of epoch 224).\n",
            "Loss: 0.105195\n",
            "[+][2020-04-04 18:58:22.839709] Training iteration 899 (batch 3 of epoch 224).\n",
            "Loss: 0.098941\n",
            "[+][2020-04-04 18:58:23.127719] Training iteration 900 (batch 4 of epoch 224).\n",
            "Loss: 0.101053\n",
            "[+][2020-04-04 18:58:23.432548] Training iteration 901 (batch 1 of epoch 225).\n",
            "Loss: 0.104513\n",
            "[+][2020-04-04 18:58:23.708910] Training iteration 902 (batch 2 of epoch 225).\n",
            "Loss: 0.110561\n",
            "[+][2020-04-04 18:58:24.005411] Training iteration 903 (batch 3 of epoch 225).\n",
            "Loss: 0.102821\n",
            "[+][2020-04-04 18:58:24.282137] Training iteration 904 (batch 4 of epoch 225).\n",
            "Loss: 0.103170\n",
            "[+][2020-04-04 18:58:24.563214] Training iteration 905 (batch 1 of epoch 226).\n",
            "Loss: 0.117167\n",
            "[+][2020-04-04 18:58:24.839163] Training iteration 906 (batch 2 of epoch 226).\n",
            "Loss: 0.125926\n",
            "[+][2020-04-04 18:58:25.113142] Training iteration 907 (batch 3 of epoch 226).\n",
            "Loss: 0.105666\n",
            "[+][2020-04-04 18:58:25.385552] Training iteration 908 (batch 4 of epoch 226).\n",
            "Loss: 0.116581\n",
            "[+][2020-04-04 18:58:25.672252] Training iteration 909 (batch 1 of epoch 227).\n",
            "Loss: 0.109811\n",
            "[+][2020-04-04 18:58:25.949618] Training iteration 910 (batch 2 of epoch 227).\n",
            "Loss: 0.123135\n",
            "[+][2020-04-04 18:58:26.236963] Training iteration 911 (batch 3 of epoch 227).\n",
            "Loss: 0.111413\n",
            "[+][2020-04-04 18:58:26.517331] Training iteration 912 (batch 4 of epoch 227).\n",
            "Loss: 0.114082\n",
            "[+][2020-04-04 18:58:26.886532] Training iteration 913 (batch 1 of epoch 228).\n",
            "Loss: 0.114275\n",
            "[+][2020-04-04 18:58:27.157404] Training iteration 914 (batch 2 of epoch 228).\n",
            "Loss: 0.112854\n",
            "[+][2020-04-04 18:58:27.447473] Training iteration 915 (batch 3 of epoch 228).\n",
            "Loss: 0.108185\n",
            "[+][2020-04-04 18:58:27.731042] Training iteration 916 (batch 4 of epoch 228).\n",
            "Loss: 0.119497\n",
            "[+][2020-04-04 18:58:28.003248] Training iteration 917 (batch 1 of epoch 229).\n",
            "Loss: 0.115583\n",
            "[+][2020-04-04 18:58:28.275948] Training iteration 918 (batch 2 of epoch 229).\n",
            "Loss: 0.099082\n",
            "[+][2020-04-04 18:58:28.555626] Training iteration 919 (batch 3 of epoch 229).\n",
            "Loss: 0.095809\n",
            "[+][2020-04-04 18:58:28.840578] Training iteration 920 (batch 4 of epoch 229).\n",
            "Loss: 0.122545\n",
            "[+][2020-04-04 18:58:29.113592] Training iteration 921 (batch 1 of epoch 230).\n",
            "Loss: 0.110256\n",
            "[+][2020-04-04 18:58:29.388542] Training iteration 922 (batch 2 of epoch 230).\n",
            "Loss: 0.108069\n",
            "[+][2020-04-04 18:58:29.679828] Training iteration 923 (batch 3 of epoch 230).\n",
            "Loss: 0.103741\n",
            "[+][2020-04-04 18:58:29.947722] Training iteration 924 (batch 4 of epoch 230).\n",
            "Loss: 0.101955\n",
            "[+][2020-04-04 18:58:30.222715] Training iteration 925 (batch 1 of epoch 231).\n",
            "Loss: 0.124378\n",
            "[+][2020-04-04 18:58:30.497143] Training iteration 926 (batch 2 of epoch 231).\n",
            "Loss: 0.110116\n",
            "[+][2020-04-04 18:58:30.779746] Training iteration 927 (batch 3 of epoch 231).\n",
            "Loss: 0.111429\n",
            "[+][2020-04-04 18:58:31.052983] Training iteration 928 (batch 4 of epoch 231).\n",
            "Loss: 0.100441\n",
            "[+][2020-04-04 18:58:31.327137] Training iteration 929 (batch 1 of epoch 232).\n",
            "Loss: 0.105108\n",
            "[+][2020-04-04 18:58:31.602471] Training iteration 930 (batch 2 of epoch 232).\n",
            "Loss: 0.109841\n",
            "[+][2020-04-04 18:58:31.875830] Training iteration 931 (batch 3 of epoch 232).\n",
            "Loss: 0.099254\n",
            "[+][2020-04-04 18:58:32.156433] Training iteration 932 (batch 4 of epoch 232).\n",
            "Loss: 0.110785\n",
            "[+][2020-04-04 18:58:32.433060] Training iteration 933 (batch 1 of epoch 233).\n",
            "Loss: 0.134299\n",
            "[+][2020-04-04 18:58:32.714624] Training iteration 934 (batch 2 of epoch 233).\n",
            "Loss: 0.103476\n",
            "[+][2020-04-04 18:58:32.984791] Training iteration 935 (batch 3 of epoch 233).\n",
            "Loss: 0.099398\n",
            "[+][2020-04-04 18:58:33.259690] Training iteration 936 (batch 4 of epoch 233).\n",
            "Loss: 0.109068\n",
            "[+][2020-04-04 18:58:33.530555] Training iteration 937 (batch 1 of epoch 234).\n",
            "Loss: 0.110636\n",
            "[+][2020-04-04 18:58:33.811947] Training iteration 938 (batch 2 of epoch 234).\n",
            "Loss: 0.106140\n",
            "[+][2020-04-04 18:58:34.085353] Training iteration 939 (batch 3 of epoch 234).\n",
            "Loss: 0.120499\n",
            "[+][2020-04-04 18:58:34.359571] Training iteration 940 (batch 4 of epoch 234).\n",
            "Loss: 0.102775\n",
            "[+][2020-04-04 18:58:34.639095] Training iteration 941 (batch 1 of epoch 235).\n",
            "Loss: 0.089628\n",
            "[+][2020-04-04 18:58:34.929413] Training iteration 942 (batch 2 of epoch 235).\n",
            "Loss: 0.097590\n",
            "[+][2020-04-04 18:58:35.202807] Training iteration 943 (batch 3 of epoch 235).\n",
            "Loss: 0.098365\n",
            "[+][2020-04-04 18:58:35.476067] Training iteration 944 (batch 4 of epoch 235).\n",
            "Loss: 0.109672\n",
            "[+][2020-04-04 18:58:35.763672] Training iteration 945 (batch 1 of epoch 236).\n",
            "Loss: 0.115169\n",
            "[+][2020-04-04 18:58:36.032831] Training iteration 946 (batch 2 of epoch 236).\n",
            "Loss: 0.098500\n",
            "[+][2020-04-04 18:58:36.304043] Training iteration 947 (batch 3 of epoch 236).\n",
            "Loss: 0.119656\n",
            "[+][2020-04-04 18:58:36.579443] Training iteration 948 (batch 4 of epoch 236).\n",
            "Loss: 0.104759\n",
            "[+][2020-04-04 18:58:36.866007] Training iteration 949 (batch 1 of epoch 237).\n",
            "Loss: 0.108439\n",
            "[+][2020-04-04 18:58:37.161130] Training iteration 950 (batch 2 of epoch 237).\n",
            "Loss: 0.115632\n",
            "[+][2020-04-04 18:58:37.463768] Training iteration 951 (batch 3 of epoch 237).\n",
            "Loss: 0.112715\n",
            "[+][2020-04-04 18:58:37.746378] Training iteration 952 (batch 4 of epoch 237).\n",
            "Loss: 0.099984\n",
            "[+][2020-04-04 18:58:38.033690] Training iteration 953 (batch 1 of epoch 238).\n",
            "Loss: 0.102155\n",
            "[+][2020-04-04 18:58:38.309986] Training iteration 954 (batch 2 of epoch 238).\n",
            "Loss: 0.098830\n",
            "[+][2020-04-04 18:58:38.586573] Training iteration 955 (batch 3 of epoch 238).\n",
            "Loss: 0.100876\n",
            "[+][2020-04-04 18:58:38.872933] Training iteration 956 (batch 4 of epoch 238).\n",
            "Loss: 0.094005\n",
            "[+][2020-04-04 18:58:39.150395] Training iteration 957 (batch 1 of epoch 239).\n",
            "Loss: 0.103865\n",
            "[+][2020-04-04 18:58:39.423523] Training iteration 958 (batch 2 of epoch 239).\n",
            "Loss: 0.113439\n",
            "[+][2020-04-04 18:58:39.717124] Training iteration 959 (batch 3 of epoch 239).\n",
            "Loss: 0.091259\n",
            "[+][2020-04-04 18:58:39.996947] Training iteration 960 (batch 4 of epoch 239).\n",
            "Loss: 0.114059\n",
            "[+][2020-04-04 18:58:40.271022] Training iteration 961 (batch 1 of epoch 240).\n",
            "Loss: 0.111312\n",
            "[+][2020-04-04 18:58:40.544894] Training iteration 962 (batch 2 of epoch 240).\n",
            "Loss: 0.103968\n",
            "[+][2020-04-04 18:58:40.828390] Training iteration 963 (batch 3 of epoch 240).\n",
            "Loss: 0.109339\n",
            "[+][2020-04-04 18:58:41.100708] Training iteration 964 (batch 4 of epoch 240).\n",
            "Loss: 0.109244\n",
            "[+][2020-04-04 18:58:41.370923] Training iteration 965 (batch 1 of epoch 241).\n",
            "Loss: 0.106416\n",
            "[+][2020-04-04 18:58:41.652051] Training iteration 966 (batch 2 of epoch 241).\n",
            "Loss: 0.104452\n",
            "[+][2020-04-04 18:58:41.936695] Training iteration 967 (batch 3 of epoch 241).\n",
            "Loss: 0.114615\n",
            "[+][2020-04-04 18:58:42.209371] Training iteration 968 (batch 4 of epoch 241).\n",
            "Loss: 0.114694\n",
            "[+][2020-04-04 18:58:42.482660] Training iteration 969 (batch 1 of epoch 242).\n",
            "Loss: 0.104119\n",
            "[+][2020-04-04 18:58:42.757003] Training iteration 970 (batch 2 of epoch 242).\n",
            "Loss: 0.108790\n",
            "[+][2020-04-04 18:58:43.040761] Training iteration 971 (batch 3 of epoch 242).\n",
            "Loss: 0.086817\n",
            "[+][2020-04-04 18:58:43.338473] Training iteration 972 (batch 4 of epoch 242).\n",
            "Loss: 0.117158\n",
            "[+][2020-04-04 18:58:43.611850] Training iteration 973 (batch 1 of epoch 243).\n",
            "Loss: 0.111150\n",
            "[+][2020-04-04 18:58:43.892931] Training iteration 974 (batch 2 of epoch 243).\n",
            "Loss: 0.108864\n",
            "[+][2020-04-04 18:58:44.173913] Training iteration 975 (batch 3 of epoch 243).\n",
            "Loss: 0.104504\n",
            "[+][2020-04-04 18:58:44.452698] Training iteration 976 (batch 4 of epoch 243).\n",
            "Loss: 0.108838\n",
            "[+][2020-04-04 18:58:44.725251] Training iteration 977 (batch 1 of epoch 244).\n",
            "Loss: 0.127678\n",
            "[+][2020-04-04 18:58:45.010567] Training iteration 978 (batch 2 of epoch 244).\n",
            "Loss: 0.118765\n",
            "[+][2020-04-04 18:58:45.282997] Training iteration 979 (batch 3 of epoch 244).\n",
            "Loss: 0.113490\n",
            "[+][2020-04-04 18:58:45.553291] Training iteration 980 (batch 4 of epoch 244).\n",
            "Loss: 0.122960\n",
            "[+][2020-04-04 18:58:45.830255] Training iteration 981 (batch 1 of epoch 245).\n",
            "Loss: 0.109126\n",
            "[+][2020-04-04 18:58:46.113966] Training iteration 982 (batch 2 of epoch 245).\n",
            "Loss: 0.108700\n",
            "[+][2020-04-04 18:58:46.384551] Training iteration 983 (batch 3 of epoch 245).\n",
            "Loss: 0.114430\n",
            "[+][2020-04-04 18:58:46.659845] Training iteration 984 (batch 4 of epoch 245).\n",
            "Loss: 0.102138\n",
            "[+][2020-04-04 18:58:46.943856] Training iteration 985 (batch 1 of epoch 246).\n",
            "Loss: 0.102596\n",
            "[+][2020-04-04 18:58:47.253840] Training iteration 986 (batch 2 of epoch 246).\n",
            "Loss: 0.114358\n",
            "[+][2020-04-04 18:58:47.538310] Training iteration 987 (batch 3 of epoch 246).\n",
            "Loss: 0.110293\n",
            "[+][2020-04-04 18:58:47.848694] Training iteration 988 (batch 4 of epoch 246).\n",
            "Loss: 0.105287\n",
            "[+][2020-04-04 18:58:48.165452] Training iteration 989 (batch 1 of epoch 247).\n",
            "Loss: 0.124758\n",
            "[+][2020-04-04 18:58:48.438143] Training iteration 990 (batch 2 of epoch 247).\n",
            "Loss: 0.100632\n",
            "[+][2020-04-04 18:58:48.714225] Training iteration 991 (batch 3 of epoch 247).\n",
            "Loss: 0.111521\n",
            "[+][2020-04-04 18:58:49.036257] Training iteration 992 (batch 4 of epoch 247).\n",
            "Loss: 0.108036\n",
            "[+][2020-04-04 18:58:49.332585] Training iteration 993 (batch 1 of epoch 248).\n",
            "Loss: 0.106388\n",
            "[+][2020-04-04 18:58:49.626017] Training iteration 994 (batch 2 of epoch 248).\n",
            "Loss: 0.119335\n",
            "[+][2020-04-04 18:58:49.963002] Training iteration 995 (batch 3 of epoch 248).\n",
            "Loss: 0.108568\n",
            "[+][2020-04-04 18:58:50.273481] Training iteration 996 (batch 4 of epoch 248).\n",
            "Loss: 0.107806\n",
            "[+][2020-04-04 18:58:50.552416] Training iteration 997 (batch 1 of epoch 249).\n",
            "Loss: 0.105963\n",
            "[+][2020-04-04 18:58:50.849975] Training iteration 998 (batch 2 of epoch 249).\n",
            "Loss: 0.086244\n",
            "[+][2020-04-04 18:58:51.175290] Training iteration 999 (batch 3 of epoch 249).\n",
            "Loss: 0.076331\n",
            "[+][2020-04-04 18:58:51.465823] Training iteration 1000 (batch 4 of epoch 249).\n",
            "Loss: 0.102404\n",
            "[+][2020-04-04 18:58:51.756284] Training iteration 1001 (batch 1 of epoch 250).\n",
            "Loss: 0.103709\n",
            "[+][2020-04-04 18:58:52.063517] Training iteration 1002 (batch 2 of epoch 250).\n",
            "Loss: 0.112172\n",
            "[+][2020-04-04 18:58:52.361299] Training iteration 1003 (batch 3 of epoch 250).\n",
            "Loss: 0.104892\n",
            "[+][2020-04-04 18:58:52.659938] Training iteration 1004 (batch 4 of epoch 250).\n",
            "Loss: 0.092776\n",
            "[+][2020-04-04 18:58:52.954435] Training iteration 1005 (batch 1 of epoch 251).\n",
            "Loss: 0.091694\n",
            "[+][2020-04-04 18:58:53.252713] Training iteration 1006 (batch 2 of epoch 251).\n",
            "Loss: 0.103571\n",
            "[+][2020-04-04 18:58:53.549989] Training iteration 1007 (batch 3 of epoch 251).\n",
            "Loss: 0.124181\n",
            "[+][2020-04-04 18:58:53.836405] Training iteration 1008 (batch 4 of epoch 251).\n",
            "Loss: 0.093009\n",
            "[+][2020-04-04 18:58:54.150496] Training iteration 1009 (batch 1 of epoch 252).\n",
            "Loss: 0.092851\n",
            "[+][2020-04-04 18:58:54.448937] Training iteration 1010 (batch 2 of epoch 252).\n",
            "Loss: 0.103705\n",
            "[+][2020-04-04 18:58:54.726972] Training iteration 1011 (batch 3 of epoch 252).\n",
            "Loss: 0.118129\n",
            "[+][2020-04-04 18:58:55.024714] Training iteration 1012 (batch 4 of epoch 252).\n",
            "Loss: 0.115710\n",
            "[+][2020-04-04 18:58:55.314093] Training iteration 1013 (batch 1 of epoch 253).\n",
            "Loss: 0.116502\n",
            "[+][2020-04-04 18:58:55.591162] Training iteration 1014 (batch 2 of epoch 253).\n",
            "Loss: 0.099854\n",
            "[+][2020-04-04 18:58:55.874211] Training iteration 1015 (batch 3 of epoch 253).\n",
            "Loss: 0.116516\n",
            "[+][2020-04-04 18:58:56.172959] Training iteration 1016 (batch 4 of epoch 253).\n",
            "Loss: 0.111916\n",
            "[+][2020-04-04 18:58:56.450630] Training iteration 1017 (batch 1 of epoch 254).\n",
            "Loss: 0.109222\n",
            "[+][2020-04-04 18:58:56.730319] Training iteration 1018 (batch 2 of epoch 254).\n",
            "Loss: 0.104489\n",
            "[+][2020-04-04 18:58:57.016563] Training iteration 1019 (batch 3 of epoch 254).\n",
            "Loss: 0.113705\n",
            "[+][2020-04-04 18:58:57.304515] Training iteration 1020 (batch 4 of epoch 254).\n",
            "Loss: 0.105362\n",
            "[+][2020-04-04 18:58:57.592734] Training iteration 1021 (batch 1 of epoch 255).\n",
            "Loss: 0.099163\n",
            "[+][2020-04-04 18:58:57.895814] Training iteration 1022 (batch 2 of epoch 255).\n",
            "Loss: 0.089537\n",
            "[+][2020-04-04 18:58:58.203216] Training iteration 1023 (batch 3 of epoch 255).\n",
            "Loss: 0.110950\n",
            "[+][2020-04-04 18:58:58.482285] Training iteration 1024 (batch 4 of epoch 255).\n",
            "Loss: 0.107254\n",
            "[+][2020-04-04 18:58:58.761299] Training iteration 1025 (batch 1 of epoch 256).\n",
            "Loss: 0.103452\n",
            "[+][2020-04-04 18:58:59.051110] Training iteration 1026 (batch 2 of epoch 256).\n",
            "Loss: 0.100735\n",
            "[+][2020-04-04 18:58:59.338782] Training iteration 1027 (batch 3 of epoch 256).\n",
            "Loss: 0.098960\n",
            "[+][2020-04-04 18:58:59.618299] Training iteration 1028 (batch 4 of epoch 256).\n",
            "Loss: 0.103521\n",
            "[+][2020-04-04 18:58:59.893608] Training iteration 1029 (batch 1 of epoch 257).\n",
            "Loss: 0.103228\n",
            "[+][2020-04-04 18:59:00.212984] Training iteration 1030 (batch 2 of epoch 257).\n",
            "Loss: 0.090319\n",
            "[+][2020-04-04 18:59:00.494579] Training iteration 1031 (batch 3 of epoch 257).\n",
            "Loss: 0.098224\n",
            "[+][2020-04-04 18:59:00.769020] Training iteration 1032 (batch 4 of epoch 257).\n",
            "Loss: 0.107807\n",
            "[+][2020-04-04 18:59:01.050706] Training iteration 1033 (batch 1 of epoch 258).\n",
            "Loss: 0.103984\n",
            "[+][2020-04-04 18:59:01.355342] Training iteration 1034 (batch 2 of epoch 258).\n",
            "Loss: 0.088023\n",
            "[+][2020-04-04 18:59:01.632165] Training iteration 1035 (batch 3 of epoch 258).\n",
            "Loss: 0.087905\n",
            "[+][2020-04-04 18:59:01.907344] Training iteration 1036 (batch 4 of epoch 258).\n",
            "Loss: 0.094112\n",
            "[+][2020-04-04 18:59:02.213536] Training iteration 1037 (batch 1 of epoch 259).\n",
            "Loss: 0.096157\n",
            "[+][2020-04-04 18:59:02.487522] Training iteration 1038 (batch 2 of epoch 259).\n",
            "Loss: 0.094238\n",
            "[+][2020-04-04 18:59:02.771630] Training iteration 1039 (batch 3 of epoch 259).\n",
            "Loss: 0.108699\n",
            "[+][2020-04-04 18:59:03.046558] Training iteration 1040 (batch 4 of epoch 259).\n",
            "Loss: 0.087632\n",
            "[+][2020-04-04 18:59:03.327796] Training iteration 1041 (batch 1 of epoch 260).\n",
            "Loss: 0.096180\n",
            "[+][2020-04-04 18:59:03.601281] Training iteration 1042 (batch 2 of epoch 260).\n",
            "Loss: 0.094731\n",
            "[+][2020-04-04 18:59:03.872110] Training iteration 1043 (batch 3 of epoch 260).\n",
            "Loss: 0.112615\n",
            "[+][2020-04-04 18:59:04.191331] Training iteration 1044 (batch 4 of epoch 260).\n",
            "Loss: 0.104257\n",
            "[+][2020-04-04 18:59:04.479258] Training iteration 1045 (batch 1 of epoch 261).\n",
            "Loss: 0.092191\n",
            "[+][2020-04-04 18:59:04.754459] Training iteration 1046 (batch 2 of epoch 261).\n",
            "Loss: 0.084842\n",
            "[+][2020-04-04 18:59:05.031347] Training iteration 1047 (batch 3 of epoch 261).\n",
            "Loss: 0.120600\n",
            "[+][2020-04-04 18:59:05.321972] Training iteration 1048 (batch 4 of epoch 261).\n",
            "Loss: 0.097879\n",
            "[+][2020-04-04 18:59:05.593477] Training iteration 1049 (batch 1 of epoch 262).\n",
            "Loss: 0.096520\n",
            "[+][2020-04-04 18:59:05.877225] Training iteration 1050 (batch 2 of epoch 262).\n",
            "Loss: 0.091418\n",
            "[+][2020-04-04 18:59:06.183287] Training iteration 1051 (batch 3 of epoch 262).\n",
            "Loss: 0.112732\n",
            "[+][2020-04-04 18:59:06.470974] Training iteration 1052 (batch 4 of epoch 262).\n",
            "Loss: 0.092702\n",
            "[+][2020-04-04 18:59:06.745189] Training iteration 1053 (batch 1 of epoch 263).\n",
            "Loss: 0.106054\n",
            "[+][2020-04-04 18:59:07.019785] Training iteration 1054 (batch 2 of epoch 263).\n",
            "Loss: 0.109114\n",
            "[+][2020-04-04 18:59:07.302257] Training iteration 1055 (batch 3 of epoch 263).\n",
            "Loss: 0.094242\n",
            "[+][2020-04-04 18:59:07.593187] Training iteration 1056 (batch 4 of epoch 263).\n",
            "Loss: 0.093634\n",
            "[+][2020-04-04 18:59:07.869650] Training iteration 1057 (batch 1 of epoch 264).\n",
            "Loss: 0.107627\n",
            "[+][2020-04-04 18:59:08.187520] Training iteration 1058 (batch 2 of epoch 264).\n",
            "Loss: 0.121770\n",
            "[+][2020-04-04 18:59:08.491343] Training iteration 1059 (batch 3 of epoch 264).\n",
            "Loss: 0.103359\n",
            "[+][2020-04-04 18:59:08.764326] Training iteration 1060 (batch 4 of epoch 264).\n",
            "Loss: 0.097572\n",
            "[+][2020-04-04 18:59:09.037866] Training iteration 1061 (batch 1 of epoch 265).\n",
            "Loss: 0.109236\n",
            "[+][2020-04-04 18:59:09.322545] Training iteration 1062 (batch 2 of epoch 265).\n",
            "Loss: 0.111491\n",
            "[+][2020-04-04 18:59:09.605648] Training iteration 1063 (batch 3 of epoch 265).\n",
            "Loss: 0.101078\n",
            "[+][2020-04-04 18:59:09.878588] Training iteration 1064 (batch 4 of epoch 265).\n",
            "Loss: 0.100259\n",
            "[+][2020-04-04 18:59:10.160319] Training iteration 1065 (batch 1 of epoch 266).\n",
            "Loss: 0.099439\n",
            "[+][2020-04-04 18:59:10.451837] Training iteration 1066 (batch 2 of epoch 266).\n",
            "Loss: 0.103342\n",
            "[+][2020-04-04 18:59:10.725081] Training iteration 1067 (batch 3 of epoch 266).\n",
            "Loss: 0.103164\n",
            "[+][2020-04-04 18:59:11.003321] Training iteration 1068 (batch 4 of epoch 266).\n",
            "Loss: 0.104328\n",
            "[+][2020-04-04 18:59:11.295971] Training iteration 1069 (batch 1 of epoch 267).\n",
            "Loss: 0.099937\n",
            "[+][2020-04-04 18:59:11.579049] Training iteration 1070 (batch 2 of epoch 267).\n",
            "Loss: 0.108025\n",
            "[+][2020-04-04 18:59:11.859415] Training iteration 1071 (batch 3 of epoch 267).\n",
            "Loss: 0.094017\n",
            "[+][2020-04-04 18:59:12.168482] Training iteration 1072 (batch 4 of epoch 267).\n",
            "Loss: 0.099447\n",
            "[+][2020-04-04 18:59:12.449723] Training iteration 1073 (batch 1 of epoch 268).\n",
            "Loss: 0.095199\n",
            "[+][2020-04-04 18:59:12.727920] Training iteration 1074 (batch 2 of epoch 268).\n",
            "Loss: 0.094693\n",
            "[+][2020-04-04 18:59:13.008290] Training iteration 1075 (batch 3 of epoch 268).\n",
            "Loss: 0.103084\n",
            "[+][2020-04-04 18:59:13.287986] Training iteration 1076 (batch 4 of epoch 268).\n",
            "Loss: 0.095852\n",
            "[+][2020-04-04 18:59:13.580419] Training iteration 1077 (batch 1 of epoch 269).\n",
            "Loss: 0.086991\n",
            "[+][2020-04-04 18:59:13.855770] Training iteration 1078 (batch 2 of epoch 269).\n",
            "Loss: 0.099274\n",
            "[+][2020-04-04 18:59:14.137655] Training iteration 1079 (batch 3 of epoch 269).\n",
            "Loss: 0.110663\n",
            "[+][2020-04-04 18:59:14.418444] Training iteration 1080 (batch 4 of epoch 269).\n",
            "Loss: 0.088875\n",
            "[+][2020-04-04 18:59:14.703407] Training iteration 1081 (batch 1 of epoch 270).\n",
            "Loss: 0.116412\n",
            "[+][2020-04-04 18:59:14.975752] Training iteration 1082 (batch 2 of epoch 270).\n",
            "Loss: 0.101703\n",
            "[+][2020-04-04 18:59:15.268407] Training iteration 1083 (batch 3 of epoch 270).\n",
            "Loss: 0.109477\n",
            "[+][2020-04-04 18:59:15.550302] Training iteration 1084 (batch 4 of epoch 270).\n",
            "Loss: 0.106113\n",
            "[+][2020-04-04 18:59:15.825086] Training iteration 1085 (batch 1 of epoch 271).\n",
            "Loss: 0.103069\n",
            "[+][2020-04-04 18:59:16.134644] Training iteration 1086 (batch 2 of epoch 271).\n",
            "Loss: 0.104509\n",
            "[+][2020-04-04 18:59:16.429612] Training iteration 1087 (batch 3 of epoch 271).\n",
            "Loss: 0.111808\n",
            "[+][2020-04-04 18:59:16.719549] Training iteration 1088 (batch 4 of epoch 271).\n",
            "Loss: 0.114221\n",
            "[+][2020-04-04 18:59:16.998324] Training iteration 1089 (batch 1 of epoch 272).\n",
            "Loss: 0.107171\n",
            "[+][2020-04-04 18:59:17.285163] Training iteration 1090 (batch 2 of epoch 272).\n",
            "Loss: 0.112801\n",
            "[+][2020-04-04 18:59:17.569223] Training iteration 1091 (batch 3 of epoch 272).\n",
            "Loss: 0.111542\n",
            "[+][2020-04-04 18:59:17.845558] Training iteration 1092 (batch 4 of epoch 272).\n",
            "Loss: 0.102323\n",
            "[+][2020-04-04 18:59:18.138895] Training iteration 1093 (batch 1 of epoch 273).\n",
            "Loss: 0.091239\n",
            "[+][2020-04-04 18:59:18.424411] Training iteration 1094 (batch 2 of epoch 273).\n",
            "Loss: 0.096643\n",
            "[+][2020-04-04 18:59:18.708644] Training iteration 1095 (batch 3 of epoch 273).\n",
            "Loss: 0.098349\n",
            "[+][2020-04-04 18:59:18.984246] Training iteration 1096 (batch 4 of epoch 273).\n",
            "Loss: 0.095425\n",
            "[+][2020-04-04 18:59:19.270509] Training iteration 1097 (batch 1 of epoch 274).\n",
            "Loss: 0.110276\n",
            "[+][2020-04-04 18:59:19.543529] Training iteration 1098 (batch 2 of epoch 274).\n",
            "Loss: 0.101870\n",
            "[+][2020-04-04 18:59:19.823899] Training iteration 1099 (batch 3 of epoch 274).\n",
            "Loss: 0.089526\n",
            "[+][2020-04-04 18:59:20.098801] Training iteration 1100 (batch 4 of epoch 274).\n",
            "Loss: 0.101340\n",
            "[+][2020-04-04 18:59:20.378565] Training iteration 1101 (batch 1 of epoch 275).\n",
            "Loss: 0.119698\n",
            "[+][2020-04-04 18:59:20.654568] Training iteration 1102 (batch 2 of epoch 275).\n",
            "Loss: 0.111998\n",
            "[+][2020-04-04 18:59:20.928680] Training iteration 1103 (batch 3 of epoch 275).\n",
            "Loss: 0.115374\n",
            "[+][2020-04-04 18:59:21.215856] Training iteration 1104 (batch 4 of epoch 275).\n",
            "Loss: 0.108410\n",
            "[+][2020-04-04 18:59:21.493532] Training iteration 1105 (batch 1 of epoch 276).\n",
            "Loss: 0.103745\n",
            "[+][2020-04-04 18:59:21.774434] Training iteration 1106 (batch 2 of epoch 276).\n",
            "Loss: 0.089153\n",
            "[+][2020-04-04 18:59:22.050463] Training iteration 1107 (batch 3 of epoch 276).\n",
            "Loss: 0.107456\n",
            "[+][2020-04-04 18:59:22.330053] Training iteration 1108 (batch 4 of epoch 276).\n",
            "Loss: 0.105409\n",
            "[+][2020-04-04 18:59:22.611355] Training iteration 1109 (batch 1 of epoch 277).\n",
            "Loss: 0.103818\n",
            "[+][2020-04-04 18:59:22.894634] Training iteration 1110 (batch 2 of epoch 277).\n",
            "Loss: 0.107604\n",
            "[+][2020-04-04 18:59:23.170846] Training iteration 1111 (batch 3 of epoch 277).\n",
            "Loss: 0.105485\n",
            "[+][2020-04-04 18:59:23.450851] Training iteration 1112 (batch 4 of epoch 277).\n",
            "Loss: 0.101916\n",
            "[+][2020-04-04 18:59:23.738918] Training iteration 1113 (batch 1 of epoch 278).\n",
            "Loss: 0.108659\n",
            "[+][2020-04-04 18:59:24.026381] Training iteration 1114 (batch 2 of epoch 278).\n",
            "Loss: 0.095106\n",
            "[+][2020-04-04 18:59:24.309635] Training iteration 1115 (batch 3 of epoch 278).\n",
            "Loss: 0.107732\n",
            "[+][2020-04-04 18:59:24.584443] Training iteration 1116 (batch 4 of epoch 278).\n",
            "Loss: 0.091030\n",
            "[+][2020-04-04 18:59:24.871980] Training iteration 1117 (batch 1 of epoch 279).\n",
            "Loss: 0.124792\n",
            "[+][2020-04-04 18:59:25.149350] Training iteration 1118 (batch 2 of epoch 279).\n",
            "Loss: 0.130214\n",
            "[+][2020-04-04 18:59:25.430649] Training iteration 1119 (batch 3 of epoch 279).\n",
            "Loss: 0.112027\n",
            "[+][2020-04-04 18:59:25.712681] Training iteration 1120 (batch 4 of epoch 279).\n",
            "Loss: 0.110499\n",
            "[+][2020-04-04 18:59:25.989746] Training iteration 1121 (batch 1 of epoch 280).\n",
            "Loss: 0.101734\n",
            "[+][2020-04-04 18:59:26.267361] Training iteration 1122 (batch 2 of epoch 280).\n",
            "Loss: 0.100155\n",
            "[+][2020-04-04 18:59:26.553220] Training iteration 1123 (batch 3 of epoch 280).\n",
            "Loss: 0.096817\n",
            "[+][2020-04-04 18:59:26.831658] Training iteration 1124 (batch 4 of epoch 280).\n",
            "Loss: 0.097355\n",
            "[+][2020-04-04 18:59:27.105660] Training iteration 1125 (batch 1 of epoch 281).\n",
            "Loss: 0.101606\n",
            "[+][2020-04-04 18:59:27.388155] Training iteration 1126 (batch 2 of epoch 281).\n",
            "Loss: 0.110953\n",
            "[+][2020-04-04 18:59:27.658989] Training iteration 1127 (batch 3 of epoch 281).\n",
            "Loss: 0.104468\n",
            "[+][2020-04-04 18:59:27.936997] Training iteration 1128 (batch 4 of epoch 281).\n",
            "Loss: 0.110512\n",
            "[+][2020-04-04 18:59:28.220800] Training iteration 1129 (batch 1 of epoch 282).\n",
            "Loss: 0.103638\n",
            "[+][2020-04-04 18:59:28.500635] Training iteration 1130 (batch 2 of epoch 282).\n",
            "Loss: 0.099090\n",
            "[+][2020-04-04 18:59:28.779527] Training iteration 1131 (batch 3 of epoch 282).\n",
            "Loss: 0.110148\n",
            "[+][2020-04-04 18:59:29.064521] Training iteration 1132 (batch 4 of epoch 282).\n",
            "Loss: 0.103219\n",
            "[+][2020-04-04 18:59:29.343031] Training iteration 1133 (batch 1 of epoch 283).\n",
            "Loss: 0.115227\n",
            "[+][2020-04-04 18:59:29.615490] Training iteration 1134 (batch 2 of epoch 283).\n",
            "Loss: 0.122091\n",
            "[+][2020-04-04 18:59:29.897688] Training iteration 1135 (batch 3 of epoch 283).\n",
            "Loss: 0.104224\n",
            "[+][2020-04-04 18:59:30.174412] Training iteration 1136 (batch 4 of epoch 283).\n",
            "Loss: 0.100730\n",
            "[+][2020-04-04 18:59:30.451013] Training iteration 1137 (batch 1 of epoch 284).\n",
            "Loss: 0.106926\n",
            "[+][2020-04-04 18:59:30.726798] Training iteration 1138 (batch 2 of epoch 284).\n",
            "Loss: 0.096590\n",
            "[+][2020-04-04 18:59:31.017290] Training iteration 1139 (batch 3 of epoch 284).\n",
            "Loss: 0.099450\n",
            "[+][2020-04-04 18:59:31.293005] Training iteration 1140 (batch 4 of epoch 284).\n",
            "Loss: 0.107110\n",
            "[+][2020-04-04 18:59:31.575793] Training iteration 1141 (batch 1 of epoch 285).\n",
            "Loss: 0.109945\n",
            "[+][2020-04-04 18:59:31.864970] Training iteration 1142 (batch 2 of epoch 285).\n",
            "Loss: 0.100034\n",
            "[+][2020-04-04 18:59:32.134359] Training iteration 1143 (batch 3 of epoch 285).\n",
            "Loss: 0.094127\n",
            "[+][2020-04-04 18:59:32.410186] Training iteration 1144 (batch 4 of epoch 285).\n",
            "Loss: 0.108838\n",
            "[+][2020-04-04 18:59:32.682423] Training iteration 1145 (batch 1 of epoch 286).\n",
            "Loss: 0.091889\n",
            "[+][2020-04-04 18:59:32.979138] Training iteration 1146 (batch 2 of epoch 286).\n",
            "Loss: 0.098907\n",
            "[+][2020-04-04 18:59:33.281603] Training iteration 1147 (batch 3 of epoch 286).\n",
            "Loss: 0.109311\n",
            "[+][2020-04-04 18:59:33.609837] Training iteration 1148 (batch 4 of epoch 286).\n",
            "Loss: 0.105590\n",
            "[+][2020-04-04 18:59:33.919332] Training iteration 1149 (batch 1 of epoch 287).\n",
            "Loss: 0.137673\n",
            "[+][2020-04-04 18:59:34.214850] Training iteration 1150 (batch 2 of epoch 287).\n",
            "Loss: 0.120708\n",
            "[+][2020-04-04 18:59:34.491395] Training iteration 1151 (batch 3 of epoch 287).\n",
            "Loss: 0.115457\n",
            "[+][2020-04-04 18:59:34.777270] Training iteration 1152 (batch 4 of epoch 287).\n",
            "Loss: 0.104728\n",
            "[+][2020-04-04 18:59:35.070042] Training iteration 1153 (batch 1 of epoch 288).\n",
            "Loss: 0.089888\n",
            "[+][2020-04-04 18:59:35.340608] Training iteration 1154 (batch 2 of epoch 288).\n",
            "Loss: 0.118447\n",
            "[+][2020-04-04 18:59:35.620706] Training iteration 1155 (batch 3 of epoch 288).\n",
            "Loss: 0.122124\n",
            "[+][2020-04-04 18:59:35.898772] Training iteration 1156 (batch 4 of epoch 288).\n",
            "Loss: 0.104125\n",
            "[+][2020-04-04 18:59:36.172671] Training iteration 1157 (batch 1 of epoch 289).\n",
            "Loss: 0.112650\n",
            "[+][2020-04-04 18:59:36.449174] Training iteration 1158 (batch 2 of epoch 289).\n",
            "Loss: 0.118121\n",
            "[+][2020-04-04 18:59:36.724768] Training iteration 1159 (batch 3 of epoch 289).\n",
            "Loss: 0.105764\n",
            "[+][2020-04-04 18:59:37.002421] Training iteration 1160 (batch 4 of epoch 289).\n",
            "Loss: 0.100332\n",
            "[+][2020-04-04 18:59:37.276639] Training iteration 1161 (batch 1 of epoch 290).\n",
            "Loss: 0.096561\n",
            "[+][2020-04-04 18:59:37.550355] Training iteration 1162 (batch 2 of epoch 290).\n",
            "Loss: 0.092246\n",
            "[+][2020-04-04 18:59:37.830459] Training iteration 1163 (batch 3 of epoch 290).\n",
            "Loss: 0.120782\n",
            "[+][2020-04-04 18:59:38.111958] Training iteration 1164 (batch 4 of epoch 290).\n",
            "Loss: 0.112657\n",
            "[+][2020-04-04 18:59:38.386901] Training iteration 1165 (batch 1 of epoch 291).\n",
            "Loss: 0.096308\n",
            "[+][2020-04-04 18:59:38.668606] Training iteration 1166 (batch 2 of epoch 291).\n",
            "Loss: 0.099086\n",
            "[+][2020-04-04 18:59:38.937320] Training iteration 1167 (batch 3 of epoch 291).\n",
            "Loss: 0.097216\n",
            "[+][2020-04-04 18:59:39.228517] Training iteration 1168 (batch 4 of epoch 291).\n",
            "Loss: 0.093806\n",
            "[+][2020-04-04 18:59:39.508389] Training iteration 1169 (batch 1 of epoch 292).\n",
            "Loss: 0.095649\n",
            "[+][2020-04-04 18:59:39.781010] Training iteration 1170 (batch 2 of epoch 292).\n",
            "Loss: 0.110411\n",
            "[+][2020-04-04 18:59:40.063540] Training iteration 1171 (batch 3 of epoch 292).\n",
            "Loss: 0.091616\n",
            "[+][2020-04-04 18:59:40.344208] Training iteration 1172 (batch 4 of epoch 292).\n",
            "Loss: 0.096117\n",
            "[+][2020-04-04 18:59:40.619164] Training iteration 1173 (batch 1 of epoch 293).\n",
            "Loss: 0.092813\n",
            "[+][2020-04-04 18:59:40.895015] Training iteration 1174 (batch 2 of epoch 293).\n",
            "Loss: 0.096614\n",
            "[+][2020-04-04 18:59:41.190617] Training iteration 1175 (batch 3 of epoch 293).\n",
            "Loss: 0.094203\n",
            "[+][2020-04-04 18:59:41.460721] Training iteration 1176 (batch 4 of epoch 293).\n",
            "Loss: 0.099653\n",
            "[+][2020-04-04 18:59:41.743988] Training iteration 1177 (batch 1 of epoch 294).\n",
            "Loss: 0.086150\n",
            "[+][2020-04-04 18:59:42.016434] Training iteration 1178 (batch 2 of epoch 294).\n",
            "Loss: 0.097336\n",
            "[+][2020-04-04 18:59:42.302435] Training iteration 1179 (batch 3 of epoch 294).\n",
            "Loss: 0.100383\n",
            "[+][2020-04-04 18:59:42.578420] Training iteration 1180 (batch 4 of epoch 294).\n",
            "Loss: 0.110731\n",
            "[+][2020-04-04 18:59:42.863059] Training iteration 1181 (batch 1 of epoch 295).\n",
            "Loss: 0.100409\n",
            "[+][2020-04-04 18:59:43.149632] Training iteration 1182 (batch 2 of epoch 295).\n",
            "Loss: 0.103156\n",
            "[+][2020-04-04 18:59:43.422283] Training iteration 1183 (batch 3 of epoch 295).\n",
            "Loss: 0.098536\n",
            "[+][2020-04-04 18:59:43.706747] Training iteration 1184 (batch 4 of epoch 295).\n",
            "Loss: 0.095158\n",
            "[+][2020-04-04 18:59:43.981497] Training iteration 1185 (batch 1 of epoch 296).\n",
            "Loss: 0.081033\n",
            "[+][2020-04-04 18:59:44.260815] Training iteration 1186 (batch 2 of epoch 296).\n",
            "Loss: 0.085222\n",
            "[+][2020-04-04 18:59:44.538267] Training iteration 1187 (batch 3 of epoch 296).\n",
            "Loss: 0.100530\n",
            "[+][2020-04-04 18:59:44.819431] Training iteration 1188 (batch 4 of epoch 296).\n",
            "Loss: 0.076979\n",
            "[+][2020-04-04 18:59:45.105543] Training iteration 1189 (batch 1 of epoch 297).\n",
            "Loss: 0.098581\n",
            "[+][2020-04-04 18:59:45.387698] Training iteration 1190 (batch 2 of epoch 297).\n",
            "Loss: 0.128032\n",
            "[+][2020-04-04 18:59:45.666582] Training iteration 1191 (batch 3 of epoch 297).\n",
            "Loss: 0.114548\n",
            "[+][2020-04-04 18:59:45.939872] Training iteration 1192 (batch 4 of epoch 297).\n",
            "Loss: 0.109692\n",
            "[+][2020-04-04 18:59:46.222661] Training iteration 1193 (batch 1 of epoch 298).\n",
            "Loss: 0.110664\n",
            "[+][2020-04-04 18:59:46.495464] Training iteration 1194 (batch 2 of epoch 298).\n",
            "Loss: 0.106595\n",
            "[+][2020-04-04 18:59:46.769713] Training iteration 1195 (batch 3 of epoch 298).\n",
            "Loss: 0.099734\n",
            "[+][2020-04-04 18:59:47.057784] Training iteration 1196 (batch 4 of epoch 298).\n",
            "Loss: 0.089910\n",
            "[+][2020-04-04 18:59:47.354691] Training iteration 1197 (batch 1 of epoch 299).\n",
            "Loss: 0.108643\n",
            "[+][2020-04-04 18:59:47.627703] Training iteration 1198 (batch 2 of epoch 299).\n",
            "Loss: 0.101414\n",
            "[+][2020-04-04 18:59:47.902951] Training iteration 1199 (batch 3 of epoch 299).\n",
            "Loss: 0.103185\n",
            "[+][2020-04-04 18:59:48.192021] Training iteration 1200 (batch 4 of epoch 299).\n",
            "Loss: 0.090897\n",
            "[+][2020-04-04 18:59:48.463124] Breaking on request from callback.\n",
            "[+][2020-04-04 18:59:48.463217] Exceeded max number of iterations / epochs, breaking.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BIehkI8dB7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = torch.autograd.Variable(mnist[np.random.randint(2000, 60000)][0].unsqueeze(dim=0))\n",
        "example[0, -10:] = -1\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "if net.weights.data.is_cuda:\n",
        "    example = example.cuda()\n",
        "example_ev = net(example, num_iters=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwCnNw4SdfYS",
        "colab_type": "code",
        "outputId": "a5a67d21-c892-4f92-c63f-7c9dc0fd242c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlJ4qNE2fDIG",
        "colab_type": "code",
        "outputId": "a15b12ed-2ffb-4de4-a967-830654b6f612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example_ev.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HCUoZTmfHPM",
        "colab_type": "text"
      },
      "source": [
        "# Restricted Boltzmann Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQGS1UEgfLho",
        "colab_type": "code",
        "outputId": "2ef42480-6463-455b-f340-5ef6e65a5901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "net = rbm_models.RestrictedBoltzmannMachine(mnist[0][0].size()[0], num_hidden=500, temperature=0.2) # initalized a stochastic hopfield net with 784 units (Mnist is 28 x 28)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/recitation-11/rbm_models.py:40: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
            "  torch.nn.init.xavier_normal(self.weights)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPql0sQyizKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = rbm_models.RestrictedBoltzmannTrainWrapper(net, 2) # This is a trainer. the \"2\" input means to evolve twice every iter."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbRlAuWrkjaI",
        "colab_type": "code",
        "outputId": "ed953f7d-ea05-43e8-9c56-3e4c44cfb98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rbm_demo_utils.train([trainer], mnist, rbm_demo_utils.IdentityLoss(), 200, 500, 0.001, 'RBM')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[+][2020-04-04 19:23:03.211588] Training iteration 0 (batch 0 of epoch 0).\n",
            "Loss: 0.317889\n",
            "[+][2020-04-04 19:23:03.363791] Training iteration 1 (batch 1 of epoch 0).\n",
            "Loss: 0.214412\n",
            "[+][2020-04-04 19:23:03.496869] Training iteration 2 (batch 2 of epoch 0).\n",
            "Loss: 0.185198\n",
            "[+][2020-04-04 19:23:03.622021] Training iteration 3 (batch 3 of epoch 0).\n",
            "Loss: 0.160867\n",
            "[+][2020-04-04 19:23:03.750951] Training iteration 4 (batch 4 of epoch 0).\n",
            "Loss: 0.131180\n",
            "[+][2020-04-04 19:23:03.876669] Training iteration 5 (batch 1 of epoch 1).\n",
            "Loss: 0.102567\n",
            "[+][2020-04-04 19:23:04.003116] Training iteration 6 (batch 2 of epoch 1).\n",
            "Loss: 0.075920\n",
            "[+][2020-04-04 19:23:04.129338] Training iteration 7 (batch 3 of epoch 1).\n",
            "Loss: 0.074035\n",
            "[+][2020-04-04 19:23:04.256727] Training iteration 8 (batch 4 of epoch 1).\n",
            "Loss: 0.084652\n",
            "[+][2020-04-04 19:23:04.379748] Training iteration 9 (batch 1 of epoch 2).\n",
            "Loss: 0.085117\n",
            "[+][2020-04-04 19:23:04.507791] Training iteration 10 (batch 2 of epoch 2).\n",
            "Loss: 0.074456\n",
            "[+][2020-04-04 19:23:04.630731] Training iteration 11 (batch 3 of epoch 2).\n",
            "Loss: 0.059448\n",
            "[+][2020-04-04 19:23:04.767656] Training iteration 12 (batch 4 of epoch 2).\n",
            "Loss: 0.050139\n",
            "[+][2020-04-04 19:23:04.889769] Training iteration 13 (batch 1 of epoch 3).\n",
            "Loss: 0.045802\n",
            "[+][2020-04-04 19:23:05.018204] Training iteration 14 (batch 2 of epoch 3).\n",
            "Loss: 0.044596\n",
            "[+][2020-04-04 19:23:05.152308] Training iteration 15 (batch 3 of epoch 3).\n",
            "Loss: 0.035549\n",
            "[+][2020-04-04 19:23:05.297010] Training iteration 16 (batch 4 of epoch 3).\n",
            "Loss: 0.035916\n",
            "[+][2020-04-04 19:23:05.427166] Training iteration 17 (batch 1 of epoch 4).\n",
            "Loss: 0.032959\n",
            "[+][2020-04-04 19:23:05.554615] Training iteration 18 (batch 2 of epoch 4).\n",
            "Loss: 0.031058\n",
            "[+][2020-04-04 19:23:05.680220] Training iteration 19 (batch 3 of epoch 4).\n",
            "Loss: 0.029681\n",
            "[+][2020-04-04 19:23:05.809097] Training iteration 20 (batch 4 of epoch 4).\n",
            "Loss: 0.027755\n",
            "[+][2020-04-04 19:23:05.945659] Training iteration 21 (batch 1 of epoch 5).\n",
            "Loss: 0.027759\n",
            "[+][2020-04-04 19:23:06.073385] Training iteration 22 (batch 2 of epoch 5).\n",
            "Loss: 0.025250\n",
            "[+][2020-04-04 19:23:06.199142] Training iteration 23 (batch 3 of epoch 5).\n",
            "Loss: 0.020424\n",
            "[+][2020-04-04 19:23:06.322122] Training iteration 24 (batch 4 of epoch 5).\n",
            "Loss: 0.018939\n",
            "[+][2020-04-04 19:23:06.452890] Training iteration 25 (batch 1 of epoch 6).\n",
            "Loss: 0.017904\n",
            "[+][2020-04-04 19:23:06.578834] Training iteration 26 (batch 2 of epoch 6).\n",
            "Loss: 0.019496\n",
            "[+][2020-04-04 19:23:06.699597] Training iteration 27 (batch 3 of epoch 6).\n",
            "Loss: 0.016908\n",
            "[+][2020-04-04 19:23:06.825770] Training iteration 28 (batch 4 of epoch 6).\n",
            "Loss: 0.015598\n",
            "[+][2020-04-04 19:23:06.953003] Training iteration 29 (batch 1 of epoch 7).\n",
            "Loss: 0.013792\n",
            "[+][2020-04-04 19:23:07.088612] Training iteration 30 (batch 2 of epoch 7).\n",
            "Loss: 0.014964\n",
            "[+][2020-04-04 19:23:07.221500] Training iteration 31 (batch 3 of epoch 7).\n",
            "Loss: 0.014334\n",
            "[+][2020-04-04 19:23:07.345431] Training iteration 32 (batch 4 of epoch 7).\n",
            "Loss: 0.014776\n",
            "[+][2020-04-04 19:23:07.468584] Training iteration 33 (batch 1 of epoch 8).\n",
            "Loss: 0.012000\n",
            "[+][2020-04-04 19:23:07.594256] Training iteration 34 (batch 2 of epoch 8).\n",
            "Loss: 0.009317\n",
            "[+][2020-04-04 19:23:07.716108] Training iteration 35 (batch 3 of epoch 8).\n",
            "Loss: 0.008615\n",
            "[+][2020-04-04 19:23:07.846100] Training iteration 36 (batch 4 of epoch 8).\n",
            "Loss: 0.009019\n",
            "[+][2020-04-04 19:23:07.978627] Training iteration 37 (batch 1 of epoch 9).\n",
            "Loss: 0.008599\n",
            "[+][2020-04-04 19:23:08.111175] Training iteration 38 (batch 2 of epoch 9).\n",
            "Loss: 0.007330\n",
            "[+][2020-04-04 19:23:08.244938] Training iteration 39 (batch 3 of epoch 9).\n",
            "Loss: 0.007345\n",
            "[+][2020-04-04 19:23:08.370632] Training iteration 40 (batch 4 of epoch 9).\n",
            "Loss: 0.007434\n",
            "[+][2020-04-04 19:23:08.494967] Training iteration 41 (batch 1 of epoch 10).\n",
            "Loss: 0.007183\n",
            "[+][2020-04-04 19:23:08.617481] Training iteration 42 (batch 2 of epoch 10).\n",
            "Loss: 0.008543\n",
            "[+][2020-04-04 19:23:08.740945] Training iteration 43 (batch 3 of epoch 10).\n",
            "Loss: 0.006761\n",
            "[+][2020-04-04 19:23:08.869715] Training iteration 44 (batch 4 of epoch 10).\n",
            "Loss: 0.005841\n",
            "[+][2020-04-04 19:23:09.014962] Training iteration 45 (batch 1 of epoch 11).\n",
            "Loss: 0.005933\n",
            "[+][2020-04-04 19:23:09.141754] Training iteration 46 (batch 2 of epoch 11).\n",
            "Loss: 0.007384\n",
            "[+][2020-04-04 19:23:09.273704] Training iteration 47 (batch 3 of epoch 11).\n",
            "Loss: 0.006017\n",
            "[+][2020-04-04 19:23:09.400290] Training iteration 48 (batch 4 of epoch 11).\n",
            "Loss: 0.005407\n",
            "[+][2020-04-04 19:23:09.525438] Training iteration 49 (batch 1 of epoch 12).\n",
            "Loss: 0.004253\n",
            "[+][2020-04-04 19:23:09.646792] Training iteration 50 (batch 2 of epoch 12).\n",
            "Loss: 0.004991\n",
            "[+][2020-04-04 19:23:09.774362] Training iteration 51 (batch 3 of epoch 12).\n",
            "Loss: 0.005371\n",
            "[+][2020-04-04 19:23:09.897829] Training iteration 52 (batch 4 of epoch 12).\n",
            "Loss: 0.006639\n",
            "[+][2020-04-04 19:23:10.029172] Training iteration 53 (batch 1 of epoch 13).\n",
            "Loss: 0.002017\n",
            "[+][2020-04-04 19:23:10.159914] Training iteration 54 (batch 2 of epoch 13).\n",
            "Loss: 0.005354\n",
            "[+][2020-04-04 19:23:10.290733] Training iteration 55 (batch 3 of epoch 13).\n",
            "Loss: 0.004605\n",
            "[+][2020-04-04 19:23:10.411373] Training iteration 56 (batch 4 of epoch 13).\n",
            "Loss: 0.003642\n",
            "[+][2020-04-04 19:23:10.533481] Training iteration 57 (batch 1 of epoch 14).\n",
            "Loss: 0.004229\n",
            "[+][2020-04-04 19:23:10.660180] Training iteration 58 (batch 2 of epoch 14).\n",
            "Loss: 0.003435\n",
            "[+][2020-04-04 19:23:10.789792] Training iteration 59 (batch 3 of epoch 14).\n",
            "Loss: 0.002924\n",
            "[+][2020-04-04 19:23:10.912683] Training iteration 60 (batch 4 of epoch 14).\n",
            "Loss: 0.004320\n",
            "[+][2020-04-04 19:23:11.032758] Training iteration 61 (batch 1 of epoch 15).\n",
            "Loss: 0.003070\n",
            "[+][2020-04-04 19:23:11.157599] Training iteration 62 (batch 2 of epoch 15).\n",
            "Loss: 0.003032\n",
            "[+][2020-04-04 19:23:11.282989] Training iteration 63 (batch 3 of epoch 15).\n",
            "Loss: 0.002060\n",
            "[+][2020-04-04 19:23:11.409087] Training iteration 64 (batch 4 of epoch 15).\n",
            "Loss: 0.004173\n",
            "[+][2020-04-04 19:23:11.529137] Training iteration 65 (batch 1 of epoch 16).\n",
            "Loss: 0.002950\n",
            "[+][2020-04-04 19:23:11.652915] Training iteration 66 (batch 2 of epoch 16).\n",
            "Loss: 0.003023\n",
            "[+][2020-04-04 19:23:11.771666] Training iteration 67 (batch 3 of epoch 16).\n",
            "Loss: 0.004281\n",
            "[+][2020-04-04 19:23:11.890053] Training iteration 68 (batch 4 of epoch 16).\n",
            "Loss: 0.003272\n",
            "[+][2020-04-04 19:23:12.008650] Training iteration 69 (batch 1 of epoch 17).\n",
            "Loss: 0.003845\n",
            "[+][2020-04-04 19:23:12.140059] Training iteration 70 (batch 2 of epoch 17).\n",
            "Loss: 0.004055\n",
            "[+][2020-04-04 19:23:12.271331] Training iteration 71 (batch 3 of epoch 17).\n",
            "Loss: 0.003515\n",
            "[+][2020-04-04 19:23:12.393787] Training iteration 72 (batch 4 of epoch 17).\n",
            "Loss: 0.001720\n",
            "[+][2020-04-04 19:23:12.513731] Training iteration 73 (batch 1 of epoch 18).\n",
            "Loss: 0.001451\n",
            "[+][2020-04-04 19:23:12.630433] Training iteration 74 (batch 2 of epoch 18).\n",
            "Loss: 0.003833\n",
            "[+][2020-04-04 19:23:12.749017] Training iteration 75 (batch 3 of epoch 18).\n",
            "Loss: 0.001674\n",
            "[+][2020-04-04 19:23:12.874885] Training iteration 76 (batch 4 of epoch 18).\n",
            "Loss: 0.002120\n",
            "[+][2020-04-04 19:23:12.995300] Training iteration 77 (batch 1 of epoch 19).\n",
            "Loss: 0.001992\n",
            "[+][2020-04-04 19:23:13.128217] Training iteration 78 (batch 2 of epoch 19).\n",
            "Loss: 0.004287\n",
            "[+][2020-04-04 19:23:13.255784] Training iteration 79 (batch 3 of epoch 19).\n",
            "Loss: 0.001516\n",
            "[+][2020-04-04 19:23:13.391621] Training iteration 80 (batch 4 of epoch 19).\n",
            "Loss: 0.001171\n",
            "[+][2020-04-04 19:23:13.510806] Training iteration 81 (batch 1 of epoch 20).\n",
            "Loss: 0.002027\n",
            "[+][2020-04-04 19:23:13.627715] Training iteration 82 (batch 2 of epoch 20).\n",
            "Loss: 0.001041\n",
            "[+][2020-04-04 19:23:13.745657] Training iteration 83 (batch 3 of epoch 20).\n",
            "Loss: 0.003595\n",
            "[+][2020-04-04 19:23:13.868071] Training iteration 84 (batch 4 of epoch 20).\n",
            "Loss: 0.002249\n",
            "[+][2020-04-04 19:23:13.987392] Training iteration 85 (batch 1 of epoch 21).\n",
            "Loss: 0.002236\n",
            "[+][2020-04-04 19:23:14.118144] Training iteration 86 (batch 2 of epoch 21).\n",
            "Loss: 0.000322\n",
            "[+][2020-04-04 19:23:14.237034] Training iteration 87 (batch 3 of epoch 21).\n",
            "Loss: 0.000991\n",
            "[+][2020-04-04 19:23:14.356344] Training iteration 88 (batch 4 of epoch 21).\n",
            "Loss: 0.002162\n",
            "[+][2020-04-04 19:23:14.473651] Training iteration 89 (batch 1 of epoch 22).\n",
            "Loss: 0.001748\n",
            "[+][2020-04-04 19:23:14.589858] Training iteration 90 (batch 2 of epoch 22).\n",
            "Loss: 0.000801\n",
            "[+][2020-04-04 19:23:14.708616] Training iteration 91 (batch 3 of epoch 22).\n",
            "Loss: 0.002849\n",
            "[+][2020-04-04 19:23:14.832124] Training iteration 92 (batch 4 of epoch 22).\n",
            "Loss: 0.002040\n",
            "[+][2020-04-04 19:23:14.956608] Training iteration 93 (batch 1 of epoch 23).\n",
            "Loss: 0.003056\n",
            "[+][2020-04-04 19:23:15.104125] Training iteration 94 (batch 2 of epoch 23).\n",
            "Loss: 0.000781\n",
            "[+][2020-04-04 19:23:15.237809] Training iteration 95 (batch 3 of epoch 23).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:15.373428] Training iteration 96 (batch 4 of epoch 23).\n",
            "Loss: 0.002005\n",
            "[+][2020-04-04 19:23:15.497289] Training iteration 97 (batch 1 of epoch 24).\n",
            "Loss: 0.002296\n",
            "[+][2020-04-04 19:23:15.624809] Training iteration 98 (batch 2 of epoch 24).\n",
            "Loss: 0.000402\n",
            "[+][2020-04-04 19:23:15.742119] Training iteration 99 (batch 3 of epoch 24).\n",
            "Loss: 0.001854\n",
            "[+][2020-04-04 19:23:15.873682] Training iteration 100 (batch 4 of epoch 24).\n",
            "Loss: 0.001565\n",
            "[+][2020-04-04 19:23:15.998842] Training iteration 101 (batch 1 of epoch 25).\n",
            "Loss: 0.002057\n",
            "[+][2020-04-04 19:23:16.126147] Training iteration 102 (batch 2 of epoch 25).\n",
            "Loss: 0.001936\n",
            "[+][2020-04-04 19:23:16.254642] Training iteration 103 (batch 3 of epoch 25).\n",
            "Loss: 0.001706\n",
            "[+][2020-04-04 19:23:16.376602] Training iteration 104 (batch 4 of epoch 25).\n",
            "Loss: 0.002091\n",
            "[+][2020-04-04 19:23:16.499806] Training iteration 105 (batch 1 of epoch 26).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:16.620856] Training iteration 106 (batch 2 of epoch 26).\n",
            "Loss: 0.000999\n",
            "[+][2020-04-04 19:23:16.743292] Training iteration 107 (batch 3 of epoch 26).\n",
            "Loss: 0.001214\n",
            "[+][2020-04-04 19:23:16.863022] Training iteration 108 (batch 4 of epoch 26).\n",
            "Loss: 0.001095\n",
            "[+][2020-04-04 19:23:16.983762] Training iteration 109 (batch 1 of epoch 27).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:17.102836] Training iteration 110 (batch 2 of epoch 27).\n",
            "Loss: 0.005523\n",
            "[+][2020-04-04 19:23:17.229529] Training iteration 111 (batch 3 of epoch 27).\n",
            "Loss: 0.003860\n",
            "[+][2020-04-04 19:23:17.355996] Training iteration 112 (batch 4 of epoch 27).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:17.473775] Training iteration 113 (batch 1 of epoch 28).\n",
            "Loss: 0.002935\n",
            "[+][2020-04-04 19:23:17.591329] Training iteration 114 (batch 2 of epoch 28).\n",
            "Loss: 0.004617\n",
            "[+][2020-04-04 19:23:17.711143] Training iteration 115 (batch 3 of epoch 28).\n",
            "Loss: 0.003094\n",
            "[+][2020-04-04 19:23:17.832136] Training iteration 116 (batch 4 of epoch 28).\n",
            "Loss: 0.002733\n",
            "[+][2020-04-04 19:23:17.957700] Training iteration 117 (batch 1 of epoch 29).\n",
            "Loss: 0.002372\n",
            "[+][2020-04-04 19:23:18.082222] Training iteration 118 (batch 2 of epoch 29).\n",
            "Loss: 0.000293\n",
            "[+][2020-04-04 19:23:18.212285] Training iteration 119 (batch 3 of epoch 29).\n",
            "Loss: 0.001056\n",
            "[+][2020-04-04 19:23:18.341340] Training iteration 120 (batch 4 of epoch 29).\n",
            "Loss: 0.000895\n",
            "[+][2020-04-04 19:23:18.464419] Training iteration 121 (batch 1 of epoch 30).\n",
            "Loss: 0.002851\n",
            "[+][2020-04-04 19:23:18.585180] Training iteration 122 (batch 2 of epoch 30).\n",
            "Loss: 0.000941\n",
            "[+][2020-04-04 19:23:18.703953] Training iteration 123 (batch 3 of epoch 30).\n",
            "Loss: 0.000464\n",
            "[+][2020-04-04 19:23:18.828769] Training iteration 124 (batch 4 of epoch 30).\n",
            "Loss: 0.001238\n",
            "[+][2020-04-04 19:23:18.945964] Training iteration 125 (batch 1 of epoch 31).\n",
            "Loss: 0.000663\n",
            "[+][2020-04-04 19:23:19.064677] Training iteration 126 (batch 2 of epoch 31).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:19.206992] Training iteration 127 (batch 3 of epoch 31).\n",
            "Loss: 0.003296\n",
            "[+][2020-04-04 19:23:19.335436] Training iteration 128 (batch 4 of epoch 31).\n",
            "Loss: 0.001625\n",
            "[+][2020-04-04 19:23:19.454855] Training iteration 129 (batch 1 of epoch 32).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:19.575769] Training iteration 130 (batch 2 of epoch 32).\n",
            "Loss: 0.002217\n",
            "[+][2020-04-04 19:23:19.695219] Training iteration 131 (batch 3 of epoch 32).\n",
            "Loss: 0.001205\n",
            "[+][2020-04-04 19:23:19.816562] Training iteration 132 (batch 4 of epoch 32).\n",
            "Loss: 0.000550\n",
            "[+][2020-04-04 19:23:19.933926] Training iteration 133 (batch 1 of epoch 33).\n",
            "Loss: 0.000562\n",
            "[+][2020-04-04 19:23:20.057410] Training iteration 134 (batch 2 of epoch 33).\n",
            "Loss: 0.002597\n",
            "[+][2020-04-04 19:23:20.178625] Training iteration 135 (batch 3 of epoch 33).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:20.314004] Training iteration 136 (batch 4 of epoch 33).\n",
            "Loss: 0.001188\n",
            "[+][2020-04-04 19:23:20.434866] Training iteration 137 (batch 1 of epoch 34).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:20.559424] Training iteration 138 (batch 2 of epoch 34).\n",
            "Loss: 0.000986\n",
            "[+][2020-04-04 19:23:20.679211] Training iteration 139 (batch 3 of epoch 34).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:20.799441] Training iteration 140 (batch 4 of epoch 34).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:20.923439] Training iteration 141 (batch 1 of epoch 35).\n",
            "Loss: 0.002794\n",
            "[+][2020-04-04 19:23:21.063472] Training iteration 142 (batch 2 of epoch 35).\n",
            "Loss: 0.000839\n",
            "[+][2020-04-04 19:23:21.181612] Training iteration 143 (batch 3 of epoch 35).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:21.313448] Training iteration 144 (batch 4 of epoch 35).\n",
            "Loss: 0.001236\n",
            "[+][2020-04-04 19:23:21.434702] Training iteration 145 (batch 1 of epoch 36).\n",
            "Loss: 0.002955\n",
            "[+][2020-04-04 19:23:21.558669] Training iteration 146 (batch 2 of epoch 36).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:21.679001] Training iteration 147 (batch 3 of epoch 36).\n",
            "Loss: 0.002520\n",
            "[+][2020-04-04 19:23:21.804044] Training iteration 148 (batch 4 of epoch 36).\n",
            "Loss: 0.001135\n",
            "[+][2020-04-04 19:23:21.921794] Training iteration 149 (batch 1 of epoch 37).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:22.056012] Training iteration 150 (batch 2 of epoch 37).\n",
            "Loss: 0.000251\n",
            "[+][2020-04-04 19:23:22.174160] Training iteration 151 (batch 3 of epoch 37).\n",
            "Loss: 0.000261\n",
            "[+][2020-04-04 19:23:22.299036] Training iteration 152 (batch 4 of epoch 37).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:22.424129] Training iteration 153 (batch 1 of epoch 38).\n",
            "Loss: 0.003288\n",
            "[+][2020-04-04 19:23:22.549514] Training iteration 154 (batch 2 of epoch 38).\n",
            "Loss: 0.001482\n",
            "[+][2020-04-04 19:23:22.666574] Training iteration 155 (batch 3 of epoch 38).\n",
            "Loss: 0.000275\n",
            "[+][2020-04-04 19:23:22.783868] Training iteration 156 (batch 4 of epoch 38).\n",
            "Loss: 0.000087\n",
            "[+][2020-04-04 19:23:22.901594] Training iteration 157 (batch 1 of epoch 39).\n",
            "Loss: 0.001137\n",
            "[+][2020-04-04 19:23:23.028549] Training iteration 158 (batch 2 of epoch 39).\n",
            "Loss: 0.000677\n",
            "[+][2020-04-04 19:23:23.149687] Training iteration 159 (batch 3 of epoch 39).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:23.274789] Training iteration 160 (batch 4 of epoch 39).\n",
            "Loss: 0.000950\n",
            "[+][2020-04-04 19:23:23.406877] Training iteration 161 (batch 1 of epoch 40).\n",
            "Loss: 0.001444\n",
            "[+][2020-04-04 19:23:23.528852] Training iteration 162 (batch 2 of epoch 40).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:23.648629] Training iteration 163 (batch 3 of epoch 40).\n",
            "Loss: 0.002629\n",
            "[+][2020-04-04 19:23:23.767078] Training iteration 164 (batch 4 of epoch 40).\n",
            "Loss: 0.001235\n",
            "[+][2020-04-04 19:23:23.885102] Training iteration 165 (batch 1 of epoch 41).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:24.014533] Training iteration 166 (batch 2 of epoch 41).\n",
            "Loss: 0.000671\n",
            "[+][2020-04-04 19:23:24.133372] Training iteration 167 (batch 3 of epoch 41).\n",
            "Loss: 0.002165\n",
            "[+][2020-04-04 19:23:24.256305] Training iteration 168 (batch 4 of epoch 41).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:24.392316] Training iteration 169 (batch 1 of epoch 42).\n",
            "Loss: 0.001610\n",
            "[+][2020-04-04 19:23:24.512591] Training iteration 170 (batch 2 of epoch 42).\n",
            "Loss: 0.000079\n",
            "[+][2020-04-04 19:23:24.628469] Training iteration 171 (batch 3 of epoch 42).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:24.746809] Training iteration 172 (batch 4 of epoch 42).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:24.864562] Training iteration 173 (batch 1 of epoch 43).\n",
            "Loss: 0.002136\n",
            "[+][2020-04-04 19:23:24.999829] Training iteration 174 (batch 2 of epoch 43).\n",
            "Loss: 0.000561\n",
            "[+][2020-04-04 19:23:25.120558] Training iteration 175 (batch 3 of epoch 43).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:25.245778] Training iteration 176 (batch 4 of epoch 43).\n",
            "Loss: 0.004424\n",
            "[+][2020-04-04 19:23:25.373641] Training iteration 177 (batch 1 of epoch 44).\n",
            "Loss: 0.004923\n",
            "[+][2020-04-04 19:23:25.495937] Training iteration 178 (batch 2 of epoch 44).\n",
            "Loss: 0.001024\n",
            "[+][2020-04-04 19:23:25.611761] Training iteration 179 (batch 3 of epoch 44).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:25.730708] Training iteration 180 (batch 4 of epoch 44).\n",
            "Loss: 0.005502\n",
            "[+][2020-04-04 19:23:25.849356] Training iteration 181 (batch 1 of epoch 45).\n",
            "Loss: 0.002585\n",
            "[+][2020-04-04 19:23:25.974805] Training iteration 182 (batch 2 of epoch 45).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:26.096640] Training iteration 183 (batch 3 of epoch 45).\n",
            "Loss: 0.001937\n",
            "[+][2020-04-04 19:23:26.218524] Training iteration 184 (batch 4 of epoch 45).\n",
            "Loss: 0.004113\n",
            "[+][2020-04-04 19:23:26.338222] Training iteration 185 (batch 1 of epoch 46).\n",
            "Loss: 0.001145\n",
            "[+][2020-04-04 19:23:26.469711] Training iteration 186 (batch 2 of epoch 46).\n",
            "Loss: 0.001395\n",
            "[+][2020-04-04 19:23:26.585115] Training iteration 187 (batch 3 of epoch 46).\n",
            "Loss: 0.000169\n",
            "[+][2020-04-04 19:23:26.707283] Training iteration 188 (batch 4 of epoch 46).\n",
            "Loss: 0.000373\n",
            "[+][2020-04-04 19:23:26.834685] Training iteration 189 (batch 1 of epoch 47).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:26.958927] Training iteration 190 (batch 2 of epoch 47).\n",
            "Loss: 0.000897\n",
            "[+][2020-04-04 19:23:27.080315] Training iteration 191 (batch 3 of epoch 47).\n",
            "Loss: 0.001107\n",
            "[+][2020-04-04 19:23:27.203365] Training iteration 192 (batch 4 of epoch 47).\n",
            "Loss: 0.000633\n",
            "[+][2020-04-04 19:23:27.326876] Training iteration 193 (batch 1 of epoch 48).\n",
            "Loss: 0.000841\n",
            "[+][2020-04-04 19:23:27.465565] Training iteration 194 (batch 2 of epoch 48).\n",
            "Loss: 0.000234\n",
            "[+][2020-04-04 19:23:27.583352] Training iteration 195 (batch 3 of epoch 48).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:27.713282] Training iteration 196 (batch 4 of epoch 48).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:27.837781] Training iteration 197 (batch 1 of epoch 49).\n",
            "Loss: 0.001810\n",
            "[+][2020-04-04 19:23:27.965701] Training iteration 198 (batch 2 of epoch 49).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:28.106370] Training iteration 199 (batch 3 of epoch 49).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:28.231106] Training iteration 200 (batch 4 of epoch 49).\n",
            "Loss: 0.001205\n",
            "[+][2020-04-04 19:23:28.354121] Training iteration 201 (batch 1 of epoch 50).\n",
            "Loss: 0.000405\n",
            "[+][2020-04-04 19:23:28.490648] Training iteration 202 (batch 2 of epoch 50).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:28.608461] Training iteration 203 (batch 3 of epoch 50).\n",
            "Loss: 0.000826\n",
            "[+][2020-04-04 19:23:28.730911] Training iteration 204 (batch 4 of epoch 50).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:28.849311] Training iteration 205 (batch 1 of epoch 51).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:28.972068] Training iteration 206 (batch 2 of epoch 51).\n",
            "Loss: 0.002334\n",
            "[+][2020-04-04 19:23:29.091463] Training iteration 207 (batch 3 of epoch 51).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:29.214182] Training iteration 208 (batch 4 of epoch 51).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:29.334528] Training iteration 209 (batch 1 of epoch 52).\n",
            "Loss: 0.002838\n",
            "[+][2020-04-04 19:23:29.467046] Training iteration 210 (batch 2 of epoch 52).\n",
            "Loss: 0.000362\n",
            "[+][2020-04-04 19:23:29.584316] Training iteration 211 (batch 3 of epoch 52).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:29.704429] Training iteration 212 (batch 4 of epoch 52).\n",
            "Loss: 0.000786\n",
            "[+][2020-04-04 19:23:29.824824] Training iteration 213 (batch 1 of epoch 53).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:29.949504] Training iteration 214 (batch 2 of epoch 53).\n",
            "Loss: 0.001201\n",
            "[+][2020-04-04 19:23:30.074987] Training iteration 215 (batch 3 of epoch 53).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:30.198140] Training iteration 216 (batch 4 of epoch 53).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:30.318225] Training iteration 217 (batch 1 of epoch 54).\n",
            "Loss: 0.001464\n",
            "[+][2020-04-04 19:23:30.443949] Training iteration 218 (batch 2 of epoch 54).\n",
            "Loss: 0.000089\n",
            "[+][2020-04-04 19:23:30.574619] Training iteration 219 (batch 3 of epoch 54).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:30.698537] Training iteration 220 (batch 4 of epoch 54).\n",
            "Loss: 0.003269\n",
            "[+][2020-04-04 19:23:30.817538] Training iteration 221 (batch 1 of epoch 55).\n",
            "Loss: 0.002093\n",
            "[+][2020-04-04 19:23:30.943517] Training iteration 222 (batch 2 of epoch 55).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:31.067907] Training iteration 223 (batch 3 of epoch 55).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:31.189501] Training iteration 224 (batch 4 of epoch 55).\n",
            "Loss: 0.003103\n",
            "[+][2020-04-04 19:23:31.325415] Training iteration 225 (batch 1 of epoch 56).\n",
            "Loss: 0.000223\n",
            "[+][2020-04-04 19:23:31.462091] Training iteration 226 (batch 2 of epoch 56).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:31.600442] Training iteration 227 (batch 3 of epoch 56).\n",
            "Loss: 0.002714\n",
            "[+][2020-04-04 19:23:31.733345] Training iteration 228 (batch 4 of epoch 56).\n",
            "Loss: 0.003276\n",
            "[+][2020-04-04 19:23:31.870552] Training iteration 229 (batch 1 of epoch 57).\n",
            "Loss: 0.001160\n",
            "[+][2020-04-04 19:23:32.004350] Training iteration 230 (batch 2 of epoch 57).\n",
            "Loss: 0.000038\n",
            "[+][2020-04-04 19:23:32.149987] Training iteration 231 (batch 3 of epoch 57).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:32.308357] Training iteration 232 (batch 4 of epoch 57).\n",
            "Loss: 0.001810\n",
            "[+][2020-04-04 19:23:32.428863] Training iteration 233 (batch 1 of epoch 58).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:32.565919] Training iteration 234 (batch 2 of epoch 58).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:32.689258] Training iteration 235 (batch 3 of epoch 58).\n",
            "Loss: 0.000876\n",
            "[+][2020-04-04 19:23:32.826214] Training iteration 236 (batch 4 of epoch 58).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:32.947438] Training iteration 237 (batch 1 of epoch 59).\n",
            "Loss: 0.001973\n",
            "[+][2020-04-04 19:23:33.079150] Training iteration 238 (batch 2 of epoch 59).\n",
            "Loss: 0.000554\n",
            "[+][2020-04-04 19:23:33.199384] Training iteration 239 (batch 3 of epoch 59).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:33.336417] Training iteration 240 (batch 4 of epoch 59).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:33.455527] Training iteration 241 (batch 1 of epoch 60).\n",
            "Loss: 0.005156\n",
            "[+][2020-04-04 19:23:33.589008] Training iteration 242 (batch 2 of epoch 60).\n",
            "Loss: 0.000890\n",
            "[+][2020-04-04 19:23:33.707457] Training iteration 243 (batch 3 of epoch 60).\n",
            "Loss: 0.000058\n",
            "[+][2020-04-04 19:23:33.841546] Training iteration 244 (batch 4 of epoch 60).\n",
            "Loss: 0.003612\n",
            "[+][2020-04-04 19:23:33.965909] Training iteration 245 (batch 1 of epoch 61).\n",
            "Loss: 0.004362\n",
            "[+][2020-04-04 19:23:34.103681] Training iteration 246 (batch 2 of epoch 61).\n",
            "Loss: 0.001378\n",
            "[+][2020-04-04 19:23:34.237430] Training iteration 247 (batch 3 of epoch 61).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:34.360331] Training iteration 248 (batch 4 of epoch 61).\n",
            "Loss: 0.003836\n",
            "[+][2020-04-04 19:23:34.483002] Training iteration 249 (batch 1 of epoch 62).\n",
            "Loss: 0.001484\n",
            "[+][2020-04-04 19:23:34.612428] Training iteration 250 (batch 2 of epoch 62).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:34.732433] Training iteration 251 (batch 3 of epoch 62).\n",
            "Loss: 0.000302\n",
            "[+][2020-04-04 19:23:34.869828] Training iteration 252 (batch 4 of epoch 62).\n",
            "Loss: 0.002976\n",
            "[+][2020-04-04 19:23:34.991996] Training iteration 253 (batch 1 of epoch 63).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:35.116144] Training iteration 254 (batch 2 of epoch 63).\n",
            "Loss: 0.001206\n",
            "[+][2020-04-04 19:23:35.234703] Training iteration 255 (batch 3 of epoch 63).\n",
            "Loss: 0.000747\n",
            "[+][2020-04-04 19:23:35.353075] Training iteration 256 (batch 4 of epoch 63).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:35.470025] Training iteration 257 (batch 1 of epoch 64).\n",
            "Loss: 0.002138\n",
            "[+][2020-04-04 19:23:35.594194] Training iteration 258 (batch 2 of epoch 64).\n",
            "Loss: 0.000523\n",
            "[+][2020-04-04 19:23:35.713272] Training iteration 259 (batch 3 of epoch 64).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:35.839319] Training iteration 260 (batch 4 of epoch 64).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:35.960576] Training iteration 261 (batch 1 of epoch 65).\n",
            "Loss: 0.003919\n",
            "[+][2020-04-04 19:23:36.085174] Training iteration 262 (batch 2 of epoch 65).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:36.203669] Training iteration 263 (batch 3 of epoch 65).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:36.341568] Training iteration 264 (batch 4 of epoch 65).\n",
            "Loss: 0.000076\n",
            "[+][2020-04-04 19:23:36.461752] Training iteration 265 (batch 1 of epoch 66).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:36.591013] Training iteration 266 (batch 2 of epoch 66).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:36.720546] Training iteration 267 (batch 3 of epoch 66).\n",
            "Loss: 0.003227\n",
            "[+][2020-04-04 19:23:36.843102] Training iteration 268 (batch 4 of epoch 66).\n",
            "Loss: 0.002024\n",
            "[+][2020-04-04 19:23:36.964430] Training iteration 269 (batch 1 of epoch 67).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:37.085006] Training iteration 270 (batch 2 of epoch 67).\n",
            "Loss: 0.002610\n",
            "[+][2020-04-04 19:23:37.203663] Training iteration 271 (batch 3 of epoch 67).\n",
            "Loss: 0.001226\n",
            "[+][2020-04-04 19:23:37.319950] Training iteration 272 (batch 4 of epoch 67).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:37.440166] Training iteration 273 (batch 1 of epoch 68).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:37.577979] Training iteration 274 (batch 2 of epoch 68).\n",
            "Loss: 0.002072\n",
            "[+][2020-04-04 19:23:37.706399] Training iteration 275 (batch 3 of epoch 68).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:37.831752] Training iteration 276 (batch 4 of epoch 68).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:37.950531] Training iteration 277 (batch 1 of epoch 69).\n",
            "Loss: 0.001916\n",
            "[+][2020-04-04 19:23:38.072603] Training iteration 278 (batch 2 of epoch 69).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:38.190489] Training iteration 279 (batch 3 of epoch 69).\n",
            "Loss: 0.001005\n",
            "[+][2020-04-04 19:23:38.305479] Training iteration 280 (batch 4 of epoch 69).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:38.426861] Training iteration 281 (batch 1 of epoch 70).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:38.546421] Training iteration 282 (batch 2 of epoch 70).\n",
            "Loss: 0.002592\n",
            "[+][2020-04-04 19:23:38.664649] Training iteration 283 (batch 3 of epoch 70).\n",
            "Loss: 0.000933\n",
            "[+][2020-04-04 19:23:38.781591] Training iteration 284 (batch 4 of epoch 70).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:38.898856] Training iteration 285 (batch 1 of epoch 71).\n",
            "Loss: 0.002588\n",
            "[+][2020-04-04 19:23:39.027596] Training iteration 286 (batch 2 of epoch 71).\n",
            "Loss: 0.003107\n",
            "[+][2020-04-04 19:23:39.155975] Training iteration 287 (batch 3 of epoch 71).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:39.275664] Training iteration 288 (batch 4 of epoch 71).\n",
            "Loss: 0.000489\n",
            "[+][2020-04-04 19:23:39.391761] Training iteration 289 (batch 1 of epoch 72).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:39.512662] Training iteration 290 (batch 2 of epoch 72).\n",
            "Loss: 0.000598\n",
            "[+][2020-04-04 19:23:39.634669] Training iteration 291 (batch 3 of epoch 72).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:39.762591] Training iteration 292 (batch 4 of epoch 72).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:39.878072] Training iteration 293 (batch 1 of epoch 73).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:39.997430] Training iteration 294 (batch 2 of epoch 73).\n",
            "Loss: 0.004639\n",
            "[+][2020-04-04 19:23:40.121038] Training iteration 295 (batch 3 of epoch 73).\n",
            "Loss: 0.000817\n",
            "[+][2020-04-04 19:23:40.246670] Training iteration 296 (batch 4 of epoch 73).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:40.378927] Training iteration 297 (batch 1 of epoch 74).\n",
            "Loss: 0.003667\n",
            "[+][2020-04-04 19:23:40.500026] Training iteration 298 (batch 2 of epoch 74).\n",
            "Loss: 0.003771\n",
            "[+][2020-04-04 19:23:40.624415] Training iteration 299 (batch 3 of epoch 74).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:40.754969] Training iteration 300 (batch 4 of epoch 74).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:40.874314] Training iteration 301 (batch 1 of epoch 75).\n",
            "Loss: 0.004228\n",
            "[+][2020-04-04 19:23:40.991618] Training iteration 302 (batch 2 of epoch 75).\n",
            "Loss: 0.001737\n",
            "[+][2020-04-04 19:23:41.113363] Training iteration 303 (batch 3 of epoch 75).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:41.234661] Training iteration 304 (batch 4 of epoch 75).\n",
            "Loss: 0.001577\n",
            "[+][2020-04-04 19:23:41.354522] Training iteration 305 (batch 1 of epoch 76).\n",
            "Loss: 0.002483\n",
            "[+][2020-04-04 19:23:41.475042] Training iteration 306 (batch 2 of epoch 76).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:41.594726] Training iteration 307 (batch 3 of epoch 76).\n",
            "Loss: 0.000125\n",
            "[+][2020-04-04 19:23:41.717882] Training iteration 308 (batch 4 of epoch 76).\n",
            "Loss: 0.000635\n",
            "[+][2020-04-04 19:23:41.837274] Training iteration 309 (batch 1 of epoch 77).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:41.963041] Training iteration 310 (batch 2 of epoch 77).\n",
            "Loss: 0.001353\n",
            "[+][2020-04-04 19:23:42.081472] Training iteration 311 (batch 3 of epoch 77).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:42.219786] Training iteration 312 (batch 4 of epoch 77).\n",
            "Loss: 0.001616\n",
            "[+][2020-04-04 19:23:42.335558] Training iteration 313 (batch 1 of epoch 78).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:42.452575] Training iteration 314 (batch 2 of epoch 78).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:42.570116] Training iteration 315 (batch 3 of epoch 78).\n",
            "Loss: 0.001376\n",
            "[+][2020-04-04 19:23:42.692862] Training iteration 316 (batch 4 of epoch 78).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:42.819077] Training iteration 317 (batch 1 of epoch 79).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:42.942094] Training iteration 318 (batch 2 of epoch 79).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:43.088522] Training iteration 319 (batch 3 of epoch 79).\n",
            "Loss: 0.003533\n",
            "[+][2020-04-04 19:23:43.210168] Training iteration 320 (batch 4 of epoch 79).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:43.336214] Training iteration 321 (batch 1 of epoch 80).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:43.459417] Training iteration 322 (batch 2 of epoch 80).\n",
            "Loss: 0.000869\n",
            "[+][2020-04-04 19:23:43.578826] Training iteration 323 (batch 3 of epoch 80).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:43.698962] Training iteration 324 (batch 4 of epoch 80).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:43.824000] Training iteration 325 (batch 1 of epoch 81).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:43.942357] Training iteration 326 (batch 2 of epoch 81).\n",
            "Loss: 0.001814\n",
            "[+][2020-04-04 19:23:44.063707] Training iteration 327 (batch 3 of epoch 81).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:44.188124] Training iteration 328 (batch 4 of epoch 81).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:44.306574] Training iteration 329 (batch 1 of epoch 82).\n",
            "Loss: 0.001717\n",
            "[+][2020-04-04 19:23:44.425878] Training iteration 330 (batch 2 of epoch 82).\n",
            "Loss: 0.002211\n",
            "[+][2020-04-04 19:23:44.544057] Training iteration 331 (batch 3 of epoch 82).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:44.673903] Training iteration 332 (batch 4 of epoch 82).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:44.790393] Training iteration 333 (batch 1 of epoch 83).\n",
            "Loss: 0.002084\n",
            "[+][2020-04-04 19:23:44.907463] Training iteration 334 (batch 2 of epoch 83).\n",
            "Loss: 0.000454\n",
            "[+][2020-04-04 19:23:45.024973] Training iteration 335 (batch 3 of epoch 83).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:45.142409] Training iteration 336 (batch 4 of epoch 83).\n",
            "Loss: 0.002852\n",
            "[+][2020-04-04 19:23:45.263559] Training iteration 337 (batch 1 of epoch 84).\n",
            "Loss: 0.002267\n",
            "[+][2020-04-04 19:23:45.380306] Training iteration 338 (batch 2 of epoch 84).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:45.511872] Training iteration 339 (batch 3 of epoch 84).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:45.631074] Training iteration 340 (batch 4 of epoch 84).\n",
            "Loss: 0.002223\n",
            "[+][2020-04-04 19:23:45.747743] Training iteration 341 (batch 1 of epoch 85).\n",
            "Loss: 0.000389\n",
            "[+][2020-04-04 19:23:45.875099] Training iteration 342 (batch 2 of epoch 85).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:45.992077] Training iteration 343 (batch 3 of epoch 85).\n",
            "Loss: 0.001983\n",
            "[+][2020-04-04 19:23:46.117437] Training iteration 344 (batch 4 of epoch 85).\n",
            "Loss: 0.000483\n",
            "[+][2020-04-04 19:23:46.237979] Training iteration 345 (batch 1 of epoch 86).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:46.360079] Training iteration 346 (batch 2 of epoch 86).\n",
            "Loss: 0.000964\n",
            "[+][2020-04-04 19:23:46.477903] Training iteration 347 (batch 3 of epoch 86).\n",
            "Loss: 0.000185\n",
            "[+][2020-04-04 19:23:46.593530] Training iteration 348 (batch 4 of epoch 86).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:46.714635] Training iteration 349 (batch 1 of epoch 87).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:46.834099] Training iteration 350 (batch 2 of epoch 87).\n",
            "Loss: 0.004208\n",
            "[+][2020-04-04 19:23:46.952697] Training iteration 351 (batch 3 of epoch 87).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:47.077878] Training iteration 352 (batch 4 of epoch 87).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:47.200870] Training iteration 353 (batch 1 of epoch 88).\n",
            "Loss: 0.001016\n",
            "[+][2020-04-04 19:23:47.320755] Training iteration 354 (batch 2 of epoch 88).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:47.438148] Training iteration 355 (batch 3 of epoch 88).\n",
            "Loss: 0.000433\n",
            "[+][2020-04-04 19:23:47.557045] Training iteration 356 (batch 4 of epoch 88).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:47.679654] Training iteration 357 (batch 1 of epoch 89).\n",
            "Loss: 0.000048\n",
            "[+][2020-04-04 19:23:47.800002] Training iteration 358 (batch 2 of epoch 89).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:47.929828] Training iteration 359 (batch 3 of epoch 89).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:48.063773] Training iteration 360 (batch 4 of epoch 89).\n",
            "Loss: 0.000381\n",
            "[+][2020-04-04 19:23:48.189361] Training iteration 361 (batch 1 of epoch 90).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:48.311350] Training iteration 362 (batch 2 of epoch 90).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:48.431394] Training iteration 363 (batch 3 of epoch 90).\n",
            "Loss: 0.001213\n",
            "[+][2020-04-04 19:23:48.548658] Training iteration 364 (batch 4 of epoch 90).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:48.674495] Training iteration 365 (batch 1 of epoch 91).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:48.796375] Training iteration 366 (batch 2 of epoch 91).\n",
            "Loss: 0.000188\n",
            "[+][2020-04-04 19:23:48.927691] Training iteration 367 (batch 3 of epoch 91).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:49.056860] Training iteration 368 (batch 4 of epoch 91).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:49.180709] Training iteration 369 (batch 1 of epoch 92).\n",
            "Loss: 0.001111\n",
            "[+][2020-04-04 19:23:49.302936] Training iteration 370 (batch 2 of epoch 92).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:49.424340] Training iteration 371 (batch 3 of epoch 92).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:49.546622] Training iteration 372 (batch 4 of epoch 92).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:49.680127] Training iteration 373 (batch 1 of epoch 93).\n",
            "Loss: 0.002807\n",
            "[+][2020-04-04 19:23:49.807370] Training iteration 374 (batch 2 of epoch 93).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:49.941660] Training iteration 375 (batch 3 of epoch 93).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:50.066028] Training iteration 376 (batch 4 of epoch 93).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:50.186711] Training iteration 377 (batch 1 of epoch 94).\n",
            "Loss: 0.002707\n",
            "[+][2020-04-04 19:23:50.306476] Training iteration 378 (batch 2 of epoch 94).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:50.425497] Training iteration 379 (batch 3 of epoch 94).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:50.547411] Training iteration 380 (batch 4 of epoch 94).\n",
            "Loss: 0.000553\n",
            "[+][2020-04-04 19:23:50.664963] Training iteration 381 (batch 1 of epoch 95).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:50.787065] Training iteration 382 (batch 2 of epoch 95).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:50.906727] Training iteration 383 (batch 3 of epoch 95).\n",
            "Loss: 0.001296\n",
            "[+][2020-04-04 19:23:51.037024] Training iteration 384 (batch 4 of epoch 95).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:51.156464] Training iteration 385 (batch 1 of epoch 96).\n",
            "Loss: 0.000492\n",
            "[+][2020-04-04 19:23:51.272327] Training iteration 386 (batch 2 of epoch 96).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:51.390299] Training iteration 387 (batch 3 of epoch 96).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:51.509587] Training iteration 388 (batch 4 of epoch 96).\n",
            "Loss: 0.002062\n",
            "[+][2020-04-04 19:23:51.625856] Training iteration 389 (batch 1 of epoch 97).\n",
            "Loss: 0.000265\n",
            "[+][2020-04-04 19:23:51.756807] Training iteration 390 (batch 2 of epoch 97).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:51.871750] Training iteration 391 (batch 3 of epoch 97).\n",
            "Loss: 0.004170\n",
            "[+][2020-04-04 19:23:52.006639] Training iteration 392 (batch 4 of epoch 97).\n",
            "Loss: 0.004228\n",
            "[+][2020-04-04 19:23:52.122087] Training iteration 393 (batch 1 of epoch 98).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:52.239097] Training iteration 394 (batch 2 of epoch 98).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:52.360757] Training iteration 395 (batch 3 of epoch 98).\n",
            "Loss: 0.005140\n",
            "[+][2020-04-04 19:23:52.482986] Training iteration 396 (batch 4 of epoch 98).\n",
            "Loss: 0.002451\n",
            "[+][2020-04-04 19:23:52.606792] Training iteration 397 (batch 1 of epoch 99).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:52.733893] Training iteration 398 (batch 2 of epoch 99).\n",
            "Loss: 0.002284\n",
            "[+][2020-04-04 19:23:52.849863] Training iteration 399 (batch 3 of epoch 99).\n",
            "Loss: 0.002685\n",
            "[+][2020-04-04 19:23:52.969889] Training iteration 400 (batch 4 of epoch 99).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:53.089842] Training iteration 401 (batch 1 of epoch 100).\n",
            "Loss: 0.001997\n",
            "[+][2020-04-04 19:23:53.211962] Training iteration 402 (batch 2 of epoch 100).\n",
            "Loss: 0.000683\n",
            "[+][2020-04-04 19:23:53.329108] Training iteration 403 (batch 3 of epoch 100).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:53.445949] Training iteration 404 (batch 4 of epoch 100).\n",
            "Loss: 0.000885\n",
            "[+][2020-04-04 19:23:53.565547] Training iteration 405 (batch 1 of epoch 101).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:53.687431] Training iteration 406 (batch 2 of epoch 101).\n",
            "Loss: 0.001056\n",
            "[+][2020-04-04 19:23:53.824478] Training iteration 407 (batch 3 of epoch 101).\n",
            "Loss: 0.001354\n",
            "[+][2020-04-04 19:23:54.073701] Training iteration 408 (batch 4 of epoch 101).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:54.193315] Training iteration 409 (batch 1 of epoch 102).\n",
            "Loss: 0.001957\n",
            "[+][2020-04-04 19:23:54.308912] Training iteration 410 (batch 2 of epoch 102).\n",
            "Loss: 0.001326\n",
            "[+][2020-04-04 19:23:54.437126] Training iteration 411 (batch 3 of epoch 102).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:54.559546] Training iteration 412 (batch 4 of epoch 102).\n",
            "Loss: 0.000119\n",
            "[+][2020-04-04 19:23:54.678417] Training iteration 413 (batch 1 of epoch 103).\n",
            "Loss: 0.001496\n",
            "[+][2020-04-04 19:23:54.803819] Training iteration 414 (batch 2 of epoch 103).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:54.919971] Training iteration 415 (batch 3 of epoch 103).\n",
            "Loss: 0.000679\n",
            "[+][2020-04-04 19:23:55.052379] Training iteration 416 (batch 4 of epoch 103).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:55.170021] Training iteration 417 (batch 1 of epoch 104).\n",
            "Loss: 0.001431\n",
            "[+][2020-04-04 19:23:55.286596] Training iteration 418 (batch 2 of epoch 104).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:55.403082] Training iteration 419 (batch 3 of epoch 104).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:55.523406] Training iteration 420 (batch 4 of epoch 104).\n",
            "Loss: 0.000464\n",
            "[+][2020-04-04 19:23:55.637878] Training iteration 421 (batch 1 of epoch 105).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:55.756834] Training iteration 422 (batch 2 of epoch 105).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:55.870369] Training iteration 423 (batch 3 of epoch 105).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:55.985907] Training iteration 424 (batch 4 of epoch 105).\n",
            "Loss: 0.003738\n",
            "[+][2020-04-04 19:23:56.115198] Training iteration 425 (batch 1 of epoch 106).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:56.235149] Training iteration 426 (batch 2 of epoch 106).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:56.350560] Training iteration 427 (batch 3 of epoch 106).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:56.476072] Training iteration 428 (batch 4 of epoch 106).\n",
            "Loss: 0.003074\n",
            "[+][2020-04-04 19:23:56.590784] Training iteration 429 (batch 1 of epoch 107).\n",
            "Loss: 0.001174\n",
            "[+][2020-04-04 19:23:56.706704] Training iteration 430 (batch 2 of epoch 107).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:56.828686] Training iteration 431 (batch 3 of epoch 107).\n",
            "Loss: 0.002735\n",
            "[+][2020-04-04 19:23:56.943543] Training iteration 432 (batch 4 of epoch 107).\n",
            "Loss: 0.002579\n",
            "[+][2020-04-04 19:23:57.065586] Training iteration 433 (batch 1 of epoch 108).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:57.182348] Training iteration 434 (batch 2 of epoch 108).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:57.329673] Training iteration 435 (batch 3 of epoch 108).\n",
            "Loss: 0.005326\n",
            "[+][2020-04-04 19:23:57.447675] Training iteration 436 (batch 4 of epoch 108).\n",
            "Loss: 0.002348\n",
            "[+][2020-04-04 19:23:57.566480] Training iteration 437 (batch 1 of epoch 109).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:57.682439] Training iteration 438 (batch 2 of epoch 109).\n",
            "Loss: 0.003337\n",
            "[+][2020-04-04 19:23:57.806109] Training iteration 439 (batch 3 of epoch 109).\n",
            "Loss: 0.003150\n",
            "[+][2020-04-04 19:23:57.926711] Training iteration 440 (batch 4 of epoch 109).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:58.050396] Training iteration 441 (batch 1 of epoch 110).\n",
            "Loss: 0.001924\n",
            "[+][2020-04-04 19:23:58.177125] Training iteration 442 (batch 2 of epoch 110).\n",
            "Loss: 0.001607\n",
            "[+][2020-04-04 19:23:58.299800] Training iteration 443 (batch 3 of epoch 110).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:58.419880] Training iteration 444 (batch 4 of epoch 110).\n",
            "Loss: 0.001626\n",
            "[+][2020-04-04 19:23:58.536966] Training iteration 445 (batch 1 of epoch 111).\n",
            "Loss: 0.000465\n",
            "[+][2020-04-04 19:23:58.652967] Training iteration 446 (batch 2 of epoch 111).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:58.771951] Training iteration 447 (batch 3 of epoch 111).\n",
            "Loss: 0.001273\n",
            "[+][2020-04-04 19:23:58.905162] Training iteration 448 (batch 4 of epoch 111).\n",
            "Loss: 0.000287\n",
            "[+][2020-04-04 19:23:59.028033] Training iteration 449 (batch 1 of epoch 112).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:59.156130] Training iteration 450 (batch 2 of epoch 112).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:59.271433] Training iteration 451 (batch 3 of epoch 112).\n",
            "Loss: 0.003227\n",
            "[+][2020-04-04 19:23:59.389409] Training iteration 452 (batch 4 of epoch 112).\n",
            "Loss: 0.000488\n",
            "[+][2020-04-04 19:23:59.505774] Training iteration 453 (batch 1 of epoch 113).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:59.621871] Training iteration 454 (batch 2 of epoch 113).\n",
            "Loss: 0.003630\n",
            "[+][2020-04-04 19:23:59.751775] Training iteration 455 (batch 3 of epoch 113).\n",
            "Loss: 0.004689\n",
            "[+][2020-04-04 19:23:59.878812] Training iteration 456 (batch 4 of epoch 113).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:23:59.995674] Training iteration 457 (batch 1 of epoch 114).\n",
            "Loss: 0.001314\n",
            "[+][2020-04-04 19:24:00.112281] Training iteration 458 (batch 2 of epoch 114).\n",
            "Loss: 0.001625\n",
            "[+][2020-04-04 19:24:00.229807] Training iteration 459 (batch 3 of epoch 114).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:00.344418] Training iteration 460 (batch 4 of epoch 114).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:00.459328] Training iteration 461 (batch 1 of epoch 115).\n",
            "Loss: 0.001516\n",
            "[+][2020-04-04 19:24:00.582642] Training iteration 462 (batch 2 of epoch 115).\n",
            "Loss: 0.000529\n",
            "[+][2020-04-04 19:24:00.698766] Training iteration 463 (batch 3 of epoch 115).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:00.820574] Training iteration 464 (batch 4 of epoch 115).\n",
            "Loss: 0.003947\n",
            "[+][2020-04-04 19:24:00.937651] Training iteration 465 (batch 1 of epoch 116).\n",
            "Loss: 0.005220\n",
            "[+][2020-04-04 19:24:01.059060] Training iteration 466 (batch 2 of epoch 116).\n",
            "Loss: 0.001413\n",
            "[+][2020-04-04 19:24:01.188186] Training iteration 467 (batch 3 of epoch 116).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:01.309832] Training iteration 468 (batch 4 of epoch 116).\n",
            "Loss: 0.004614\n",
            "[+][2020-04-04 19:24:01.428494] Training iteration 469 (batch 1 of epoch 117).\n",
            "Loss: 0.001669\n",
            "[+][2020-04-04 19:24:01.547411] Training iteration 470 (batch 2 of epoch 117).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:01.679322] Training iteration 471 (batch 3 of epoch 117).\n",
            "Loss: 0.002085\n",
            "[+][2020-04-04 19:24:01.799939] Training iteration 472 (batch 4 of epoch 117).\n",
            "Loss: 0.003655\n",
            "[+][2020-04-04 19:24:01.918540] Training iteration 473 (batch 1 of epoch 118).\n",
            "Loss: 0.000343\n",
            "[+][2020-04-04 19:24:02.035007] Training iteration 474 (batch 2 of epoch 118).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:02.152782] Training iteration 475 (batch 3 of epoch 118).\n",
            "Loss: 0.004465\n",
            "[+][2020-04-04 19:24:02.271749] Training iteration 476 (batch 4 of epoch 118).\n",
            "Loss: 0.001808\n",
            "[+][2020-04-04 19:24:02.387753] Training iteration 477 (batch 1 of epoch 119).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:02.506718] Training iteration 478 (batch 2 of epoch 119).\n",
            "Loss: 0.002343\n",
            "[+][2020-04-04 19:24:02.620995] Training iteration 479 (batch 3 of epoch 119).\n",
            "Loss: 0.004750\n",
            "[+][2020-04-04 19:24:02.737260] Training iteration 480 (batch 4 of epoch 119).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:02.859095] Training iteration 481 (batch 1 of epoch 120).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:02.974903] Training iteration 482 (batch 2 of epoch 120).\n",
            "Loss: 0.002709\n",
            "[+][2020-04-04 19:24:03.092768] Training iteration 483 (batch 3 of epoch 120).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:03.218382] Training iteration 484 (batch 4 of epoch 120).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:03.335835] Training iteration 485 (batch 1 of epoch 121).\n",
            "Loss: 0.001399\n",
            "[+][2020-04-04 19:24:03.451098] Training iteration 486 (batch 2 of epoch 121).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:03.571612] Training iteration 487 (batch 3 of epoch 121).\n",
            "Loss: 0.001391\n",
            "[+][2020-04-04 19:24:03.686560] Training iteration 488 (batch 4 of epoch 121).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:03.804245] Training iteration 489 (batch 1 of epoch 122).\n",
            "Loss: 0.000995\n",
            "[+][2020-04-04 19:24:03.924869] Training iteration 490 (batch 2 of epoch 122).\n",
            "Loss: 0.001233\n",
            "[+][2020-04-04 19:24:04.063205] Training iteration 491 (batch 3 of epoch 122).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:04.184416] Training iteration 492 (batch 4 of epoch 122).\n",
            "Loss: 0.000314\n",
            "[+][2020-04-04 19:24:04.313862] Training iteration 493 (batch 1 of epoch 123).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:04.433574] Training iteration 494 (batch 2 of epoch 123).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:04.551353] Training iteration 495 (batch 3 of epoch 123).\n",
            "Loss: 0.002111\n",
            "[+][2020-04-04 19:24:04.673041] Training iteration 496 (batch 4 of epoch 123).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:04.802908] Training iteration 497 (batch 1 of epoch 124).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:04.932305] Training iteration 498 (batch 2 of epoch 124).\n",
            "Loss: 0.002119\n",
            "[+][2020-04-04 19:24:05.054797] Training iteration 499 (batch 3 of epoch 124).\n",
            "Loss: 0.000107\n",
            "[+][2020-04-04 19:24:05.177817] Training iteration 500 (batch 4 of epoch 124).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:05.303158] Training iteration 501 (batch 1 of epoch 125).\n",
            "Loss: 0.001336\n",
            "[+][2020-04-04 19:24:05.422356] Training iteration 502 (batch 2 of epoch 125).\n",
            "Loss: 0.000266\n",
            "[+][2020-04-04 19:24:05.537072] Training iteration 503 (batch 3 of epoch 125).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:05.651729] Training iteration 504 (batch 4 of epoch 125).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:05.772274] Training iteration 505 (batch 1 of epoch 126).\n",
            "Loss: 0.002509\n",
            "[+][2020-04-04 19:24:05.897511] Training iteration 506 (batch 2 of epoch 126).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:06.020153] Training iteration 507 (batch 3 of epoch 126).\n",
            "Loss: 0.000293\n",
            "[+][2020-04-04 19:24:06.142672] Training iteration 508 (batch 4 of epoch 126).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:06.259940] Training iteration 509 (batch 1 of epoch 127).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:06.374417] Training iteration 510 (batch 2 of epoch 127).\n",
            "Loss: 0.002428\n",
            "[+][2020-04-04 19:24:06.487954] Training iteration 511 (batch 3 of epoch 127).\n",
            "Loss: 0.000468\n",
            "[+][2020-04-04 19:24:06.606343] Training iteration 512 (batch 4 of epoch 127).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:06.724072] Training iteration 513 (batch 1 of epoch 128).\n",
            "Loss: 0.003734\n",
            "[+][2020-04-04 19:24:06.850840] Training iteration 514 (batch 2 of epoch 128).\n",
            "Loss: 0.003514\n",
            "[+][2020-04-04 19:24:06.975963] Training iteration 515 (batch 3 of epoch 128).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:07.096553] Training iteration 516 (batch 4 of epoch 128).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:07.212632] Training iteration 517 (batch 1 of epoch 129).\n",
            "Loss: 0.002448\n",
            "[+][2020-04-04 19:24:07.339541] Training iteration 518 (batch 2 of epoch 129).\n",
            "Loss: 0.000024\n",
            "[+][2020-04-04 19:24:07.458420] Training iteration 519 (batch 3 of epoch 129).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:07.573397] Training iteration 520 (batch 4 of epoch 129).\n",
            "Loss: 0.002229\n",
            "[+][2020-04-04 19:24:07.687504] Training iteration 521 (batch 1 of epoch 130).\n",
            "Loss: 0.002726\n",
            "[+][2020-04-04 19:24:07.803277] Training iteration 522 (batch 2 of epoch 130).\n",
            "Loss: 0.000742\n",
            "[+][2020-04-04 19:24:07.930019] Training iteration 523 (batch 3 of epoch 130).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:08.050125] Training iteration 524 (batch 4 of epoch 130).\n",
            "Loss: 0.003839\n",
            "[+][2020-04-04 19:24:08.166562] Training iteration 525 (batch 1 of epoch 131).\n",
            "Loss: 0.001842\n",
            "[+][2020-04-04 19:24:08.283840] Training iteration 526 (batch 2 of epoch 131).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:08.404859] Training iteration 527 (batch 3 of epoch 131).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:08.519539] Training iteration 528 (batch 4 of epoch 131).\n",
            "Loss: 0.007212\n",
            "[+][2020-04-04 19:24:08.635653] Training iteration 529 (batch 1 of epoch 132).\n",
            "Loss: 0.004096\n",
            "[+][2020-04-04 19:24:08.756903] Training iteration 530 (batch 2 of epoch 132).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:08.873559] Training iteration 531 (batch 3 of epoch 132).\n",
            "Loss: 0.003331\n",
            "[+][2020-04-04 19:24:08.999971] Training iteration 532 (batch 4 of epoch 132).\n",
            "Loss: 0.004251\n",
            "[+][2020-04-04 19:24:09.117793] Training iteration 533 (batch 1 of epoch 133).\n",
            "Loss: 0.001066\n",
            "[+][2020-04-04 19:24:09.235597] Training iteration 534 (batch 2 of epoch 133).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:09.361195] Training iteration 535 (batch 3 of epoch 133).\n",
            "Loss: 0.006297\n",
            "[+][2020-04-04 19:24:09.476336] Training iteration 536 (batch 4 of epoch 133).\n",
            "Loss: 0.004052\n",
            "[+][2020-04-04 19:24:09.591206] Training iteration 537 (batch 1 of epoch 134).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:09.713709] Training iteration 538 (batch 2 of epoch 134).\n",
            "Loss: 0.001609\n",
            "[+][2020-04-04 19:24:09.827578] Training iteration 539 (batch 3 of epoch 134).\n",
            "Loss: 0.002859\n",
            "[+][2020-04-04 19:24:09.948894] Training iteration 540 (batch 4 of epoch 134).\n",
            "Loss: 0.000748\n",
            "[+][2020-04-04 19:24:10.067142] Training iteration 541 (batch 1 of epoch 135).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:10.184005] Training iteration 542 (batch 2 of epoch 135).\n",
            "Loss: 0.002980\n",
            "[+][2020-04-04 19:24:10.300821] Training iteration 543 (batch 3 of epoch 135).\n",
            "Loss: 0.001477\n",
            "[+][2020-04-04 19:24:10.427780] Training iteration 544 (batch 4 of epoch 135).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:10.546724] Training iteration 545 (batch 1 of epoch 136).\n",
            "Loss: 0.001372\n",
            "[+][2020-04-04 19:24:10.660325] Training iteration 546 (batch 2 of epoch 136).\n",
            "Loss: 0.002356\n",
            "[+][2020-04-04 19:24:10.778594] Training iteration 547 (batch 3 of epoch 136).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:10.894245] Training iteration 548 (batch 4 of epoch 136).\n",
            "Loss: 0.000193\n",
            "[+][2020-04-04 19:24:11.019475] Training iteration 549 (batch 1 of epoch 137).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:11.141962] Training iteration 550 (batch 2 of epoch 137).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:11.266325] Training iteration 551 (batch 3 of epoch 137).\n",
            "Loss: 0.002707\n",
            "[+][2020-04-04 19:24:11.394914] Training iteration 552 (batch 4 of epoch 137).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:11.511402] Training iteration 553 (batch 1 of epoch 138).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:11.626764] Training iteration 554 (batch 2 of epoch 138).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:11.743602] Training iteration 555 (batch 3 of epoch 138).\n",
            "Loss: 0.002263\n",
            "[+][2020-04-04 19:24:11.861194] Training iteration 556 (batch 4 of epoch 138).\n",
            "Loss: 0.000258\n",
            "[+][2020-04-04 19:24:11.983504] Training iteration 557 (batch 1 of epoch 139).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:12.101854] Training iteration 558 (batch 2 of epoch 139).\n",
            "Loss: 0.003200\n",
            "[+][2020-04-04 19:24:12.217071] Training iteration 559 (batch 3 of epoch 139).\n",
            "Loss: 0.002280\n",
            "[+][2020-04-04 19:24:12.348147] Training iteration 560 (batch 4 of epoch 139).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:12.471362] Training iteration 561 (batch 1 of epoch 140).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:12.584930] Training iteration 562 (batch 2 of epoch 140).\n",
            "Loss: 0.002097\n",
            "[+][2020-04-04 19:24:12.700069] Training iteration 563 (batch 3 of epoch 140).\n",
            "Loss: 0.001350\n",
            "[+][2020-04-04 19:24:12.820164] Training iteration 564 (batch 4 of epoch 140).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:12.942072] Training iteration 565 (batch 1 of epoch 141).\n",
            "Loss: 0.000153\n",
            "[+][2020-04-04 19:24:13.060847] Training iteration 566 (batch 2 of epoch 141).\n",
            "Loss: 0.002024\n",
            "[+][2020-04-04 19:24:13.176278] Training iteration 567 (batch 3 of epoch 141).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:13.289856] Training iteration 568 (batch 4 of epoch 141).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:13.403924] Training iteration 569 (batch 1 of epoch 142).\n",
            "Loss: 0.001627\n",
            "[+][2020-04-04 19:24:13.518289] Training iteration 570 (batch 2 of epoch 142).\n",
            "Loss: 0.000199\n",
            "[+][2020-04-04 19:24:13.635149] Training iteration 571 (batch 3 of epoch 142).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:13.748489] Training iteration 572 (batch 4 of epoch 142).\n",
            "Loss: 0.001634\n",
            "[+][2020-04-04 19:24:13.862774] Training iteration 573 (batch 1 of epoch 143).\n",
            "Loss: 0.001870\n",
            "[+][2020-04-04 19:24:13.991893] Training iteration 574 (batch 2 of epoch 143).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:14.105974] Training iteration 575 (batch 3 of epoch 143).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:14.233187] Training iteration 576 (batch 4 of epoch 143).\n",
            "Loss: 0.000490\n",
            "[+][2020-04-04 19:24:14.348841] Training iteration 577 (batch 1 of epoch 144).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:14.475689] Training iteration 578 (batch 2 of epoch 144).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:14.590321] Training iteration 579 (batch 3 of epoch 144).\n",
            "Loss: 0.000588\n",
            "[+][2020-04-04 19:24:14.704782] Training iteration 580 (batch 4 of epoch 144).\n",
            "Loss: 0.000307\n",
            "[+][2020-04-04 19:24:14.820966] Training iteration 581 (batch 1 of epoch 145).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:14.935088] Training iteration 582 (batch 2 of epoch 145).\n",
            "Loss: 0.002637\n",
            "[+][2020-04-04 19:24:15.056555] Training iteration 583 (batch 3 of epoch 145).\n",
            "Loss: 0.001105\n",
            "[+][2020-04-04 19:24:15.171957] Training iteration 584 (batch 4 of epoch 145).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:15.285689] Training iteration 585 (batch 1 of epoch 146).\n",
            "Loss: 0.000844\n",
            "[+][2020-04-04 19:24:15.401174] Training iteration 586 (batch 2 of epoch 146).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:15.528758] Training iteration 587 (batch 3 of epoch 146).\n",
            "Loss: 0.002080\n",
            "[+][2020-04-04 19:24:15.645762] Training iteration 588 (batch 4 of epoch 146).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:15.761251] Training iteration 589 (batch 1 of epoch 147).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:15.875903] Training iteration 590 (batch 2 of epoch 147).\n",
            "Loss: 0.001140\n",
            "[+][2020-04-04 19:24:15.991297] Training iteration 591 (batch 3 of epoch 147).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:16.111842] Training iteration 592 (batch 4 of epoch 147).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:16.227159] Training iteration 593 (batch 1 of epoch 148).\n",
            "Loss: 0.000595\n",
            "[+][2020-04-04 19:24:16.345995] Training iteration 594 (batch 2 of epoch 148).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:16.460862] Training iteration 595 (batch 3 of epoch 148).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:16.574425] Training iteration 596 (batch 4 of epoch 148).\n",
            "Loss: 0.000884\n",
            "[+][2020-04-04 19:24:16.688470] Training iteration 597 (batch 1 of epoch 149).\n",
            "Loss: 0.000200\n",
            "[+][2020-04-04 19:24:16.805437] Training iteration 598 (batch 2 of epoch 149).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:16.923389] Training iteration 599 (batch 3 of epoch 149).\n",
            "Loss: 0.002445\n",
            "[+][2020-04-04 19:24:17.050522] Training iteration 600 (batch 4 of epoch 149).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:17.169691] Training iteration 601 (batch 1 of epoch 150).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:17.283552] Training iteration 602 (batch 2 of epoch 150).\n",
            "Loss: 0.001569\n",
            "[+][2020-04-04 19:24:17.397568] Training iteration 603 (batch 3 of epoch 150).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:17.520568] Training iteration 604 (batch 4 of epoch 150).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:17.635146] Training iteration 605 (batch 1 of epoch 151).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:17.750103] Training iteration 606 (batch 2 of epoch 151).\n",
            "Loss: 0.004417\n",
            "[+][2020-04-04 19:24:17.868321] Training iteration 607 (batch 3 of epoch 151).\n",
            "Loss: 0.000794\n",
            "[+][2020-04-04 19:24:17.997216] Training iteration 608 (batch 4 of epoch 151).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:18.116105] Training iteration 609 (batch 1 of epoch 152).\n",
            "Loss: 0.002736\n",
            "[+][2020-04-04 19:24:18.240642] Training iteration 610 (batch 2 of epoch 152).\n",
            "Loss: 0.002331\n",
            "[+][2020-04-04 19:24:18.354350] Training iteration 611 (batch 3 of epoch 152).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:18.467598] Training iteration 612 (batch 4 of epoch 152).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:18.590136] Training iteration 613 (batch 1 of epoch 153).\n",
            "Loss: 0.004469\n",
            "[+][2020-04-04 19:24:18.703432] Training iteration 614 (batch 2 of epoch 153).\n",
            "Loss: 0.002143\n",
            "[+][2020-04-04 19:24:18.822769] Training iteration 615 (batch 3 of epoch 153).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:18.937379] Training iteration 616 (batch 4 of epoch 153).\n",
            "Loss: 0.000065\n",
            "[+][2020-04-04 19:24:19.059535] Training iteration 617 (batch 1 of epoch 154).\n",
            "Loss: 0.000158\n",
            "[+][2020-04-04 19:24:19.174576] Training iteration 618 (batch 2 of epoch 154).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:19.288833] Training iteration 619 (batch 3 of epoch 154).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:19.403742] Training iteration 620 (batch 4 of epoch 154).\n",
            "Loss: 0.002661\n",
            "[+][2020-04-04 19:24:19.520368] Training iteration 621 (batch 1 of epoch 155).\n",
            "Loss: 0.000120\n",
            "[+][2020-04-04 19:24:19.639121] Training iteration 622 (batch 2 of epoch 155).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:19.756703] Training iteration 623 (batch 3 of epoch 155).\n",
            "Loss: 0.001890\n",
            "[+][2020-04-04 19:24:19.872902] Training iteration 624 (batch 4 of epoch 155).\n",
            "Loss: 0.001278\n",
            "[+][2020-04-04 19:24:19.987216] Training iteration 625 (batch 1 of epoch 156).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:20.109242] Training iteration 626 (batch 2 of epoch 156).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:20.235215] Training iteration 627 (batch 3 of epoch 156).\n",
            "Loss: 0.001210\n",
            "[+][2020-04-04 19:24:20.357553] Training iteration 628 (batch 4 of epoch 156).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:20.476010] Training iteration 629 (batch 1 of epoch 157).\n",
            "Loss: 0.000234\n",
            "[+][2020-04-04 19:24:20.607173] Training iteration 630 (batch 2 of epoch 157).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:20.722804] Training iteration 631 (batch 3 of epoch 157).\n",
            "Loss: 0.001936\n",
            "[+][2020-04-04 19:24:20.838680] Training iteration 632 (batch 4 of epoch 157).\n",
            "Loss: 0.000648\n",
            "[+][2020-04-04 19:24:20.958990] Training iteration 633 (batch 1 of epoch 158).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:21.085584] Training iteration 634 (batch 2 of epoch 158).\n",
            "Loss: 0.000386\n",
            "[+][2020-04-04 19:24:21.202678] Training iteration 635 (batch 3 of epoch 158).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:21.337727] Training iteration 636 (batch 4 of epoch 158).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:21.455767] Training iteration 637 (batch 1 of epoch 159).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:21.584574] Training iteration 638 (batch 2 of epoch 159).\n",
            "Loss: 0.000884\n",
            "[+][2020-04-04 19:24:21.700335] Training iteration 639 (batch 3 of epoch 159).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:21.817597] Training iteration 640 (batch 4 of epoch 159).\n",
            "Loss: 0.000330\n",
            "[+][2020-04-04 19:24:21.936023] Training iteration 641 (batch 1 of epoch 160).\n",
            "Loss: 0.000068\n",
            "[+][2020-04-04 19:24:22.055208] Training iteration 642 (batch 2 of epoch 160).\n",
            "Loss: 0.000692\n",
            "[+][2020-04-04 19:24:22.181367] Training iteration 643 (batch 3 of epoch 160).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:22.307758] Training iteration 644 (batch 4 of epoch 160).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:22.425173] Training iteration 645 (batch 1 of epoch 161).\n",
            "Loss: 0.000598\n",
            "[+][2020-04-04 19:24:22.544404] Training iteration 646 (batch 2 of epoch 161).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:22.667595] Training iteration 647 (batch 3 of epoch 161).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:22.786915] Training iteration 648 (batch 4 of epoch 161).\n",
            "Loss: 0.001100\n",
            "[+][2020-04-04 19:24:22.902837] Training iteration 649 (batch 1 of epoch 162).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:23.025728] Training iteration 650 (batch 2 of epoch 162).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:23.147455] Training iteration 651 (batch 3 of epoch 162).\n",
            "Loss: 0.001732\n",
            "[+][2020-04-04 19:24:23.269854] Training iteration 652 (batch 4 of epoch 162).\n",
            "Loss: 0.000098\n",
            "[+][2020-04-04 19:24:23.391008] Training iteration 653 (batch 1 of epoch 163).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:23.509690] Training iteration 654 (batch 2 of epoch 163).\n",
            "Loss: 0.001290\n",
            "[+][2020-04-04 19:24:23.631749] Training iteration 655 (batch 3 of epoch 163).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:23.753899] Training iteration 656 (batch 4 of epoch 163).\n",
            "Loss: 0.000433\n",
            "[+][2020-04-04 19:24:23.870676] Training iteration 657 (batch 1 of epoch 164).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:23.992168] Training iteration 658 (batch 2 of epoch 164).\n",
            "Loss: 0.001041\n",
            "[+][2020-04-04 19:24:24.117463] Training iteration 659 (batch 3 of epoch 164).\n",
            "Loss: 0.001073\n",
            "[+][2020-04-04 19:24:24.240774] Training iteration 660 (batch 4 of epoch 164).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:24.363142] Training iteration 661 (batch 1 of epoch 165).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:24.502284] Training iteration 662 (batch 2 of epoch 165).\n",
            "Loss: 0.001915\n",
            "[+][2020-04-04 19:24:24.620909] Training iteration 663 (batch 3 of epoch 165).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:24.754242] Training iteration 664 (batch 4 of epoch 165).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:24.871459] Training iteration 665 (batch 1 of epoch 166).\n",
            "Loss: 0.001085\n",
            "[+][2020-04-04 19:24:24.986192] Training iteration 666 (batch 2 of epoch 166).\n",
            "Loss: 0.000067\n",
            "[+][2020-04-04 19:24:25.108590] Training iteration 667 (batch 3 of epoch 166).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:25.232087] Training iteration 668 (batch 4 of epoch 166).\n",
            "Loss: 0.001295\n",
            "[+][2020-04-04 19:24:25.351040] Training iteration 669 (batch 1 of epoch 167).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:25.467760] Training iteration 670 (batch 2 of epoch 167).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:25.582216] Training iteration 671 (batch 3 of epoch 167).\n",
            "Loss: 0.000386\n",
            "[+][2020-04-04 19:24:25.707141] Training iteration 672 (batch 4 of epoch 167).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:25.837507] Training iteration 673 (batch 1 of epoch 168).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:25.957029] Training iteration 674 (batch 2 of epoch 168).\n",
            "Loss: 0.000367\n",
            "[+][2020-04-04 19:24:26.076470] Training iteration 675 (batch 3 of epoch 168).\n",
            "Loss: 0.000277\n",
            "[+][2020-04-04 19:24:26.205731] Training iteration 676 (batch 4 of epoch 168).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:26.324619] Training iteration 677 (batch 1 of epoch 169).\n",
            "Loss: 0.000963\n",
            "[+][2020-04-04 19:24:26.440566] Training iteration 678 (batch 2 of epoch 169).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:26.556144] Training iteration 679 (batch 3 of epoch 169).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:26.673094] Training iteration 680 (batch 4 of epoch 169).\n",
            "Loss: 0.001170\n",
            "[+][2020-04-04 19:24:26.792868] Training iteration 681 (batch 1 of epoch 170).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:26.913205] Training iteration 682 (batch 2 of epoch 170).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:27.031933] Training iteration 683 (batch 3 of epoch 170).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:27.158773] Training iteration 684 (batch 4 of epoch 170).\n",
            "Loss: 0.002669\n",
            "[+][2020-04-04 19:24:27.285107] Training iteration 685 (batch 1 of epoch 171).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:27.407984] Training iteration 686 (batch 2 of epoch 171).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:27.526802] Training iteration 687 (batch 3 of epoch 171).\n",
            "Loss: 0.001427\n",
            "[+][2020-04-04 19:24:27.644682] Training iteration 688 (batch 4 of epoch 171).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:27.771913] Training iteration 689 (batch 1 of epoch 172).\n",
            "Loss: 0.000583\n",
            "[+][2020-04-04 19:24:27.892312] Training iteration 690 (batch 2 of epoch 172).\n",
            "Loss: 0.000469\n",
            "[+][2020-04-04 19:24:28.012905] Training iteration 691 (batch 3 of epoch 172).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:28.132468] Training iteration 692 (batch 4 of epoch 172).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:28.252633] Training iteration 693 (batch 1 of epoch 173).\n",
            "Loss: 0.002354\n",
            "[+][2020-04-04 19:24:28.377026] Training iteration 694 (batch 2 of epoch 173).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:28.496383] Training iteration 695 (batch 3 of epoch 173).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:28.611944] Training iteration 696 (batch 4 of epoch 173).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:28.730409] Training iteration 697 (batch 1 of epoch 174).\n",
            "Loss: 0.003574\n",
            "[+][2020-04-04 19:24:28.864960] Training iteration 698 (batch 2 of epoch 174).\n",
            "Loss: 0.000144\n",
            "[+][2020-04-04 19:24:28.980750] Training iteration 699 (batch 3 of epoch 174).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:29.095455] Training iteration 700 (batch 4 of epoch 174).\n",
            "Loss: 0.002024\n",
            "[+][2020-04-04 19:24:29.225784] Training iteration 701 (batch 1 of epoch 175).\n",
            "Loss: 0.002120\n",
            "[+][2020-04-04 19:24:29.346663] Training iteration 702 (batch 2 of epoch 175).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:29.465573] Training iteration 703 (batch 3 of epoch 175).\n",
            "Loss: 0.000650\n",
            "[+][2020-04-04 19:24:29.584320] Training iteration 704 (batch 4 of epoch 175).\n",
            "Loss: 0.001133\n",
            "[+][2020-04-04 19:24:29.701878] Training iteration 705 (batch 1 of epoch 176).\n",
            "Loss: 0.000064\n",
            "[+][2020-04-04 19:24:29.832096] Training iteration 706 (batch 2 of epoch 176).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:29.948642] Training iteration 707 (batch 3 of epoch 176).\n",
            "Loss: 0.001299\n",
            "[+][2020-04-04 19:24:30.066563] Training iteration 708 (batch 4 of epoch 176).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:30.188429] Training iteration 709 (batch 1 of epoch 177).\n",
            "Loss: 0.000100\n",
            "[+][2020-04-04 19:24:30.312393] Training iteration 710 (batch 2 of epoch 177).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:30.432340] Training iteration 711 (batch 3 of epoch 177).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:30.549393] Training iteration 712 (batch 4 of epoch 177).\n",
            "Loss: 0.003377\n",
            "[+][2020-04-04 19:24:30.663619] Training iteration 713 (batch 1 of epoch 178).\n",
            "Loss: 0.000886\n",
            "[+][2020-04-04 19:24:30.781489] Training iteration 714 (batch 2 of epoch 178).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:30.894995] Training iteration 715 (batch 3 of epoch 178).\n",
            "Loss: 0.001048\n",
            "[+][2020-04-04 19:24:31.012795] Training iteration 716 (batch 4 of epoch 178).\n",
            "Loss: 0.000173\n",
            "[+][2020-04-04 19:24:31.130559] Training iteration 717 (batch 1 of epoch 179).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:31.254956] Training iteration 718 (batch 2 of epoch 179).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:31.373621] Training iteration 719 (batch 3 of epoch 179).\n",
            "Loss: 0.003873\n",
            "[+][2020-04-04 19:24:31.489686] Training iteration 720 (batch 4 of epoch 179).\n",
            "Loss: 0.001157\n",
            "[+][2020-04-04 19:24:31.613559] Training iteration 721 (batch 1 of epoch 180).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:31.742202] Training iteration 722 (batch 2 of epoch 180).\n",
            "Loss: 0.001431\n",
            "[+][2020-04-04 19:24:31.883743] Training iteration 723 (batch 3 of epoch 180).\n",
            "Loss: 0.001902\n",
            "[+][2020-04-04 19:24:32.011417] Training iteration 724 (batch 4 of epoch 180).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:32.139081] Training iteration 725 (batch 1 of epoch 181).\n",
            "Loss: 0.000183\n",
            "[+][2020-04-04 19:24:32.284525] Training iteration 726 (batch 2 of epoch 181).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:32.398764] Training iteration 727 (batch 3 of epoch 181).\n",
            "Loss: 0.001099\n",
            "[+][2020-04-04 19:24:32.513119] Training iteration 728 (batch 4 of epoch 181).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:32.627510] Training iteration 729 (batch 1 of epoch 182).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:32.746496] Training iteration 730 (batch 2 of epoch 182).\n",
            "Loss: 0.001839\n",
            "[+][2020-04-04 19:24:32.868284] Training iteration 731 (batch 3 of epoch 182).\n",
            "Loss: 0.000499\n",
            "[+][2020-04-04 19:24:32.983600] Training iteration 732 (batch 4 of epoch 182).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:33.102149] Training iteration 733 (batch 1 of epoch 183).\n",
            "Loss: 0.001338\n",
            "[+][2020-04-04 19:24:33.225322] Training iteration 734 (batch 2 of epoch 183).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:33.342184] Training iteration 735 (batch 3 of epoch 183).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:33.456540] Training iteration 736 (batch 4 of epoch 183).\n",
            "Loss: 0.001099\n",
            "[+][2020-04-04 19:24:33.572752] Training iteration 737 (batch 1 of epoch 184).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:33.687591] Training iteration 738 (batch 2 of epoch 184).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:33.802686] Training iteration 739 (batch 3 of epoch 184).\n",
            "Loss: 0.002087\n",
            "[+][2020-04-04 19:24:33.928162] Training iteration 740 (batch 4 of epoch 184).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:34.047253] Training iteration 741 (batch 1 of epoch 185).\n",
            "Loss: 0.000609\n",
            "[+][2020-04-04 19:24:34.160503] Training iteration 742 (batch 2 of epoch 185).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:34.293692] Training iteration 743 (batch 3 of epoch 185).\n",
            "Loss: 0.000182\n",
            "[+][2020-04-04 19:24:34.415151] Training iteration 744 (batch 4 of epoch 185).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:34.530974] Training iteration 745 (batch 1 of epoch 186).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:34.644624] Training iteration 746 (batch 2 of epoch 186).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:34.766483] Training iteration 747 (batch 3 of epoch 186).\n",
            "Loss: 0.000867\n",
            "[+][2020-04-04 19:24:34.891537] Training iteration 748 (batch 4 of epoch 186).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:35.009505] Training iteration 749 (batch 1 of epoch 187).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:35.132585] Training iteration 750 (batch 2 of epoch 187).\n",
            "Loss: 0.001305\n",
            "[+][2020-04-04 19:24:35.255920] Training iteration 751 (batch 3 of epoch 187).\n",
            "Loss: 0.001991\n",
            "[+][2020-04-04 19:24:35.368415] Training iteration 752 (batch 4 of epoch 187).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:35.484341] Training iteration 753 (batch 1 of epoch 188).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:35.614926] Training iteration 754 (batch 2 of epoch 188).\n",
            "Loss: 0.001845\n",
            "[+][2020-04-04 19:24:35.731859] Training iteration 755 (batch 3 of epoch 188).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:35.847356] Training iteration 756 (batch 4 of epoch 188).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:35.971840] Training iteration 757 (batch 1 of epoch 189).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:36.087993] Training iteration 758 (batch 2 of epoch 189).\n",
            "Loss: 0.001198\n",
            "[+][2020-04-04 19:24:36.202072] Training iteration 759 (batch 3 of epoch 189).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:36.320461] Training iteration 760 (batch 4 of epoch 189).\n",
            "Loss: 0.000911\n",
            "[+][2020-04-04 19:24:36.436069] Training iteration 761 (batch 1 of epoch 190).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:36.553654] Training iteration 762 (batch 2 of epoch 190).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:36.666940] Training iteration 763 (batch 3 of epoch 190).\n",
            "Loss: 0.001640\n",
            "[+][2020-04-04 19:24:36.783901] Training iteration 764 (batch 4 of epoch 190).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:36.899837] Training iteration 765 (batch 1 of epoch 191).\n",
            "Loss: 0.000537\n",
            "[+][2020-04-04 19:24:37.013750] Training iteration 766 (batch 2 of epoch 191).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:37.126964] Training iteration 767 (batch 3 of epoch 191).\n",
            "Loss: 0.001526\n",
            "[+][2020-04-04 19:24:37.241747] Training iteration 768 (batch 4 of epoch 191).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:37.357786] Training iteration 769 (batch 1 of epoch 192).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:37.474861] Training iteration 770 (batch 2 of epoch 192).\n",
            "Loss: 0.001311\n",
            "[+][2020-04-04 19:24:37.588496] Training iteration 771 (batch 3 of epoch 192).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:37.703296] Training iteration 772 (batch 4 of epoch 192).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:37.822932] Training iteration 773 (batch 1 of epoch 193).\n",
            "Loss: 0.000147\n",
            "[+][2020-04-04 19:24:37.940967] Training iteration 774 (batch 2 of epoch 193).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:38.055363] Training iteration 775 (batch 3 of epoch 193).\n",
            "Loss: 0.000077\n",
            "[+][2020-04-04 19:24:38.169769] Training iteration 776 (batch 4 of epoch 193).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:38.295989] Training iteration 777 (batch 1 of epoch 194).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:38.411674] Training iteration 778 (batch 2 of epoch 194).\n",
            "Loss: 0.000673\n",
            "[+][2020-04-04 19:24:38.525881] Training iteration 779 (batch 3 of epoch 194).\n",
            "Loss: 0.000298\n",
            "[+][2020-04-04 19:24:38.639810] Training iteration 780 (batch 4 of epoch 194).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:38.758586] Training iteration 781 (batch 1 of epoch 195).\n",
            "Loss: 0.000686\n",
            "[+][2020-04-04 19:24:38.872808] Training iteration 782 (batch 2 of epoch 195).\n",
            "Loss: 0.000700\n",
            "[+][2020-04-04 19:24:38.994504] Training iteration 783 (batch 3 of epoch 195).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:39.117114] Training iteration 784 (batch 4 of epoch 195).\n",
            "Loss: 0.000614\n",
            "[+][2020-04-04 19:24:39.246908] Training iteration 785 (batch 1 of epoch 196).\n",
            "Loss: 0.000400\n",
            "[+][2020-04-04 19:24:39.363544] Training iteration 786 (batch 2 of epoch 196).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:39.477462] Training iteration 787 (batch 3 of epoch 196).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:39.590283] Training iteration 788 (batch 4 of epoch 196).\n",
            "Loss: 0.000985\n",
            "[+][2020-04-04 19:24:39.704379] Training iteration 789 (batch 1 of epoch 197).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:39.818816] Training iteration 790 (batch 2 of epoch 197).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:39.936584] Training iteration 791 (batch 3 of epoch 197).\n",
            "Loss: 0.000550\n",
            "[+][2020-04-04 19:24:40.061757] Training iteration 792 (batch 4 of epoch 197).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:40.184853] Training iteration 793 (batch 1 of epoch 198).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:40.310299] Training iteration 794 (batch 2 of epoch 198).\n",
            "Loss: 0.002535\n",
            "[+][2020-04-04 19:24:40.431728] Training iteration 795 (batch 3 of epoch 198).\n",
            "Loss: 0.000553\n",
            "[+][2020-04-04 19:24:40.570651] Training iteration 796 (batch 4 of epoch 198).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:40.706183] Training iteration 797 (batch 1 of epoch 199).\n",
            "Loss: 0.000262\n",
            "[+][2020-04-04 19:24:40.861037] Training iteration 798 (batch 2 of epoch 199).\n",
            "Loss: 0.000967\n",
            "[+][2020-04-04 19:24:40.993249] Training iteration 799 (batch 3 of epoch 199).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:41.114609] Training iteration 800 (batch 4 of epoch 199).\n",
            "Loss: -0.000000\n",
            "[+][2020-04-04 19:24:41.244943] Breaking on request from callback.\n",
            "[+][2020-04-04 19:24:41.245036] Exceeded max number of iterations / epochs, breaking.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9C_pgWvksrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = torch.autograd.Variable(mnist[np.random.randint(2000, 60000)][0].unsqueeze(dim=0))\n",
        "example[0, -10:] = -1\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "    example = example.cuda()\n",
        "expanded = net.expand_data(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYd2Xdltlqt_",
        "colab_type": "code",
        "outputId": "b523aea6-5605-4bb6-c2c2-573c5fa78d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wG4tVg23FdWhgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWwzvVY4mTqm",
        "colab_type": "code",
        "outputId": "e9a9c080-54c7-4aff-da95-3cdfad0ea168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "example_ev = net(expanded, num_iters=100)\n",
        "rbm_demo_utils.display_image(example_ev.data[0][:794])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf5OizYHEMLnfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdqT7GAep1_O",
        "colab_type": "code",
        "outputId": "4859d943-f3fd-4229-a56b-1d5cac8a832f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "example_ev = net.inference(expanded, num_iters=50, num_inference_iters=100)\n",
        "rbm_demo_utils.display_image(example_ev.data[0][:794])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP28_vNsmVO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example2 = torch.autograd.Variable(mnist[np.random.randint(2000, 60000)][0].unsqueeze(dim=0))\n",
        "example2[0, -10:] = -1\n",
        "dropout = torch.nn.Dropout(0.2) # used to mess up the input\n",
        "# print(example2)\n",
        "example2_changed = dropout(example2) * (4/5)\n",
        "example2_changed[example2_changed == 0] = 1\n",
        "if torch.cuda.is_available():\n",
        "    net.cuda()\n",
        "    example2 = example2.cuda()\n",
        "    example2_changed = example2_changed.cuda()\n",
        "expanded2 = net.expand_data(example2_changed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgv7DXckq99X",
        "colab_type": "code",
        "outputId": "3996ec3d-45c0-4c02-fad5-612bc6d84de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example2.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB5L4P79RxwwcfbnRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2cqYZySrBSe",
        "colab_type": "code",
        "outputId": "97ac2afe-d74c-469f-b0b0-646ab7973876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "rbm_demo_utils.display_image(example2_changed.data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxBOdBtxrG8t",
        "colab_type": "code",
        "outputId": "3570e5fc-e32b-4bc2-f61e-4b6cc323ce6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "example2_ev = net.inference(expanded2, num_iters=50, num_inference_iters=100)\n",
        "rbm_demo_utils.display_image(example2_ev.data[0][:794])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2, device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkARhB5L4P9bmNeAXGdcvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}